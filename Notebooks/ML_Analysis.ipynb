{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # for visualizations\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns # for visualizations\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from numpy import asarray\n",
    "\n",
    "#model packages\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import tree\n",
    "import statsmodels.api as sm #linear regression tool\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "\n",
    "#Pre-processing and metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing #for normalizing values\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, IntegerType, DateType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the spark session and context.\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema with the column names, type, and nullable as true or false\n",
    "schema = StructType([ \\\n",
    "    StructField(\"index\", IntegerType(), True), \\\n",
    "    StructField(\"MLSNumber\", StringType(), True), \\\n",
    "    StructField(\"DOM\", IntegerType(), True), \\\n",
    "    StructField(\"CDOM\", IntegerType(), True), \\\n",
    "    StructField(\"ListDate\", StringType(), True), \\\n",
    "    StructField(\"AgreementOfSaleSignedLeaseDate\", StringType(), True), \\\n",
    "    StructField(\"OffMarketDate\", StringType(), True), \\\n",
    "    StructField(\"SettledDate\", StringType(), True), \\\n",
    "    StructField(\"OriginalPrice\", StringType(), True), \\\n",
    "    StructField(\"ListPrice\", StringType(), True), \\\n",
    "    StructField(\"SoldPrice\", StringType(), True), \\\n",
    "    StructField(\"StreetNumber\", StringType(), True), \\\n",
    "    StructField(\"StreetDirection\", StringType(), True), \\\n",
    "    StructField(\"StreetName\", StringType(), True), \\\n",
    "    StructField(\"UnitNumber\", StringType(), False), \\\n",
    "    StructField(\"City\", StringType(), True), \\\n",
    "    StructField(\"ZipCode\", IntegerType(), True), \\\n",
    "    StructField(\"County\", StringType(), True), \\\n",
    "    StructField(\"Subdivision\", StringType(), True), \\\n",
    "    StructField(\"ListAgentName\", StringType(), True), \\\n",
    "    StructField(\"ListAgentCode\", IntegerType(), True), \\\n",
    "    StructField(\"ListOfficeName\", StringType(), True), \\\n",
    "    StructField(\"ListOfficeCode\", StringType(), True), \\\n",
    "    StructField(\"SellingAgent\", StringType(), True), \\\n",
    "    StructField(\"SellingAgentCode\", StringType(), True), \\\n",
    "    StructField(\"SellingOfficeName\", StringType(), True), \\\n",
    "    StructField(\"SellingOfficeCode\", StringType(), True), \\\n",
    "    StructField(\"SellerConcessionsAmount\", StringType(), True), \\\n",
    "    StructField(\"FinalFinancing\", StringType(), True), \\\n",
    "    StructField(\"FinalShortSale\", StringType(), True), \\\n",
    "    StructField(\"FinalThirdPartyApproval\", StringType(), True), \\\n",
    "    StructField(\"FinalBankOwned\", StringType(), True), \\\n",
    "    StructField(\"TaxAnnualTotal\", StringType(), True), \\\n",
    "    StructField(\"TaxYear\", StringType(), True), \\\n",
    "    StructField(\"AcresTotal\", DoubleType(), True), \\\n",
    "    StructField(\"LandUseCode\", StringType(), True), \\\n",
    "    StructField(\"Ownership\", StringType(), True), \\\n",
    "    StructField(\"SeniorCommunity\", StringType(), False), \\\n",
    "    StructField(\"CondoCoopAssoc\", StringType(), True), \\\n",
    "    StructField(\"HOA\", StringType(), True), \\\n",
    "    StructField(\"OneTimeAssociationFee\", StringType(), True), \\\n",
    "    StructField(\"AssociationFee\", StringType(), True), \\\n",
    "    StructField(\"AssociationFeeFrequency\", StringType(), True), \\\n",
    "    StructField(\"Age\", StringType(), True), \\\n",
    "    StructField(\"InteriorSqFt\", StringType(), True), \\\n",
    "    StructField(\"PropertyCondition\", StringType(), True), \\\n",
    "    StructField(\"Bedrooms\", StringType(), True), \\\n",
    "    StructField(\"BathsFull\", StringType(), True), \\\n",
    "    StructField(\"BathsHalf\", StringType(), True), \\\n",
    "    StructField(\"Design\", StringType(), True), \\\n",
    "    StructField(\"Style\", StringType(), True), \\\n",
    "    StructField(\"NumberofStories\", StringType(), True), \\\n",
    "    StructField(\"FloorNumber\", StringType(), True), \\\n",
    "    StructField(\"Basement\", StringType(), True), \\\n",
    "    StructField(\"GarageSpaces\", StringType(), True), \\\n",
    "    StructField(\"Fireplace\", StringType(), True), \\\n",
    "    StructField(\"Laundry\", StringType(), True), \\\n",
    "    StructField(\"OtherRooms\", StringType(), True), \\\n",
    "    StructField(\"RoomCount\", StringType(), True), \\\n",
    "    StructField(\"CentralAir\", StringType(), True), \\\n",
    "    StructField(\"Waterfront\", StringType(), True), \\\n",
    "    StructField(\"NewConstruction\", StringType(), True), \\\n",
    "    StructField(\"ModelName\", StringType(), True), \\\n",
    "    StructField(\"BuyerBrokerCompensation\", DoubleType(), True), \\\n",
    "    StructField(\"SubAgentCompensation\", DoubleType(), True), \\\n",
    "    StructField(\"TransactionBrokerCompensation\", DoubleType(), True), \\\n",
    "    StructField(\"OriginatingMLS\", StringType(), True), \\\n",
    "    StructField(\"AboveGradeSqFt\", StringType(), True), \\\n",
    "    StructField(\"BelowGradeSqFt\", StringType(), True), \\\n",
    "    StructField(\"HomeBuilt\", StringType(), True), \\\n",
    "    StructField(\"BasementFootprintPct\", StringType(), True), \\\n",
    "    StructField(\"BasementFinishedPct\", StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data\n",
    "df = spark.read.csv(\"../CleanedData/cleaned_data.csv\", sep=\",\", schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the columns that came in as strings using dollars to integers\n",
    "df = df.withColumn('OriginalPrice', f.regexp_replace('OriginalPrice', '[$,]', '').cast('integer'))\n",
    "df = df.withColumn('ListPrice', f.regexp_replace('ListPrice', '[$,]', '').cast('integer'))\n",
    "df = df.withColumn('SoldPrice', f.regexp_replace('SoldPrice', '[$,]', '').cast('integer'))\n",
    "df = df.withColumn('SellerConcessionsAmount', f.regexp_replace('SellerConcessionsAmount', '[$,]', '').cast('integer'))\n",
    "df = df.withColumn('StreetNumber', df['StreetNumber'].cast('integer'))\n",
    "df = df.withColumn('UnitNumber', df['UnitNumber'].cast('integer'))\n",
    "df = df.withColumn('SellingAgentCode', df['SellingAgentCode'].cast('integer'))\n",
    "df = df.withColumn('TaxAnnualTotal', df['TaxAnnualTotal'].cast('integer'))\n",
    "df = df.withColumn('TaxYear', df['TaxYear'].cast('integer'))\n",
    "df = df.withColumn('LandUseCode', df['LandUseCode'].cast('integer'))\n",
    "df = df.withColumn('AssociationFee', df['AssociationFee'].cast('integer'))\n",
    "df = df.withColumn('Age', df['Age'].cast('integer'))\n",
    "df = df.withColumn('InteriorSqFt', df['InteriorSqFt'].cast('integer'))\n",
    "df = df.withColumn('Bedrooms', df['Bedrooms'].cast('integer'))\n",
    "df = df.withColumn('BathsFull', df['BathsFull'].cast('integer'))\n",
    "df = df.withColumn('BathsHalf', df['BathsHalf'].cast('integer'))\n",
    "df = df.withColumn('GarageSpaces', df['GarageSpaces'].cast('integer'))\n",
    "df = df.withColumn('RoomCount', df['RoomCount'].cast('integer'))\n",
    "df = df.withColumn('AboveGradeSqFt', df['AboveGradeSqFt'].cast('integer'))\n",
    "df = df.withColumn('BelowGradeSqFt', df['BelowGradeSqFt'].cast('integer'))\n",
    "df = df.withColumn('BasementFootprintPct', df['BasementFootprintPct'].cast('integer'))\n",
    "df = df.withColumn('BasementFinishedPct', df['BasementFinishedPct'].cast('integer'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90758"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the dataframe\n",
    "dfCache = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpdForLinearRegression = dfCache.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def oneHotEncode(dfToEncode, columnToEncode):\n",
    "    # Get one hot encoding of columns provided\n",
    "    one_hot = pd.get_dummies(dfToEncode[columnToEncode])\n",
    "    # Drop column provided as it is now encoded\n",
    "    dfToEncode = dfToEncode.drop(columnToEncode, axis = 1)\n",
    "    # Join the encoded df\n",
    "    return dfToEncode.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step will be to clean the categorical data into usable fields for regression - needs to be quantitative\n",
    "\n",
    "dfpdForLinearRegression = dfpdForLinearRegression[['SoldPrice', 'SettledDate', 'ZipCode', 'County', 'AcresTotal', \n",
    "                                                   'Age', 'InteriorSqFt', 'Bedrooms', \n",
    "                                                   'BathsFull', 'BathsHalf', 'Style', 'Basement', 'GarageSpaces', \n",
    "                                                   'Fireplace', 'CentralAir', 'Waterfront', 'NewConstruction']]\n",
    "\n",
    "dfpdForLinearRegression['SettledDate'] = pd.to_datetime(dfpdForLinearRegression['SettledDate'])\n",
    "dfpdForLinearRegression['SettledDate'] = dfpdForLinearRegression['SettledDate'].map(dt.datetime.toordinal)\n",
    "\n",
    "dfpdForLinearRegression = oneHotEncode(dfpdForLinearRegression, 'County')\n",
    "dfpdForLinearRegression = oneHotEncode(dfpdForLinearRegression, 'Style')\n",
    "\n",
    "# Get one hot encoding of columns Basement\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegression['Basement'])\n",
    "one_hot.rename(columns = {'No':'NoBasement', 'Yes':'HasBasement'}, inplace = True)\n",
    "# Drop column Basement as it is now encoded\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.drop('Basement', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns Fireplace\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegression['Fireplace'])\n",
    "one_hot.rename(columns = {'No':'NoFireplace', 'Yes':'HasFireplace'}, inplace = True)\n",
    "# Drop column Fireplace as it is now encoded\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.drop('Fireplace', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns CentralAir\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegression['CentralAir'])\n",
    "one_hot.rename(columns = {'No':'NoCentralAir', 'Yes':'HasCentralAir'}, inplace = True)\n",
    "# Drop column CentralAir as it is now encoded\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.drop('CentralAir', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns Waterfront\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegression['Waterfront'])\n",
    "one_hot.rename(columns = {'No':'NotWaterfront', 'Yes':'IsWaterfront'}, inplace = True)\n",
    "# Drop column Waterfront as it is now encoded\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.drop('Waterfront', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns NewConstruction\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegression['NewConstruction'])\n",
    "one_hot.rename(columns = {'No':'NotNewConstruction', 'Yes':'IsNewConstruction'}, inplace = True)\n",
    "# Drop column NewConstruction as it is now encoded\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.drop('NewConstruction', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.join(one_hot)\n",
    "\n",
    "dfpdForLinearRegression[\"BathsHalf\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegression[\"BathsFull\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegression[\"Bedrooms\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegression[\"AcresTotal\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegression[\"Bedrooms\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegression[\"GarageSpaces\"].fillna(0, inplace = True)\n",
    "\n",
    "dfpdForLinearRegression = dfpdForLinearRegression.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same thing, except without style since it seems to be insignificant\n",
    "\n",
    "dfpdForLinearRegressionNoStyle = dfCache.toPandas()\n",
    "\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle[['SoldPrice', 'SettledDate', 'ZipCode', 'County', \n",
    "                                                    'AcresTotal', 'Age', 'InteriorSqFt', 'Bedrooms', \n",
    "                                                   'BathsFull', 'BathsHalf', 'Basement', 'GarageSpaces', \n",
    "                                                   'Fireplace', 'CentralAir', 'Waterfront', 'NewConstruction']]\n",
    "\n",
    "dfpdForLinearRegressionNoStyle['SettledDate'] = pd.to_datetime(dfpdForLinearRegressionNoStyle['SettledDate'])\n",
    "dfpdForLinearRegressionNoStyle['SettledDate'] = dfpdForLinearRegressionNoStyle['SettledDate'].map(dt.datetime.toordinal)\n",
    "\n",
    "dfpdForLinearRegressionNoStyle = oneHotEncode(dfpdForLinearRegressionNoStyle, 'County')\n",
    "\n",
    "# Get one hot encoding of columns Basement\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegressionNoStyle['Basement'])\n",
    "one_hot.rename(columns = {'No':'NoBasement', 'Yes':'HasBasement'}, inplace = True)\n",
    "# Drop column Basement as it is now encoded\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.drop('Basement', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns Fireplace\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegressionNoStyle['Fireplace'])\n",
    "one_hot.rename(columns = {'No':'NoFireplace', 'Yes':'HasFireplace'}, inplace = True)\n",
    "# Drop column Fireplace as it is now encoded\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.drop('Fireplace', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns CentralAir\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegressionNoStyle['CentralAir'])\n",
    "one_hot.rename(columns = {'No':'NoCentralAir', 'Yes':'HasCentralAir'}, inplace = True)\n",
    "# Drop column CentralAir as it is now encoded\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.drop('CentralAir', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns Waterfront\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegressionNoStyle['Waterfront'])\n",
    "one_hot.rename(columns = {'No':'NotWaterfront', 'Yes':'IsWaterfront'}, inplace = True)\n",
    "# Drop column Waterfront as it is now encoded\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.drop('Waterfront', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.join(one_hot)\n",
    "\n",
    "# Get one hot encoding of columns NewConstruction\n",
    "one_hot = pd.get_dummies(dfpdForLinearRegressionNoStyle['NewConstruction'])\n",
    "one_hot.rename(columns = {'No':'NotNewConstruction', 'Yes':'IsNewConstruction'}, inplace = True)\n",
    "# Drop column NewConstruction as it is now encoded\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.drop('NewConstruction', axis = 1)\n",
    "# Join the encoded df\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.join(one_hot)\n",
    "\n",
    "dfpdForLinearRegressionNoStyle[\"BathsHalf\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegressionNoStyle[\"BathsFull\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegressionNoStyle[\"Bedrooms\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegressionNoStyle[\"AcresTotal\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegressionNoStyle[\"Bedrooms\"].fillna(0, inplace = True)\n",
    "dfpdForLinearRegressionNoStyle[\"GarageSpaces\"].fillna(0, inplace = True)\n",
    "\n",
    "dfpdForLinearRegressionNoStyle = dfpdForLinearRegressionNoStyle.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61000, 686)\n",
      "(15251, 686)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(dfpdForLinearRegression, test_size = 0.2, random_state = 2) # splits the data into two parts with 1:4 ratio\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SoldPrice   R-squared:                       0.638\n",
      "Model:                            OLS   Adj. R-squared:                  0.635\n",
      "Method:                 Least Squares   F-statistic:                     183.0\n",
      "Date:                Tue, 17 May 2022   Prob (F-statistic):               0.00\n",
      "Time:                        20:40:57   Log-Likelihood:            -8.2336e+05\n",
      "No. Observations:               61000   AIC:                         1.648e+06\n",
      "Df Residuals:                   60417   BIC:                         1.653e+06\n",
      "Df Model:                         582                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "const                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.708e+07   3.57e+05    -47.844      0.000   -1.78e+07   -1.64e+07\n",
      "SettledDate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            72.3162      1.567     46.156      0.000      69.245      75.387\n",
      "ZipCode                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               118.4917      5.052     23.455      0.000     108.590     128.393\n",
      "AcresTotal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1.785e+04    270.738     65.919      0.000    1.73e+04    1.84e+04\n",
      "Age                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -28.8306      6.129     -4.704      0.000     -40.843     -16.818\n",
      "InteriorSqFt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.4766      0.119     12.376      0.000       1.243       1.710\n",
      "Bedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1.954e+04   1071.848     18.232      0.000    1.74e+04    2.16e+04\n",
      "BathsFull                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.238e+05   1078.793    114.712      0.000    1.22e+05    1.26e+05\n",
      "BathsHalf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            9.278e+04   1488.958     62.310      0.000    8.99e+04    9.57e+04\n",
      "GarageSpaces                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2625.6681    210.753     12.458      0.000    2212.591    3038.745\n",
      "ANNEARUNDELMD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -4.223e+06   8.93e+04    -47.303      0.000    -4.4e+06   -4.05e+06\n",
      "BALTIMOREMD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -4.326e+06   8.93e+04    -48.440      0.000    -4.5e+06   -4.15e+06\n",
      "HARFORDMD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -4.337e+06   8.92e+04    -48.603      0.000   -4.51e+06   -4.16e+06\n",
      "HOWARDMD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -4.193e+06   8.92e+04    -47.004      0.000   -4.37e+06   -4.02e+06\n",
      "AFrame                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -9.728e+04   2.04e+04     -4.780      0.000   -1.37e+05   -5.74e+04\n",
      "AFrame,AirLite,ArtDeco,BacktoBack,BeauxArts,Bilevel,Bungalow,CabinLodge,CapeCod,CarriageHouse,Chalet,Coastal,Colonial,Contemporary,ConvertedBarn,ConvertedDwelling,Cottage,Craftsman,Dome,Dutch,DwellingwSeparateLivingArea,FarmhouseNationalFolk,Federal,French,Georgian,Loft,LoftwithBedrooms,LogHome,Manor,Mediterranean,PreFabricated,Normandy,Other,PostAndBeam,Prairie,RaisedRanch,Rancher,Reverse,SaltBox,SidebySide,Spanish,SplitFoyer,SplitLevel,StraightThru,Traditional,Transitional,Trinity,Tudor,Condo    -0.0016      0.001     -1.172      0.241      -0.004       0.001\n",
      "AFrame,Bilevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.016e+05   1.77e+05     -0.575      0.565   -4.48e+05    2.45e+05\n",
      "AFrame,Bungalow,CapeCod,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -2.92e+05   1.77e+05     -1.652      0.099   -6.38e+05    5.45e+04\n",
      "AFrame,CabinLodge,Chalet,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -9.988e+04   1.77e+05     -0.565      0.572   -4.46e+05    2.47e+05\n",
      "AFrame,CabinLodge,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            6.935e+04   1.77e+05      0.392      0.695   -2.77e+05    4.16e+05\n",
      "AFrame,CapeCod                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.842e+05   1.77e+05     -1.042      0.297   -5.31e+05    1.62e+05\n",
      "AFrame,CapeCod,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1.349e+05   1.25e+05     -1.078      0.281    -3.8e+05     1.1e+05\n",
      "AFrame,CapeCod,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -6.12e+04   1.77e+05     -0.346      0.729   -4.08e+05    2.85e+05\n",
      "AFrame,Chalet                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -2.966e+04   1.77e+05     -0.168      0.867   -3.76e+05    3.17e+05\n",
      "AFrame,Coastal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0015      0.002      0.788      0.430      -0.002       0.005\n",
      "AFrame,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1.035e+04   1.77e+05      0.059      0.953   -3.36e+05    3.57e+05\n",
      "AFrame,Colonial,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -0.0030      0.002     -1.204      0.229      -0.008       0.002\n",
      "AFrame,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.135e+05   1.02e+05     -1.111      0.267   -3.14e+05    8.68e+04\n",
      "AFrame,Contemporary,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -2.708e+05   1.77e+05     -1.532      0.126   -6.17e+05    7.57e+04\n",
      "AFrame,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.539e+05   1.25e+05     -2.030      0.042   -4.99e+05   -8730.369\n",
      "AFrame,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -934.5482   8.86e+04     -0.011      0.992   -1.75e+05    1.73e+05\n",
      "AFrame,PreFabricated,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.717e+05   1.77e+05     -1.537      0.124   -6.18e+05    7.48e+04\n",
      "AFrame,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -0.0005      0.002     -0.200      0.841      -0.005       0.004\n",
      "AFrame,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -4.605e+04   8.85e+04     -0.520      0.603    -2.2e+05    1.27e+05\n",
      "AFrame,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -0.0026      0.003     -0.974      0.330      -0.008       0.003\n",
      "AFrame,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -1.834e+05   1.77e+05     -1.038      0.299    -5.3e+05    1.63e+05\n",
      "AFrame,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -1.47e+05   1.25e+05     -1.175      0.240   -3.92e+05    9.82e+04\n",
      "AFrame,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -0.0039      0.002     -1.627      0.104      -0.009       0.001\n",
      "AirLite,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.194e+05   1.77e+05     -0.675      0.500   -4.66e+05    2.27e+05\n",
      "ArtDeco                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1.035e+05   8.86e+04      1.168      0.243   -7.01e+04    2.77e+05\n",
      "ArtDeco,BeauxArts,Colonial,Craftsman,Dutch,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                     3.779e+04   1.77e+05      0.214      0.831   -3.09e+05    3.84e+05\n",
      "ArtDeco,BeauxArts,Cottage,French,Normandy                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1.27e+05   1.77e+05      0.718      0.472   -2.19e+05    4.73e+05\n",
      "ArtDeco,Bilevel,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -0.0030      0.003     -1.107      0.268      -0.008       0.002\n",
      "ArtDeco,Bungalow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     5.244e+04   1.77e+05      0.297      0.767   -2.94e+05    3.99e+05\n",
      "ArtDeco,Bungalow,Cottage,Craftsman,RaisedRanch,Rancher,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                        -4.366e+04   1.77e+05     -0.247      0.805    -3.9e+05    3.03e+05\n",
      "ArtDeco,CapeCod,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.233e+05   1.77e+05     -1.263      0.206    -5.7e+05    1.23e+05\n",
      "ArtDeco,CapeCod,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -2.185e+05   1.77e+05     -1.236      0.216   -5.65e+05    1.28e+05\n",
      "ArtDeco,Colonial,Contemporary,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -5.62e+04   1.77e+05     -0.318      0.751   -4.03e+05     2.9e+05\n",
      "ArtDeco,Craftsman,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.813e+05   1.77e+05     -1.026      0.305   -5.28e+05    1.65e+05\n",
      "ArtDeco,Mediterranean,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          5.211e+05   1.77e+05      2.948      0.003    1.75e+05    8.68e+05\n",
      "ArtDeco,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -6.593e+04   1.77e+05     -0.373      0.709   -4.12e+05    2.81e+05\n",
      "ArtDeco,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -2.703e+05   1.77e+05     -1.529      0.126   -6.17e+05    7.62e+04\n",
      "BacktoBack,DwellingwSeparateLivingArea,Rancher,SidebySide                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.204e+05   1.25e+05     -0.962      0.336   -3.66e+05    1.25e+05\n",
      "BeauxArts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            3.487e+05   1.25e+05      2.788      0.005    1.04e+05    5.94e+05\n",
      "BeauxArts,Bungalow,CapeCod,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -0.0042      0.002     -1.938      0.053      -0.008    4.75e-05\n",
      "BeauxArts,CapeCod,Contemporary,ConvertedDwelling,Mediterranean,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                          -3.853e+04   1.77e+05     -0.218      0.827   -3.85e+05    3.08e+05\n",
      "BeauxArts,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.815e+05   1.77e+05     -1.027      0.305   -5.28e+05    1.65e+05\n",
      "BeauxArts,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -0.0003      0.002     -0.104      0.918      -0.005       0.005\n",
      "BeauxArts,Normandy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.0027      0.003      1.076      0.282      -0.002       0.008\n",
      "Bilevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.019e+05    1.2e+04     -8.462      0.000   -1.26e+05   -7.83e+04\n",
      "Bilevel,Bungalow,Rancher,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  3384.8251   1.77e+05      0.019      0.985   -3.43e+05     3.5e+05\n",
      "Bilevel,CapeCod                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.0032      0.002      1.474      0.140      -0.001       0.007\n",
      "Bilevel,Chalet,Colonial,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -2.114e+05   1.77e+05     -1.196      0.232   -5.58e+05    1.35e+05\n",
      "Bilevel,Coastal,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.636e+05   1.77e+05     -0.925      0.355    -5.1e+05    1.83e+05\n",
      "Bilevel,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.963e+05   6.28e+04     -3.127      0.002   -3.19e+05   -7.33e+04\n",
      "Bilevel,Colonial,Contemporary,Cottage,Craftsman,French,RaisedRanch,Rancher,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                               -1.585e+05   1.77e+05     -0.897      0.370   -5.05e+05    1.88e+05\n",
      "Bilevel,Colonial,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.237e+05   1.77e+05     -0.700      0.484    -4.7e+05    2.23e+05\n",
      "Bilevel,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -8.717e+04   7.24e+04     -1.204      0.229   -2.29e+05    5.47e+04\n",
      "Bilevel,Contemporary,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1.848e+05   1.77e+05      1.045      0.296   -1.62e+05    5.31e+05\n",
      "Bilevel,Contemporary,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    4.597e+05   1.77e+05      2.600      0.009    1.13e+05    8.06e+05\n",
      "Bilevel,ConvertedDwelling                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.179e+05   1.77e+05      0.667      0.505   -2.29e+05    4.64e+05\n",
      "Bilevel,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -6.933e+04   1.25e+05     -0.554      0.579   -3.14e+05    1.76e+05\n",
      "Bilevel,LoftwithBedrooms,Other,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -2.623e+05   1.77e+05     -1.484      0.138   -6.09e+05    8.42e+04\n",
      "Bilevel,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1.167e+05   1.77e+05      0.660      0.509    -2.3e+05    4.63e+05\n",
      "Bilevel,Other,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -6.813e+04   1.77e+05     -0.385      0.700   -4.15e+05    2.78e+05\n",
      "Bilevel,Other,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -0.0014      0.002     -0.681      0.496      -0.005       0.003\n",
      "Bilevel,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -8.221e+04   1.77e+05     -0.465      0.642   -4.29e+05    2.64e+05\n",
      "Bilevel,RaisedRanch,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.007e+05   1.25e+05     -0.805      0.421   -3.46e+05    1.44e+05\n",
      "Bilevel,RaisedRanch,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -8.801e+04   1.02e+05     -0.861      0.389   -2.88e+05    1.12e+05\n",
      "Bilevel,RaisedRanch,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -3.405e+04   1.77e+05     -0.193      0.847   -3.81e+05    3.12e+05\n",
      "Bilevel,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -6.997e+04   8.85e+04     -0.790      0.429   -2.44e+05    1.04e+05\n",
      "Bilevel,Reverse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -2.283e+05   1.77e+05     -1.291      0.197   -5.75e+05    1.18e+05\n",
      "Bilevel,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.279e+05   4.21e+04     -3.038      0.002    -2.1e+05   -4.54e+04\n",
      "Bilevel,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -6.76e+04   7.24e+04     -0.934      0.350   -2.09e+05    7.43e+04\n",
      "Bilevel,SplitFoyer,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -9.383e+04   1.77e+05     -0.531      0.596    -4.4e+05    2.53e+05\n",
      "Bilevel,SplitFoyer,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.317e+05   1.77e+05     -0.745      0.456   -4.78e+05    2.15e+05\n",
      "Bilevel,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.086e+05   3.52e+04     -3.086      0.002   -1.78e+05   -3.96e+04\n",
      "Bilevel,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.665e+05   1.77e+05     -0.942      0.346   -5.13e+05     1.8e+05\n",
      "Bilevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -9.881e+04   1.25e+05     -0.790      0.430   -3.44e+05    1.46e+05\n",
      "Bilevel,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.0006      0.002     -0.329      0.742      -0.004       0.003\n",
      "Bungalow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -9.443e+04   8827.551    -10.697      0.000   -1.12e+05   -7.71e+04\n",
      "Bungalow,CabinLodge                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.0017      0.002      0.802      0.422      -0.002       0.006\n",
      "Bungalow,CabinLodge,CapeCod,CarriageHouse                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -2.751e+05   1.77e+05     -1.556      0.120   -6.22e+05    7.14e+04\n",
      "Bungalow,CabinLodge,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.491e+05   1.77e+05     -0.843      0.399   -4.96e+05    1.97e+05\n",
      "Bungalow,CabinLodge,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          3.995e+04   1.77e+05      0.226      0.821   -3.07e+05    3.86e+05\n",
      "Bungalow,CapeCod                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.304e+05    2.9e+04     -4.500      0.000   -1.87e+05   -7.36e+04\n",
      "Bungalow,CapeCod,Coastal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -0.0034      0.002     -1.869      0.062      -0.007       0.000\n",
      "Bungalow,CapeCod,Coastal,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.547e+04   1.77e+05     -0.088      0.930   -3.62e+05    3.31e+05\n",
      "Bungalow,CapeCod,Coastal,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -2.906e+05   1.77e+05     -1.644      0.100   -6.37e+05    5.59e+04\n",
      "Bungalow,CapeCod,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -7.549e+04   1.25e+05     -0.603      0.546   -3.21e+05     1.7e+05\n",
      "Bungalow,CapeCod,Colonial,Contemporary,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -2.605e+05   1.77e+05     -1.474      0.140   -6.07e+05    8.59e+04\n",
      "Bungalow,CapeCod,Colonial,Contemporary,Cottage,Craftsman,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                       4.547e+04   1.77e+05      0.257      0.797   -3.01e+05    3.92e+05\n",
      "Bungalow,CapeCod,Colonial,Contemporary,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -0.0033      0.002     -1.768      0.077      -0.007       0.000\n",
      "Bungalow,CapeCod,Colonial,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -9.593e+04   1.77e+05     -0.543      0.587   -4.42e+05    2.51e+05\n",
      "Bungalow,CapeCod,Colonial,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.389e+05   1.77e+05     -0.786      0.432   -4.85e+05    2.08e+05\n",
      "Bungalow,CapeCod,ConvertedDwelling,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1.332e+05   1.77e+05      0.754      0.451   -2.13e+05     4.8e+05\n",
      "Bungalow,CapeCod,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -7.317e+04   6.71e+04     -1.091      0.275   -2.05e+05    5.83e+04\n",
      "Bungalow,CapeCod,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.967e+05   1.77e+05     -1.112      0.266   -5.43e+05     1.5e+05\n",
      "Bungalow,CapeCod,Cottage,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0058      0.002      2.999      0.003       0.002       0.010\n",
      "Bungalow,CapeCod,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2.069e+04   1.02e+05      0.202      0.840    -1.8e+05    2.21e+05\n",
      "Bungalow,CapeCod,Craftsman,Georgian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.299e+05   1.77e+05     -0.735      0.462   -4.76e+05    2.17e+05\n",
      "Bungalow,CapeCod,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -3.568e+04   1.77e+05     -0.202      0.840   -3.82e+05    3.11e+05\n",
      "Bungalow,CapeCod,RaisedRanch,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.763e+05   1.77e+05     -0.997      0.319   -5.23e+05     1.7e+05\n",
      "Bungalow,CapeCod,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.398e+05   1.77e+05     -0.791      0.429   -4.86e+05    2.07e+05\n",
      "Bungalow,CapeCod,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.349e+05   1.25e+05     -1.078      0.281    -3.8e+05     1.1e+05\n",
      "Bungalow,CarriageHouse,Colonial,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                         -2.695e+05   1.77e+05     -1.524      0.127   -6.16e+05     7.7e+04\n",
      "Bungalow,CarriageHouse,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -7.246e+04   1.77e+05     -0.410      0.682   -4.19e+05    2.74e+05\n",
      "Bungalow,Coastal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -2.73e+05   1.77e+05     -1.544      0.123    -6.2e+05    7.36e+04\n",
      "Bungalow,Coastal,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.602e+04   1.02e+05     -0.157      0.875   -2.16e+05    1.84e+05\n",
      "Bungalow,Coastal,Cottage,Craftsman,Mediterranean,Rancher,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0057      0.002      2.982      0.003       0.002       0.010\n",
      "Bungalow,Coastal,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.0004      0.003      0.152      0.879      -0.005       0.006\n",
      "Bungalow,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -7.027e+04   7.24e+04     -0.971      0.332   -2.12e+05    7.16e+04\n",
      "Bungalow,Colonial,Contemporary,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -0.0013      0.002     -0.639      0.523      -0.005       0.003\n",
      "Bungalow,Colonial,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.0031      0.002      1.357      0.175      -0.001       0.008\n",
      "Bungalow,Colonial,Cottage,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -0.0003      0.002     -0.203      0.839      -0.004       0.003\n",
      "Bungalow,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.285e+05   7.93e+04     -1.621      0.105   -2.84e+05    2.69e+04\n",
      "Bungalow,Contemporary,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.503e+05   1.77e+05     -0.850      0.395   -4.97e+05    1.96e+05\n",
      "Bungalow,Contemporary,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -6.345e+04   1.25e+05     -0.507      0.612   -3.09e+05    1.82e+05\n",
      "Bungalow,ConvertedDwelling,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4.665e+04   1.77e+05      0.264      0.792      -3e+05    3.93e+05\n",
      "Bungalow,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -3.955e+04   3.82e+04     -1.035      0.300   -1.14e+05    3.53e+04\n",
      "Bungalow,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -7.044e+04   1.77e+05     -0.398      0.690   -4.17e+05    2.76e+05\n",
      "Bungalow,Cottage,Craftsman,Other,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                -4.112e+04   1.77e+05     -0.233      0.816   -3.88e+05    3.05e+05\n",
      "Bungalow,Cottage,Craftsman,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.618e+04   1.77e+05     -0.092      0.927   -3.63e+05     3.3e+05\n",
      "Bungalow,Cottage,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -7.865e+04   1.77e+05     -0.445      0.656   -4.25e+05    2.68e+05\n",
      "Bungalow,Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -3.784e+04   6.71e+04     -0.564      0.573   -1.69e+05    9.36e+04\n",
      "Bungalow,Cottage,Rancher,Traditional,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -0.0017      0.002     -0.877      0.381      -0.006       0.002\n",
      "Bungalow,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.684e+04   1.77e+05     -0.152      0.879   -3.73e+05     3.2e+05\n",
      "Bungalow,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -4.78e+04   8.85e+04     -0.540      0.589   -2.21e+05    1.26e+05\n",
      "Bungalow,Craftsman,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             6803.6365   1.77e+05      0.038      0.969    -3.4e+05    3.53e+05\n",
      "Bungalow,Craftsman,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.828e+05   1.02e+05     -1.789      0.074   -3.83e+05    1.75e+04\n",
      "Bungalow,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -0.0003      0.002     -0.160      0.873      -0.004       0.003\n",
      "Bungalow,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -2.017e+05   1.77e+05     -1.141      0.254   -5.48e+05    1.45e+05\n",
      "Bungalow,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -8.158e+04   1.77e+05     -0.462      0.644   -4.28e+05    2.65e+05\n",
      "Bungalow,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -3.183e+04   1.77e+05     -0.180      0.857   -3.78e+05    3.15e+05\n",
      "Bungalow,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -9.375e+04   2.71e+04     -3.461      0.001   -1.47e+05   -4.07e+04\n",
      "Bungalow,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -0.0044      0.002     -1.952      0.051      -0.009    1.87e-05\n",
      "Bungalow,Rancher,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -3.092e+04   1.77e+05     -0.175      0.861   -3.77e+05    3.16e+05\n",
      "Bungalow,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.471e+05   1.77e+05     -0.832      0.405   -4.94e+05    1.99e+05\n",
      "Bungalow,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.0025      0.002      1.224      0.221      -0.002       0.007\n",
      "Bungalow,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.066e+05   5.62e+04     -1.896      0.058   -2.17e+05    3591.810\n",
      "CabinLodge                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -7.633e+04   3.29e+04     -2.321      0.020   -1.41e+05   -1.19e+04\n",
      "CabinLodge,CapeCod                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.0028      0.002      1.368      0.171      -0.001       0.007\n",
      "CabinLodge,CapeCod,CarriageHouse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.462e+05   1.77e+05     -0.827      0.408   -4.93e+05       2e+05\n",
      "CabinLodge,CarriageHouse,Coastal,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.336e+05   1.77e+05     -0.756      0.450    -4.8e+05    2.13e+05\n",
      "CabinLodge,Chalet,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1.914e+04   1.77e+05      0.108      0.914   -3.27e+05    3.66e+05\n",
      "CabinLodge,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -3.626e+05   1.25e+05     -2.899      0.004   -6.08e+05   -1.17e+05\n",
      "CabinLodge,Colonial,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.74e+05   1.77e+05     -0.984      0.325    -5.2e+05    1.72e+05\n",
      "CabinLodge,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -8916.9023   1.02e+05     -0.087      0.930   -2.09e+05    1.91e+05\n",
      "CabinLodge,Contemporary,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.092e+05   1.25e+05     -0.873      0.383   -3.54e+05    1.36e+05\n",
      "CabinLodge,Contemporary,Craftsman,Other,PostAndBeam,Prairie,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                    -0.0052      0.002     -2.743      0.006      -0.009      -0.001\n",
      "CabinLodge,Contemporary,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -9.216e+04   1.77e+05     -0.521      0.602   -4.39e+05    2.54e+05\n",
      "CabinLodge,Contemporary,Other,PostAndBeam,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -8.181e+04   1.77e+05     -0.463      0.643   -4.28e+05    2.65e+05\n",
      "CabinLodge,Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.18e+05   1.77e+05     -0.667      0.505   -4.65e+05    2.29e+05\n",
      "CabinLodge,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.334e+06    1.8e+05     -7.389      0.000   -1.69e+06    -9.8e+05\n",
      "CabinLodge,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -7.696e+04   7.93e+04     -0.971      0.332   -2.32e+05    7.84e+04\n",
      "CabinLodge,LogHome,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3.633e+04   1.77e+05      0.206      0.837    -3.1e+05    3.83e+05\n",
      "CabinLodge,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.624e+05   1.77e+05     -0.919      0.358   -5.09e+05    1.84e+05\n",
      "CabinLodge,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -0.0038      0.001     -2.582      0.010      -0.007      -0.001\n",
      "CapeCod                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.081e+05   6613.984    -16.350      0.000   -1.21e+05   -9.52e+04\n",
      "CapeCod,CarriageHouse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -3.079e+05   1.77e+05     -1.742      0.082   -6.54e+05    3.86e+04\n",
      "CapeCod,CarriageHouse,Colonial,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.638e+05   1.77e+05     -0.926      0.354    -5.1e+05    1.83e+05\n",
      "CapeCod,CarriageHouse,Colonial,Georgian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.0002      0.002      0.099      0.921      -0.003       0.004\n",
      "CapeCod,CarriageHouse,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.0032      0.002      1.525      0.127      -0.001       0.007\n",
      "CapeCod,Chalet                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -7.283e+05   1.77e+05     -4.119      0.000   -1.07e+06   -3.82e+05\n",
      "CapeCod,Coastal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.308e+05   1.77e+05     -0.740      0.459   -4.77e+05    2.16e+05\n",
      "CapeCod,Coastal,Colonial,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -4.376e+04   1.77e+05     -0.247      0.805    -3.9e+05    3.03e+05\n",
      "CapeCod,Coastal,Colonial,Cottage,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1.391e+05   1.77e+05      0.787      0.431   -2.07e+05    4.86e+05\n",
      "CapeCod,Coastal,Colonial,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   6.591e+05   1.77e+05      3.729      0.000    3.13e+05    1.01e+06\n",
      "CapeCod,Coastal,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.436e+05   1.77e+05     -1.378      0.168    -5.9e+05    1.03e+05\n",
      "CapeCod,Coastal,Contemporary,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.0015      0.002      0.770      0.441      -0.002       0.005\n",
      "CapeCod,Coastal,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3.622e+04   1.25e+05      0.290      0.772   -2.09e+05    2.81e+05\n",
      "CapeCod,Coastal,Cottage,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -2.823e+04   1.77e+05     -0.160      0.873   -3.75e+05    3.18e+05\n",
      "CapeCod,Coastal,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  3.448e+05   1.77e+05      1.951      0.051   -1668.723    6.91e+05\n",
      "CapeCod,Coastal,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1.54e+05   8.86e+04      1.738      0.082   -1.96e+04    3.28e+05\n",
      "CapeCod,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -9.885e+04   2.68e+04     -3.692      0.000   -1.51e+05   -4.64e+04\n",
      "CapeCod,Colonial,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -7.715e+04   1.77e+05     -0.436      0.663   -4.24e+05    2.69e+05\n",
      "CapeCod,Colonial,Contemporary,Cottage,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.95e+05   1.77e+05     -1.103      0.270   -5.41e+05    1.51e+05\n",
      "CapeCod,Colonial,Contemporary,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -2.18e+05   1.25e+05     -1.743      0.081   -4.63e+05    2.72e+04\n",
      "CapeCod,Colonial,Contemporary,DwellingwSeparateLivingArea,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.33e+05   1.77e+05     -0.752      0.452   -4.79e+05    2.13e+05\n",
      "CapeCod,Colonial,Cottage,Georgian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1.486e+05   1.77e+05      0.841      0.401   -1.98e+05    4.95e+05\n",
      "CapeCod,Colonial,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -8.025e+04   1.77e+05     -0.454      0.650   -4.27e+05    2.66e+05\n",
      "CapeCod,Colonial,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -6.124e+04   1.25e+05     -0.490      0.624   -3.06e+05    1.84e+05\n",
      "CapeCod,Colonial,Craftsman,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.0058      0.002      2.568      0.010       0.001       0.010\n",
      "CapeCod,Colonial,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -0.0033      0.002     -1.470      0.142      -0.008       0.001\n",
      "CapeCod,Colonial,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -5.067e+05   1.77e+05     -2.866      0.004   -8.53e+05    -1.6e+05\n",
      "CapeCod,Colonial,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.216e+05   1.77e+05     -0.688      0.491   -4.68e+05    2.25e+05\n",
      "CapeCod,Colonial,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.305e+05   1.77e+05     -0.738      0.460   -4.77e+05    2.16e+05\n",
      "CapeCod,Colonial,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -2.772e+05   1.77e+05     -1.568      0.117   -6.24e+05    6.92e+04\n",
      "CapeCod,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.41e+05   7.24e+04     -1.948      0.051   -2.83e+05     834.338\n",
      "CapeCod,Contemporary,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.129e+05   1.77e+05     -0.639      0.523   -4.59e+05    2.34e+05\n",
      "CapeCod,Contemporary,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.178e+05   1.77e+05     -0.667      0.505   -4.64e+05    2.29e+05\n",
      "CapeCod,Contemporary,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -2.79e+05   1.25e+05     -2.231      0.026   -5.24e+05   -3.39e+04\n",
      "CapeCod,Contemporary,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -5.572e+04   1.77e+05     -0.315      0.753   -4.02e+05    2.91e+05\n",
      "CapeCod,Contemporary,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.442e+05   1.77e+05     -0.816      0.415   -4.91e+05    2.02e+05\n",
      "CapeCod,Contemporary,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -2.351e+05   1.77e+05     -1.330      0.184   -5.82e+05    1.11e+05\n",
      "CapeCod,Contemporary,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.29e+05   1.77e+05     -0.730      0.466   -4.75e+05    2.17e+05\n",
      "CapeCod,ConvertedDwelling,DwellingwSeparateLivingArea,LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                               -2.05e+05   1.77e+05     -1.160      0.246   -5.51e+05    1.41e+05\n",
      "CapeCod,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -9.748e+04   3.91e+04     -2.496      0.013   -1.74e+05   -2.09e+04\n",
      "CapeCod,Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.416e+05   1.77e+05     -0.801      0.423   -4.88e+05    2.05e+05\n",
      "CapeCod,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1.919e+04   1.77e+05      0.109      0.914   -3.27e+05    3.66e+05\n",
      "CapeCod,Cottage,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1.336e+05   1.77e+05      0.756      0.450   -2.13e+05     4.8e+05\n",
      "CapeCod,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.14e+05   5.92e+04     -1.925      0.054    -2.3e+05    2070.805\n",
      "CapeCod,Craftsman,Dutch,Federal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -4.048e+05   1.77e+05     -2.290      0.022   -7.51e+05   -5.84e+04\n",
      "CapeCod,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -6.879e+04   1.77e+05     -0.389      0.697   -4.15e+05    2.78e+05\n",
      "CapeCod,Craftsman,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2.453e+05   1.77e+05      1.388      0.165   -1.01e+05    5.92e+05\n",
      "CapeCod,DwellingwSeparateLivingArea,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -2.255e+05   1.77e+05     -1.276      0.202   -5.72e+05    1.21e+05\n",
      "CapeCod,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.25e+05   1.25e+05     -0.999      0.318    -3.7e+05     1.2e+05\n",
      "CapeCod,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.057e+05   1.77e+05      0.598      0.550   -2.41e+05    4.52e+05\n",
      "CapeCod,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.095e+05   1.77e+05     -0.620      0.536   -4.56e+05    2.37e+05\n",
      "CapeCod,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -3.659e+04   7.93e+04     -0.462      0.644   -1.92e+05    1.19e+05\n",
      "CapeCod,LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.986e+05   1.77e+05     -1.121      0.262   -5.46e+05    1.49e+05\n",
      "CapeCod,Mediterranean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0022      0.002     -1.019      0.308      -0.006       0.002\n",
      "CapeCod,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.672e+05   5.92e+04     -2.824      0.005   -2.83e+05   -5.11e+04\n",
      "CapeCod,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -3.17e+04   1.77e+05     -0.179      0.858   -3.79e+05    3.16e+05\n",
      "CapeCod,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -5.949e+04   1.77e+05     -0.337      0.736   -4.06e+05    2.87e+05\n",
      "CapeCod,PreFabricated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.433e+05   1.77e+05     -0.811      0.417    -4.9e+05    2.03e+05\n",
      "CapeCod,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.0076      0.002      3.788      0.000       0.004       0.012\n",
      "CapeCod,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -2.105e+05   1.25e+05     -1.683      0.092   -4.56e+05    3.46e+04\n",
      "CapeCod,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.203e+05   4.46e+04     -2.697      0.007   -2.08e+05   -3.29e+04\n",
      "CapeCod,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -2.788e+05   1.77e+05     -1.577      0.115   -6.25e+05    6.77e+04\n",
      "CapeCod,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.523e+05   1.77e+05     -0.861      0.389   -4.99e+05    1.94e+05\n",
      "CapeCod,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -9656.5869   1.77e+05     -0.055      0.956   -3.56e+05    3.37e+05\n",
      "CapeCod,StraightThru,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.519e+05   1.77e+05     -0.859      0.390   -4.98e+05    1.95e+05\n",
      "CapeCod,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.108e+05    3.9e+04     -2.837      0.005   -1.87e+05   -3.42e+04\n",
      "CapeCod,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 4.737e+05   1.25e+05      3.787      0.000    2.29e+05    7.19e+05\n",
      "CapeCod,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.0033      0.002      1.858      0.063      -0.000       0.007\n",
      "CapeCod,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1.254e+05   1.77e+05      0.709      0.478   -2.21e+05    4.72e+05\n",
      "CarriageHouse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -5.758e+04      4e+04     -1.440      0.150   -1.36e+05    2.08e+04\n",
      "CarriageHouse,Coastal,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       4.918e+05   1.77e+05      2.782      0.005    1.45e+05    8.38e+05\n",
      "CarriageHouse,Coastal,Cottage,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.902e+05   1.77e+05      1.076      0.282   -1.56e+05    5.37e+05\n",
      "CarriageHouse,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.098e+05   1.02e+05     -1.074      0.283    -3.1e+05    9.05e+04\n",
      "CarriageHouse,Colonial,DwellingwSeparateLivingArea,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                       2.849e+04   1.77e+05      0.161      0.872   -3.18e+05    3.75e+05\n",
      "CarriageHouse,Colonial,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   5.273e+05   1.77e+05      2.983      0.003    1.81e+05    8.74e+05\n",
      "CarriageHouse,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -2.282e+05   1.77e+05     -1.291      0.197   -5.75e+05    1.18e+05\n",
      "CarriageHouse,ConvertedBarn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -0.0007      0.002     -0.430      0.667      -0.004       0.003\n",
      "CarriageHouse,ConvertedBarn,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0066      0.002     -3.126      0.002      -0.011      -0.002\n",
      "CarriageHouse,Federal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0009      0.002     -0.557      0.577      -0.004       0.002\n",
      "CarriageHouse,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0036      0.002     -1.993      0.046      -0.007   -5.91e-05\n",
      "CarriageHouse,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.758e+05   1.77e+05     -0.995      0.320   -5.22e+05    1.71e+05\n",
      "CarriageHouse,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.638e+05   1.77e+05     -0.927      0.354    -5.1e+05    1.83e+05\n",
      "CarriageHouse,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -0.0020      0.002     -0.962      0.336      -0.006       0.002\n",
      "Chalet                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -9.814e+04    4.1e+04     -2.394      0.017   -1.78e+05   -1.78e+04\n",
      "Chalet,Coastal,Colonial,Contemporary,Cottage,Craftsman,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                            -2.602e+04   1.77e+05     -0.147      0.883   -3.72e+05     3.2e+05\n",
      "Chalet,Coastal,Cottage,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                             6.038e+04   1.77e+05      0.342      0.733   -2.86e+05    4.07e+05\n",
      "Chalet,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2.369e+04   1.25e+05      0.189      0.850   -2.21e+05    2.69e+05\n",
      "Chalet,Colonial,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           19.6991   1.77e+05      0.000      1.000   -3.46e+05    3.46e+05\n",
      "Chalet,Colonial,Contemporary,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                               6505.3049   1.77e+05      0.037      0.971    -3.4e+05    3.53e+05\n",
      "Chalet,Colonial,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -8.224e+04   1.77e+05     -0.465      0.642   -4.29e+05    2.64e+05\n",
      "Chalet,Colonial,RaisedRanch,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.51e+05   1.77e+05     -0.854      0.393   -4.97e+05    1.95e+05\n",
      "Chalet,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  5.169e+04   1.77e+05      0.292      0.770   -2.95e+05    3.98e+05\n",
      "Chalet,Contemporary,Cottage,Traditional,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.325e+05   1.77e+05     -0.750      0.453   -4.79e+05    2.14e+05\n",
      "Chalet,Contemporary,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -6.443e+04   1.77e+05     -0.365      0.715   -4.11e+05    2.82e+05\n",
      "Chalet,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -7.587e+04   1.25e+05     -0.607      0.544   -3.21e+05    1.69e+05\n",
      "Coastal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1.792e+05   1.58e+04     11.319      0.000    1.48e+05     2.1e+05\n",
      "Coastal,Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     5.698e+04   3.19e+04      1.788      0.074   -5475.041    1.19e+05\n",
      "Coastal,Colonial,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -7.636e+04   8.86e+04     -0.862      0.389    -2.5e+05    9.72e+04\n",
      "Coastal,Colonial,Contemporary,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                              7.138e+04   1.77e+05      0.404      0.686   -2.75e+05    4.18e+05\n",
      "Coastal,Colonial,Contemporary,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.106e+05   1.77e+05     -0.626      0.532   -4.57e+05    2.36e+05\n",
      "Coastal,Colonial,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -3.661e+05   1.25e+05     -2.926      0.003   -6.11e+05   -1.21e+05\n",
      "Coastal,Colonial,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -7.452e+04   1.25e+05     -0.596      0.551    -3.2e+05    1.71e+05\n",
      "Coastal,Colonial,Craftsman,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1.188e+06   1.77e+05      6.717      0.000    8.41e+05    1.53e+06\n",
      "Coastal,Colonial,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.0045      0.002      2.061      0.039       0.000       0.009\n",
      "Coastal,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 3.649e+05   4.01e+04      9.105      0.000    2.86e+05    4.43e+05\n",
      "Coastal,Contemporary,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1.718e+06   1.25e+05     13.734      0.000    1.47e+06    1.96e+06\n",
      "Coastal,Contemporary,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1.413e+05   1.77e+05      0.799      0.424   -2.05e+05    4.88e+05\n",
      "Coastal,Contemporary,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1.686e+05   1.77e+05      0.954      0.340   -1.78e+05    5.15e+05\n",
      "Coastal,Contemporary,LoftwithBedrooms,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -2.634e+05   1.77e+05     -1.490      0.136    -6.1e+05    8.31e+04\n",
      "Coastal,Contemporary,MidCenturyModern                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0053      0.002     -2.982      0.003      -0.009      -0.002\n",
      "Coastal,Contemporary,Prairie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2.712e+05   1.77e+05      1.534      0.125   -7.53e+04    6.18e+05\n",
      "Coastal,Contemporary,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.624e+05   1.77e+05     -0.919      0.358   -5.09e+05    1.84e+05\n",
      "Coastal,Contemporary,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2.869e+05   1.25e+05      2.293      0.022    4.17e+04    5.32e+05\n",
      "Coastal,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4.681e+04   4.61e+04      1.016      0.310   -4.35e+04    1.37e+05\n",
      "Coastal,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -6.869e+04   1.25e+05     -0.549      0.583   -3.14e+05    1.76e+05\n",
      "Coastal,Cottage,Dutch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                5.525e+05   1.77e+05      3.126      0.002    2.06e+05    8.99e+05\n",
      "Coastal,Cottage,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1.318e+05   1.77e+05     -0.745      0.456   -4.78e+05    2.15e+05\n",
      "Coastal,Cottage,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -4.765e+04   1.77e+05     -0.270      0.788   -3.94e+05    2.99e+05\n",
      "Coastal,Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -2.174e+04   1.77e+05     -0.123      0.902   -3.68e+05    3.25e+05\n",
      "Coastal,Cottage,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -0.0014      0.002     -0.740      0.459      -0.005       0.002\n",
      "Coastal,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1.333e+04   1.25e+05     -0.107      0.915   -2.58e+05    2.32e+05\n",
      "Coastal,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2.181e+05   4.94e+04      4.415      0.000    1.21e+05    3.15e+05\n",
      "Coastal,Craftsman,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -2.55e+05   1.77e+05     -1.442      0.149   -6.02e+05    9.15e+04\n",
      "Coastal,Craftsman,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.898e+05   1.77e+05     -1.074      0.283   -5.36e+05    1.57e+05\n",
      "Coastal,Craftsman,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -5.505e+04   1.77e+05     -0.311      0.756   -4.02e+05    2.91e+05\n",
      "Coastal,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        3.356e+05   1.77e+05      1.898      0.058    -1.1e+04    6.82e+05\n",
      "Coastal,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       5.555e+05   1.77e+05      3.141      0.002    2.09e+05    9.02e+05\n",
      "Coastal,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.031e+05   1.77e+05     -0.583      0.560    -4.5e+05    2.43e+05\n",
      "Coastal,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      6.972e+04   1.77e+05      0.394      0.693   -2.77e+05    4.16e+05\n",
      "Coastal,MidCenturyModern                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1.393e+06   1.77e+05      7.878      0.000    1.05e+06    1.74e+06\n",
      "Coastal,Other,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.37e+04   1.77e+05     -0.078      0.938    -3.6e+05    3.33e+05\n",
      "Coastal,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            3.121e+05   1.25e+05      2.495      0.013    6.69e+04    5.57e+05\n",
      "Coastal,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -2.175e+05   1.77e+05     -1.231      0.218   -5.64e+05    1.29e+05\n",
      "Coastal,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -2.359e+05   1.25e+05     -1.886      0.059   -4.81e+05    9297.490\n",
      "Coastal,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          3.489e+04   1.77e+05      0.197      0.844   -3.12e+05    3.81e+05\n",
      "Coastal,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -2.019e+04   1.77e+05     -0.114      0.909   -3.67e+05    3.26e+05\n",
      "Coastal,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.617e+04   1.25e+05     -0.129      0.897   -2.61e+05    2.29e+05\n",
      "Coastal,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  4.902e+05   5.62e+04      8.714      0.000     3.8e+05       6e+05\n",
      "Coastal,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2.198e+05   7.24e+04      3.036      0.002    7.79e+04    3.62e+05\n",
      "Coastal,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    9.559e+05   1.77e+05      5.408      0.000    6.09e+05     1.3e+06\n",
      "Coastal,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         3.14e+05   1.77e+05      1.777      0.076   -3.24e+04     6.6e+05\n",
      "Colonial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -8.244e+04   6293.465    -13.099      0.000   -9.48e+04   -7.01e+04\n",
      "Colonial,Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.139e+05   1.57e+04     -7.236      0.000   -1.45e+05   -8.31e+04\n",
      "Colonial,Contemporary,ConvertedDwelling                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.524e+05   1.77e+05     -0.862      0.389   -4.99e+05    1.94e+05\n",
      "Colonial,Contemporary,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.67e+05   1.77e+05     -1.510      0.131   -6.14e+05    7.95e+04\n",
      "Colonial,Contemporary,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -2.632e+05   1.77e+05     -1.488      0.137    -6.1e+05    8.34e+04\n",
      "Colonial,Contemporary,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4.619e+04   5.92e+04      0.780      0.435   -6.99e+04    1.62e+05\n",
      "Colonial,Contemporary,Craftsman,LoftwithBedrooms,LogHome,MidCenturyModern,Other,PostAndBeam,RaisedRanch,Rancher,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                            -1.056e+05   1.77e+05     -0.597      0.550   -4.52e+05    2.41e+05\n",
      "Colonial,Contemporary,Craftsman,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.687e+05   1.77e+05     -0.954      0.340   -5.15e+05    1.78e+05\n",
      "Colonial,Contemporary,Craftsman,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2.523e+05   1.77e+05      1.427      0.153   -9.41e+04    5.99e+05\n",
      "Colonial,Contemporary,Craftsman,SplitFoyer,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                   -1.914e+05   1.25e+05     -1.530      0.126   -4.37e+05    5.38e+04\n",
      "Colonial,Contemporary,Craftsman,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -2.486e+05   1.77e+05     -1.406      0.160   -5.95e+05    9.79e+04\n",
      "Colonial,Contemporary,Dutch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -4.756e+04   1.77e+05     -0.269      0.788   -3.94e+05    2.99e+05\n",
      "Colonial,Contemporary,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2.736e+05   1.77e+05      1.548      0.122   -7.29e+04     6.2e+05\n",
      "Colonial,Contemporary,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -4613.9319   1.77e+05     -0.026      0.979   -3.51e+05    3.42e+05\n",
      "Colonial,Contemporary,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -0.0062      0.002     -3.486      0.000      -0.010      -0.003\n",
      "Colonial,Contemporary,Mediterranean,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.171e+05   1.77e+05     -0.663      0.508   -4.64e+05    2.29e+05\n",
      "Colonial,Contemporary,MidCenturyModern                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -4.674e+04   1.77e+05     -0.264      0.791   -3.93e+05       3e+05\n",
      "Colonial,Contemporary,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1567.5321   7.92e+04     -0.020      0.984   -1.57e+05    1.54e+05\n",
      "Colonial,Contemporary,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              2.567e+05   1.77e+05      1.452      0.147   -8.98e+04    6.03e+05\n",
      "Colonial,Contemporary,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -1.554e+05   1.77e+05     -0.879      0.379   -5.02e+05    1.91e+05\n",
      "Colonial,Contemporary,RaisedRanch,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.289e+05   1.77e+05     -0.729      0.466   -4.75e+05    2.18e+05\n",
      "Colonial,Contemporary,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1.98e+04   1.02e+05      0.194      0.846    -1.8e+05     2.2e+05\n",
      "Colonial,Contemporary,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -0.0048      0.002     -1.972      0.049      -0.010   -2.88e-05\n",
      "Colonial,Contemporary,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -2.164e+05   1.77e+05     -1.224      0.221   -5.63e+05     1.3e+05\n",
      "Colonial,Contemporary,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    8.249e+04   8.85e+04      0.932      0.352   -9.11e+04    2.56e+05\n",
      "Colonial,Contemporary,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1.949e+05   7.93e+04      2.459      0.014    3.96e+04     3.5e+05\n",
      "Colonial,Contemporary,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1702.9423   1.77e+05      0.010      0.992   -3.45e+05    3.48e+05\n",
      "Colonial,ConvertedBarn,LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -8.464e+05   1.77e+05     -4.784      0.000   -1.19e+06      -5e+05\n",
      "Colonial,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     8.435e+04   6.28e+04      1.343      0.179   -3.87e+04    2.07e+05\n",
      "Colonial,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           8.149e+04   1.77e+05      0.461      0.645   -2.65e+05    4.28e+05\n",
      "Colonial,Cottage,Craftsman,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                        -7131.5945   1.77e+05     -0.040      0.968   -3.54e+05    3.39e+05\n",
      "Colonial,Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -8.388e+04   1.02e+05     -0.821      0.412   -2.84e+05    1.16e+05\n",
      "Colonial,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -4.153e+04   1.51e+04     -2.746      0.006   -7.12e+04   -1.19e+04\n",
      "Colonial,Craftsman,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -5579.1962   1.25e+05     -0.045      0.964   -2.51e+05     2.4e+05\n",
      "Colonial,Craftsman,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                 5.384e+04   1.77e+05      0.305      0.761   -2.93e+05       4e+05\n",
      "Colonial,Craftsman,Federal,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -6.335e+04   1.77e+05     -0.358      0.720    -4.1e+05    2.83e+05\n",
      "Colonial,Craftsman,LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -4.897e+04   2.96e+04     -1.654      0.098   -1.07e+05    9065.020\n",
      "Colonial,Craftsman,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.38e+05   1.25e+05     -1.103      0.270   -3.83e+05    1.07e+05\n",
      "Colonial,Craftsman,Other,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.372e+05   1.77e+05     -0.776      0.438   -4.84e+05    2.09e+05\n",
      "Colonial,Craftsman,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2.06e+05   1.77e+05      1.165      0.244   -1.41e+05    5.53e+05\n",
      "Colonial,Craftsman,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -9.858e+04   1.77e+05     -0.558      0.577   -4.45e+05    2.48e+05\n",
      "Colonial,Craftsman,SaltBox,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.989e+05   1.77e+05     -1.125      0.261   -5.45e+05    1.48e+05\n",
      "Colonial,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -4.019e+04   4.33e+04     -0.927      0.354   -1.25e+05    4.48e+04\n",
      "Colonial,Craftsman,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      2.601e+04   1.02e+05      0.254      0.799   -1.74e+05    2.26e+05\n",
      "Colonial,Dutch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -7.695e+04   2.38e+04     -3.229      0.001   -1.24e+05   -3.02e+04\n",
      "Colonial,Dutch,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.0005      0.002      0.224      0.822      -0.004       0.005\n",
      "Colonial,Dutch,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -4.944e+04   1.25e+05     -0.395      0.693   -2.95e+05    1.96e+05\n",
      "Colonial,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -2.034e+05   6.28e+04     -3.240      0.001   -3.26e+05   -8.04e+04\n",
      "Colonial,DwellingwSeparateLivingArea,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -7.637e+04   1.77e+05     -0.432      0.666   -4.23e+05     2.7e+05\n",
      "Colonial,DwellingwSeparateLivingArea,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -2.203e+05   1.25e+05     -1.761      0.078   -4.66e+05    2.49e+04\n",
      "Colonial,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -6.96e+04   3.34e+04     -2.085      0.037   -1.35e+05   -4158.742\n",
      "Colonial,FarmhouseNationalFolk,Federal                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.307e+05   1.77e+05     -0.739      0.460   -4.77e+05    2.16e+05\n",
      "Colonial,FarmhouseNationalFolk,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.367e+04   1.77e+05     -0.077      0.938    -3.6e+05    3.33e+05\n",
      "Colonial,FarmhouseNationalFolk,Manor,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4.369e+05   1.77e+05      2.472      0.013    9.05e+04    7.83e+05\n",
      "Colonial,FarmhouseNationalFolk,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -5.585e+04   1.77e+05     -0.316      0.752   -4.02e+05    2.91e+05\n",
      "Colonial,FarmhouseNationalFolk,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.534e+05   1.77e+05     -0.868      0.386      -5e+05    1.93e+05\n",
      "Colonial,FarmhouseNationalFolk,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.251e+05   1.77e+05     -0.708      0.479   -4.72e+05    2.21e+05\n",
      "Colonial,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -5.627e+04   6.28e+04     -0.897      0.370   -1.79e+05    6.67e+04\n",
      "Colonial,FarmhouseNationalFolk,Traditional,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.131e+05   1.77e+05     -0.640      0.522    -4.6e+05    2.33e+05\n",
      "Colonial,FarmhouseNationalFolk,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -6.531e+04   1.02e+05     -0.639      0.523   -2.66e+05    1.35e+05\n",
      "Colonial,Federal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -6.565e+04   7.93e+04     -0.828      0.407   -2.21e+05    8.97e+04\n",
      "Colonial,Federal,Georgian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -0.0021      0.002     -1.012      0.312      -0.006       0.002\n",
      "Colonial,Federal,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.701e+04   1.25e+05     -0.136      0.892   -2.62e+05    2.28e+05\n",
      "Colonial,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3.28e+05   6.28e+04      5.225      0.000    2.05e+05    4.51e+05\n",
      "Colonial,French,Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0010      0.002     -0.592      0.554      -0.004       0.002\n",
      "Colonial,French,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1.669e+04   1.77e+05      0.094      0.925    -3.3e+05    3.63e+05\n",
      "Colonial,French,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -6.45e+04   1.77e+05     -0.365      0.715   -4.11e+05    2.82e+05\n",
      "Colonial,French,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.0047      0.002      2.171      0.030       0.000       0.009\n",
      "Colonial,Georgian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -2.096e+05   7.24e+04     -2.894      0.004   -3.52e+05   -6.77e+04\n",
      "Colonial,Georgian,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        8.951e+05   1.25e+05      7.156      0.000     6.5e+05    1.14e+06\n",
      "Colonial,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -2.403e+04    3.4e+04     -0.708      0.479   -9.06e+04    4.25e+04\n",
      "Colonial,Loft,LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0014      0.002      0.812      0.417      -0.002       0.005\n",
      "Colonial,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -0.0023      0.002     -0.948      0.343      -0.007       0.002\n",
      "Colonial,Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       9.023e+04   8.86e+04      1.019      0.308   -8.34e+04    2.64e+05\n",
      "Colonial,Manor,Other,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.352e+05   1.77e+05     -0.765      0.444   -4.82e+05    2.11e+05\n",
      "Colonial,Manor,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5126.9244   1.77e+05      0.029      0.977   -3.41e+05    3.52e+05\n",
      "Colonial,Mediterranean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               3.135e+05   1.77e+05      1.774      0.076    -3.3e+04     6.6e+05\n",
      "Colonial,Mediterranean,Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -2.782e+05   1.77e+05     -1.574      0.115   -6.25e+05    6.82e+04\n",
      "Colonial,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       4.244e+04   5.36e+04      0.792      0.429   -6.27e+04    1.48e+05\n",
      "Colonial,Other,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.996e+05   8.86e+04     -2.254      0.024   -3.73e+05    -2.6e+04\n",
      "Colonial,Other,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -9.414e+04   1.77e+05     -0.533      0.594   -4.41e+05    2.52e+05\n",
      "Colonial,Other,SplitFoyer,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.0010      0.002      0.592      0.554      -0.002       0.004\n",
      "Colonial,Other,SplitFoyer,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -6.926e+04   1.77e+05     -0.392      0.695   -4.16e+05    2.77e+05\n",
      "Colonial,Other,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.393e+05   1.77e+05     -0.788      0.431   -4.86e+05    2.07e+05\n",
      "Colonial,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -6.95e+04   1.02e+05     -0.680      0.496    -2.7e+05    1.31e+05\n",
      "Colonial,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  3.12e+05   1.02e+05      3.053      0.002    1.12e+05    5.12e+05\n",
      "Colonial,PreFabricated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -4.411e+05   1.77e+05     -2.495      0.013   -7.88e+05   -9.45e+04\n",
      "Colonial,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.0013      0.002     -0.602      0.547      -0.006       0.003\n",
      "Colonial,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.421e+05   4.76e+04     -2.984      0.003   -2.35e+05   -4.88e+04\n",
      "Colonial,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.186e+05   1.25e+05     -1.747      0.081   -4.64e+05    2.66e+04\n",
      "Colonial,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.155e+05   1.02e+05     -1.130      0.258   -3.16e+05    8.48e+04\n",
      "Colonial,SidebySide                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.095e+05   1.77e+05     -0.620      0.535   -4.56e+05    2.37e+05\n",
      "Colonial,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.754e+05   4.46e+04     -3.932      0.000   -2.63e+05   -8.79e+04\n",
      "Colonial,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -2.694e+05   1.77e+05     -1.524      0.128   -6.16e+05    7.71e+04\n",
      "Colonial,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.113e+05   3.82e+04     -2.915      0.004   -1.86e+05   -3.64e+04\n",
      "Colonial,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.218e+05   1.25e+05     -0.974      0.330   -3.67e+05    1.23e+05\n",
      "Colonial,StraightThru,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -8.479e+04   1.77e+05     -0.480      0.631   -4.31e+05    2.62e+05\n",
      "Colonial,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -4.316e+04   1.28e+04     -3.384      0.001   -6.81e+04   -1.82e+04\n",
      "Colonial,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -8.403e+04   3.66e+04     -2.296      0.022   -1.56e+05   -1.23e+04\n",
      "Colonial,Traditional,Transitional,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -2.011e+05   1.77e+05     -1.138      0.255   -5.47e+05    1.45e+05\n",
      "Colonial,Traditional,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1.359e+05   1.77e+05      0.769      0.442   -2.11e+05    4.82e+05\n",
      "Colonial,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4.908e+04   2.37e+04      2.071      0.038    2637.655    9.55e+04\n",
      "Colonial,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3269.9071   5.92e+04      0.055      0.956   -1.13e+05    1.19e+05\n",
      "Colonial,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.494e+05    4.6e+04     -3.245      0.001    -2.4e+05   -5.92e+04\n",
      "CondoUnit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.433e+05   5.14e+04     -2.790      0.005   -2.44e+05   -4.26e+04\n",
      "Contemporary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -3.928e+04   7395.353     -5.311      0.000   -5.38e+04   -2.48e+04\n",
      "Contemporary,ConvertedDwelling                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -8.287e+04   1.77e+05     -0.469      0.639   -4.29e+05    2.64e+05\n",
      "Contemporary,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -7.344e+04   1.77e+05     -0.415      0.678    -4.2e+05    2.73e+05\n",
      "Contemporary,Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1.774e+05   1.77e+05      1.004      0.316   -1.69e+05    5.24e+05\n",
      "Contemporary,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -3.068e+04   6.71e+04     -0.457      0.647   -1.62e+05    1.01e+05\n",
      "Contemporary,Craftsman,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -4363.6827   1.25e+05     -0.035      0.972    -2.5e+05    2.41e+05\n",
      "Contemporary,Craftsman,FarmhouseNationalFolk,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                       -0.0056      0.002     -2.539      0.011      -0.010      -0.001\n",
      "Contemporary,Craftsman,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.0037      0.002      1.584      0.113      -0.001       0.008\n",
      "Contemporary,Craftsman,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0026      0.001      2.045      0.041       0.000       0.005\n",
      "Contemporary,Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.276e+05   1.02e+05     -1.249      0.212   -3.28e+05    7.27e+04\n",
      "Contemporary,Craftsman,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1.984e+05   1.02e+05      1.941      0.052   -1971.176    3.99e+05\n",
      "Contemporary,Dome,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -0.0049      0.002     -2.351      0.019      -0.009      -0.001\n",
      "Contemporary,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -3.552e+05   1.77e+05     -2.009      0.045   -7.02e+05   -8664.772\n",
      "Contemporary,DwellingwSeparateLivingArea,Rancher,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.302e+04   1.77e+05     -0.074      0.941    -3.6e+05    3.34e+05\n",
      "Contemporary,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1.376e+05   1.25e+05      1.100      0.271   -1.08e+05    3.83e+05\n",
      "Contemporary,Federal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.308e+05   1.25e+05     -1.046      0.296   -3.76e+05    1.14e+05\n",
      "Contemporary,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  4.133e+05   1.77e+05      2.338      0.019    6.69e+04     7.6e+05\n",
      "Contemporary,French,Mediterranean,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                4.817e+04   1.77e+05      0.273      0.785   -2.98e+05    3.95e+05\n",
      "Contemporary,French,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -0.0011      0.001     -0.808      0.419      -0.004       0.002\n",
      "Contemporary,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -5.203e+04   4.46e+04     -1.167      0.243   -1.39e+05    3.54e+04\n",
      "Contemporary,Loft,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -0.0028      0.002     -1.751      0.080      -0.006       0.000\n",
      "Contemporary,LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        8.785e+04   1.77e+05      0.497      0.619   -2.59e+05    4.34e+05\n",
      "Contemporary,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -3.738e+05   1.25e+05     -2.988      0.003   -6.19e+05   -1.29e+05\n",
      "Contemporary,Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2.862e+05   1.77e+05      1.619      0.105   -6.03e+04    6.33e+05\n",
      "Contemporary,Mediterranean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           7.198e+05   1.77e+05      4.070      0.000    3.73e+05    1.07e+06\n",
      "Contemporary,Mediterranean,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1.467e+05   1.77e+05      0.830      0.406      -2e+05    4.93e+05\n",
      "Contemporary,MidCenturyModern                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        4615.4173   6.28e+04      0.074      0.941   -1.18e+05    1.28e+05\n",
      "Contemporary,MidCenturyModern,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -0.0006      0.002     -0.277      0.782      -0.005       0.004\n",
      "Contemporary,MidCenturyModern,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.542e+05   1.77e+05     -0.872      0.383   -5.01e+05    1.92e+05\n",
      "Contemporary,MidCenturyModern,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -0.0055      0.003     -2.043      0.041      -0.011      -0.000\n",
      "Contemporary,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2.735e+05   5.62e+04      4.866      0.000    1.63e+05    3.84e+05\n",
      "Contemporary,Other,RaisedRanch,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2.049e+04   1.77e+05      0.116      0.908   -3.26e+05    3.67e+05\n",
      "Contemporary,Other,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           3.447e+04   1.02e+05      0.337      0.736   -1.66e+05    2.35e+05\n",
      "Contemporary,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -4.519e+04   1.77e+05     -0.256      0.798   -3.92e+05    3.01e+05\n",
      "Contemporary,Other,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -4.626e+04   1.25e+05     -0.370      0.711   -2.91e+05    1.99e+05\n",
      "Contemporary,Other,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -3.944e+04   1.25e+05     -0.315      0.753   -2.85e+05    2.06e+05\n",
      "Contemporary,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             6.161e+04   7.24e+04      0.851      0.395   -8.03e+04    2.04e+05\n",
      "Contemporary,PostAndBeam,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  6.157e+04   1.25e+05      0.492      0.623   -1.84e+05    3.07e+05\n",
      "Contemporary,Prairie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -0.0010      0.001     -0.734      0.463      -0.004       0.002\n",
      "Contemporary,Prairie,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1.578e+05   1.77e+05      0.893      0.372   -1.89e+05    5.04e+05\n",
      "Contemporary,PreFabricated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.0036      0.002      1.894      0.058      -0.000       0.007\n",
      "Contemporary,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -6.679e+04   1.25e+05     -0.534      0.593   -3.12e+05    1.78e+05\n",
      "Contemporary,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -4.155e+04   1.77e+05     -0.235      0.814   -3.88e+05    3.05e+05\n",
      "Contemporary,RaisedRanch,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.11e+05   1.77e+05     -0.628      0.530   -4.58e+05    2.35e+05\n",
      "Contemporary,RaisedRanch,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.178e+05   1.02e+05     -1.153      0.249   -3.18e+05    8.25e+04\n",
      "Contemporary,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -5.838e+04   2.38e+04     -2.451      0.014   -1.05e+05   -1.17e+04\n",
      "Contemporary,Rancher,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -3.042e+05   1.77e+05     -1.720      0.085   -6.51e+05    4.23e+04\n",
      "Contemporary,Rancher,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -9.916e+04   1.25e+05     -0.793      0.428   -3.44e+05    1.46e+05\n",
      "Contemporary,Reverse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -2.539e+05   1.77e+05     -1.436      0.151      -6e+05    9.26e+04\n",
      "Contemporary,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.0003      0.003      0.117      0.907      -0.005       0.005\n",
      "Contemporary,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.092e+05    3.9e+04     -2.797      0.005   -1.86e+05   -3.27e+04\n",
      "Contemporary,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.618e+05   1.77e+05     -0.915      0.360   -5.08e+05    1.85e+05\n",
      "Contemporary,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.294e+05   3.09e+04     -4.184      0.000    -1.9e+05   -6.88e+04\n",
      "Contemporary,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -2.133e+05   1.77e+05     -1.207      0.228    -5.6e+05    1.33e+05\n",
      "Contemporary,SplitLevel,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -4.566e+04   1.25e+05     -0.365      0.715   -2.91e+05    1.99e+05\n",
      "Contemporary,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             3.899e+04   3.59e+04      1.087      0.277   -3.13e+04    1.09e+05\n",
      "Contemporary,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.35e+05   1.02e+05     -1.321      0.187   -3.35e+05    6.53e+04\n",
      "Contemporary,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            7.931e+04      4e+04      1.983      0.047     931.265    1.58e+05\n",
      "Contemporary,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    8.18e+04   1.77e+05      0.463      0.644   -2.65e+05    4.28e+05\n",
      "Contemporary,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -2.03e+05   1.77e+05     -1.149      0.251   -5.49e+05    1.43e+05\n",
      "Contemporary,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.408e+05   1.77e+05     -0.797      0.426   -4.87e+05    2.06e+05\n",
      "ConvertedBarn                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.017e+05   1.02e+05     -0.996      0.319   -3.02e+05    9.86e+04\n",
      "ConvertedBarn,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0.0033      0.002      2.028      0.043       0.000       0.007\n",
      "ConvertedBarn,Cottage,FarmhouseNationalFolk,Loft,Other,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.167e+05   1.77e+05     -0.660      0.509   -4.63e+05     2.3e+05\n",
      "ConvertedBarn,Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -8.37e+04   1.77e+05     -0.474      0.636    -4.3e+05    2.63e+05\n",
      "ConvertedBarn,DwellingwSeparateLivingArea,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                1.335e+05   1.77e+05      0.755      0.450   -2.13e+05     4.8e+05\n",
      "ConvertedDwelling                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -4.162e+05   1.77e+05     -2.354      0.019   -7.63e+05   -6.97e+04\n",
      "ConvertedDwelling,Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -6.277e+05   1.77e+05     -3.550      0.000   -9.74e+05   -2.81e+05\n",
      "ConvertedDwelling,Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -9.828e+04   1.77e+05     -0.556      0.578   -4.45e+05    2.48e+05\n",
      "ConvertedDwelling,DwellingwSeparateLivingArea,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.0050      0.001      3.581      0.000       0.002       0.008\n",
      "ConvertedDwelling,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3.328e+04   1.77e+05      0.188      0.851   -3.13e+05     3.8e+05\n",
      "ConvertedDwelling,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -3.045e+05   1.77e+05     -1.723      0.085   -6.51e+05    4.19e+04\n",
      "ConvertedDwelling,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1.403e+05   1.77e+05      0.794      0.427   -2.06e+05    4.87e+05\n",
      "Cottage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -4.292e+04   1.02e+04     -4.223      0.000   -6.28e+04    -2.3e+04\n",
      "Cottage,Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.211e+04   7.93e+04      0.405      0.685   -1.23e+05    1.87e+05\n",
      "Cottage,Craftsman,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -2.436e+05   1.77e+05     -1.378      0.168    -5.9e+05    1.03e+05\n",
      "Cottage,Craftsman,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -0.0009      0.003     -0.338      0.736      -0.006       0.005\n",
      "Cottage,Craftsman,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.0014      0.001      1.162      0.245      -0.001       0.004\n",
      "Cottage,DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   9.64e+04   1.25e+05      0.771      0.441   -1.49e+05    3.42e+05\n",
      "Cottage,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.0012      0.003      0.401      0.688      -0.005       0.007\n",
      "Cottage,FarmhouseNationalFolk,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -6.97e+05   1.77e+05     -3.932      0.000   -1.04e+06    -3.5e+05\n",
      "Cottage,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -7.201e+04   1.77e+05     -0.407      0.684   -4.18e+05    2.74e+05\n",
      "Cottage,FarmhouseNationalFolk,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                           3.054e+05   1.77e+05      1.727      0.084   -4.12e+04    6.52e+05\n",
      "Cottage,FarmhouseNationalFolk,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -7.165e+05   1.77e+05     -4.052      0.000   -1.06e+06    -3.7e+05\n",
      "Cottage,FarmhouseNationalFolk,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -0.0016      0.002     -0.896      0.371      -0.005       0.002\n",
      "Cottage,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3.491e+05   1.25e+05      2.791      0.005    1.04e+05    5.94e+05\n",
      "Cottage,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5287.7583   1.25e+05      0.042      0.966    -2.4e+05     2.5e+05\n",
      "Cottage,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -0.0014      0.001     -2.061      0.039      -0.003   -6.62e-05\n",
      "Cottage,Other,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -6.967e+04   1.77e+05     -0.394      0.694   -4.16e+05    2.77e+05\n",
      "Cottage,PreFabricated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -3.1e+04   1.77e+05     -0.175      0.861   -3.77e+05    3.15e+05\n",
      "Cottage,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -9000.7159   1.77e+05     -0.051      0.959   -3.56e+05    3.38e+05\n",
      "Cottage,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -7.226e+04    3.4e+04     -2.126      0.033   -1.39e+05   -5655.003\n",
      "Cottage,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          4.612e+04   1.77e+05      0.261      0.794      -3e+05    3.93e+05\n",
      "Cottage,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -542.8740   1.25e+05     -0.004      0.997   -2.46e+05    2.45e+05\n",
      "Cottage,SplitLevel,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.0008      0.002      0.349      0.727      -0.004       0.005\n",
      "Cottage,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  5.105e+05   5.92e+04      8.619      0.000    3.94e+05    6.27e+05\n",
      "Cottage,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2.43e+05   1.77e+05      1.375      0.169   -1.03e+05    5.89e+05\n",
      "Cottage,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        9.469e+04   1.77e+05      0.536      0.592   -2.52e+05    4.41e+05\n",
      "Craftsman                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.669e+04   7894.344      2.115      0.034    1221.769    3.22e+04\n",
      "Craftsman,Dutch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -0.0005      0.001     -0.586      0.558      -0.002       0.001\n",
      "Craftsman,FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4420.7941   1.77e+05      0.025      0.980   -3.43e+05    3.52e+05\n",
      "Craftsman,FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                          6.464e+04   1.77e+05      0.366      0.715   -2.82e+05    4.11e+05\n",
      "Craftsman,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2.4e+04   7.92e+04      0.303      0.762   -1.31e+05    1.79e+05\n",
      "Craftsman,Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      6.438e+05   1.77e+05      3.642      0.000    2.97e+05     9.9e+05\n",
      "Craftsman,Other,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.785e+05   1.77e+05     -1.010      0.313   -5.25e+05    1.68e+05\n",
      "Craftsman,Other,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.965e+05   1.77e+05     -1.112      0.266   -5.43e+05     1.5e+05\n",
      "Craftsman,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1411.2283   1.77e+05      0.008      0.994   -3.46e+05    3.48e+05\n",
      "Craftsman,Prairie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     2.56e+05   1.77e+05      1.448      0.148   -9.05e+04    6.02e+05\n",
      "Craftsman,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -3.676e+05   1.77e+05     -2.079      0.038   -7.14e+05   -2.11e+04\n",
      "Craftsman,RaisedRanch,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.321e+05   1.77e+05     -0.747      0.455   -4.79e+05    2.14e+05\n",
      "Craftsman,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.22e+04    4.6e+04     -0.265      0.791   -1.02e+05     7.8e+04\n",
      "Craftsman,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -3.973e+04   1.77e+05     -0.225      0.822   -3.86e+05    3.07e+05\n",
      "Craftsman,Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.0030      0.003      0.945      0.344      -0.003       0.009\n",
      "Craftsman,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 4.67e+04   2.46e+04      1.895      0.058   -1595.710     9.5e+04\n",
      "Craftsman,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   4.902e+04   1.02e+05      0.479      0.632   -1.51e+05    2.49e+05\n",
      "Craftsman,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               3.408e+05   7.93e+04      4.300      0.000    1.85e+05    4.96e+05\n",
      "Craftsman,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.0007      0.003      0.240      0.811      -0.005       0.007\n",
      "Dome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -5.297e+04   1.77e+05     -0.300      0.764   -3.99e+05    2.94e+05\n",
      "Dutch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -5.126e+04   2.14e+04     -2.391      0.017   -9.33e+04   -9232.388\n",
      "Dutch,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.395e+05   1.77e+05     -0.789      0.430   -4.86e+05    2.07e+05\n",
      "Dutch,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -4.107e+04   1.77e+05     -0.232      0.816   -3.88e+05    3.05e+05\n",
      "DwellingwSeparateLivingArea                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1.483e+05   5.92e+04     -2.504      0.012   -2.64e+05   -3.22e+04\n",
      "DwellingwSeparateLivingArea,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.0006      0.003      0.189      0.850      -0.006       0.007\n",
      "DwellingwSeparateLivingArea,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -6.078e+04   1.25e+05     -0.486      0.627   -3.06e+05    1.84e+05\n",
      "DwellingwSeparateLivingArea,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -7.361e+04   8.85e+04     -0.831      0.406   -2.47e+05    9.99e+04\n",
      "DwellingwSeparateLivingArea,Rancher,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.502e+05   1.77e+05     -1.415      0.157   -5.97e+05    9.63e+04\n",
      "DwellingwSeparateLivingArea,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.789e+05   1.77e+05     -1.012      0.312   -5.25e+05    1.68e+05\n",
      "DwellingwSeparateLivingArea,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -3.816e+05   1.02e+05     -3.734      0.000   -5.82e+05   -1.81e+05\n",
      "DwellingwSeparateLivingArea,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -2.589e+05   1.25e+05     -2.069      0.039   -5.04e+05   -1.37e+04\n",
      "FarmhouseNationalFolk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.41e+04   1.25e+04     -1.130      0.258   -3.85e+04    1.03e+04\n",
      "FarmhouseNationalFolk,French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1.46e+05   1.77e+05      0.826      0.409      -2e+05    4.92e+05\n",
      "FarmhouseNationalFolk,LoftwithBedrooms,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0020      0.003      0.619      0.536      -0.004       0.008\n",
      "FarmhouseNationalFolk,Manor,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -0.0046      0.003     -1.437      0.151      -0.011       0.002\n",
      "FarmhouseNationalFolk,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -6.47e+04   1.25e+05     -0.517      0.605    -3.1e+05     1.8e+05\n",
      "FarmhouseNationalFolk,Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.97e+05   1.77e+05     -1.114      0.265   -5.43e+05    1.49e+05\n",
      "FarmhouseNationalFolk,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.33e+05   1.77e+05     -0.752      0.452   -4.79e+05    2.13e+05\n",
      "FarmhouseNationalFolk,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1.38e+05   5.93e+04      2.329      0.020    2.18e+04    2.54e+05\n",
      "FarmhouseNationalFolk,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1.229e+05   1.77e+05      0.695      0.487   -2.24e+05    4.69e+05\n",
      "FarmhouseNationalFolk,Traditional,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.0025      0.003      0.786      0.432      -0.004       0.009\n",
      "FarmhouseNationalFolk,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1.401e+05   1.25e+05      1.120      0.263   -1.05e+05    3.85e+05\n",
      "FarmhouseNationalFolk,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -1.166e+05   6.71e+04     -1.739      0.082   -2.48e+05    1.48e+04\n",
      "Federal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              7.415e+04   2.55e+04      2.908      0.004    2.42e+04    1.24e+05\n",
      "French                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               3.471e+05   2.07e+04     16.798      0.000    3.07e+05    3.88e+05\n",
      "French,Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         8.659e+05   1.77e+05      4.896      0.000    5.19e+05    1.21e+06\n",
      "French,Manor,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -0.0009      0.003     -0.277      0.781      -0.007       0.005\n",
      "French,Normandy,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1.586e+06   1.77e+05      8.967      0.000    1.24e+06    1.93e+06\n",
      "French,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.0012      0.003      0.373      0.709      -0.005       0.008\n",
      "French,Rancher,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1.933e+04   1.77e+05      0.109      0.913   -3.27e+05    3.66e+05\n",
      "French,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   4.232e+05   1.02e+05      4.140      0.000    2.23e+05    6.24e+05\n",
      "French,Traditional,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1.649e+05   1.77e+05      0.933      0.351   -1.82e+05    5.11e+05\n",
      "French,Traditional,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1.472e+06   1.77e+05      8.325      0.000    1.13e+06    1.82e+06\n",
      "French,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  8.979e+04   1.25e+05      0.718      0.473   -1.55e+05    3.35e+05\n",
      "Georgian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              4.61e+05   3.15e+04     14.650      0.000    3.99e+05    5.23e+05\n",
      "Georgian,Loft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -2.064e+05   1.77e+05     -1.167      0.243   -5.53e+05     1.4e+05\n",
      "Georgian,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1.493e+05   1.25e+05      1.193      0.233   -9.59e+04    3.94e+05\n",
      "Loft,LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -0.0030      0.003     -0.919      0.358      -0.009       0.003\n",
      "Loft,Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1.058e+06   1.77e+05      5.983      0.000    7.11e+05     1.4e+06\n",
      "Loft,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.0030      0.003      0.933      0.351      -0.003       0.009\n",
      "Loft,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.347e+05   8.86e+04     -1.521      0.128   -3.08e+05    3.89e+04\n",
      "Loft,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.0085      0.003      2.738      0.006       0.002       0.015\n",
      "Loft,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1.294e+05   1.25e+05      1.034      0.301   -1.16e+05    3.74e+05\n",
      "Loft,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -2.256e+04   1.77e+05     -0.128      0.898   -3.69e+05    3.24e+05\n",
      "Loft,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.956e-10   1.32e-10     -1.478      0.139   -4.55e-10    6.37e-11\n",
      "LoftwithBedrooms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     7.458e+04   1.77e+05      0.422      0.673   -2.72e+05    4.21e+05\n",
      "LoftwithBedrooms,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.046e+05   1.77e+05     -1.158      0.247   -5.51e+05    1.42e+05\n",
      "LogHome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -9.315e+04   3.46e+04     -2.694      0.007   -1.61e+05   -2.54e+04\n",
      "LogHome,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -2.43e+05   1.77e+05     -1.374      0.169    -5.9e+05    1.04e+05\n",
      "LogHome,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      5.227e-06   5.06e-06      1.033      0.301   -4.69e-06    1.51e-05\n",
      "Manor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                5.317e+05   3.74e+04     14.210      0.000    4.58e+05    6.05e+05\n",
      "Manor,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1.376e+05   1.77e+05      0.778      0.436   -2.09e+05    4.84e+05\n",
      "Manor,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -2.072e+05   1.77e+05     -1.172      0.241   -5.54e+05    1.39e+05\n",
      "Manor,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -7.441e+04   1.25e+05     -0.595      0.552    -3.2e+05    1.71e+05\n",
      "Manor,Traditional,Trinity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -7.528e+04   8.87e+04     -0.849      0.396   -2.49e+05    9.85e+04\n",
      "Manor,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   6.718e+05   1.77e+05      3.800      0.000    3.25e+05    1.02e+06\n",
      "Manor,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -2.043e+05   1.77e+05     -1.155      0.248   -5.51e+05    1.42e+05\n",
      "Mediterranean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -2.442e+04   6.71e+04     -0.364      0.716   -1.56e+05    1.07e+05\n",
      "Mediterranean,Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -5.412e+04   1.77e+05     -0.306      0.759   -4.01e+05    2.92e+05\n",
      "Mediterranean,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -0.0016      0.003     -0.505      0.613      -0.008       0.005\n",
      "Mediterranean,Traditional,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.0028      0.003      0.869      0.385      -0.004       0.009\n",
      "Mediterranean,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1.038e+04   1.77e+05      0.059      0.953   -3.36e+05    3.57e+05\n",
      "Mediterranean,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  6.217e+04   1.77e+05      0.352      0.725   -2.84e+05    4.09e+05\n",
      "MidCenturyModern                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     2.464e+04   4.33e+04      0.569      0.569   -6.02e+04     1.1e+05\n",
      "MidCenturyModern,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -4.081e+04   1.25e+05     -0.326      0.744   -2.86e+05    2.04e+05\n",
      "MidCenturyModern,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.0046      0.003      1.569      0.117      -0.001       0.010\n",
      "MidCenturyModern,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2.729e+04   5.62e+04      0.485      0.627   -8.29e+04    1.37e+05\n",
      "MidCenturyModern,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  3.06e+04   1.77e+05      0.173      0.863   -3.16e+05    3.77e+05\n",
      "MidCenturyModern,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -6.216e+04   1.25e+05     -0.497      0.619   -3.07e+05    1.83e+05\n",
      "MidCenturyModern,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -5.999e+04   1.25e+05     -0.480      0.631   -3.05e+05    1.85e+05\n",
      "MidCenturyModern,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1.291e+05   1.77e+05      0.730      0.465   -2.17e+05    4.76e+05\n",
      "Normandy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -1.102e+05   1.77e+05     -0.624      0.533   -4.57e+05    2.36e+05\n",
      "Normandy,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -9.389e+04   1.77e+05     -0.531      0.595    -4.4e+05    2.53e+05\n",
      "Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -9.927e+04   8535.879    -11.630      0.000   -1.16e+05   -8.25e+04\n",
      "Other,PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -3.024e+04   1.77e+05     -0.171      0.864   -3.77e+05    3.16e+05\n",
      "Other,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -2.282e+05   1.02e+05     -2.233      0.026   -4.28e+05   -2.79e+04\n",
      "Other,RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -2.097e+05   1.77e+05     -1.186      0.236   -5.56e+05    1.37e+05\n",
      "Other,RaisedRanch,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -1.534e+05   1.77e+05     -0.868      0.386      -5e+05    1.93e+05\n",
      "Other,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -7.704e+04   6.71e+04     -1.149      0.251   -2.08e+05    5.44e+04\n",
      "Other,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2.272e+04   1.25e+05      0.182      0.856   -2.22e+05    2.68e+05\n",
      "Other,Rancher,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -6.943e+04   1.77e+05     -0.393      0.695   -4.16e+05    2.77e+05\n",
      "Other,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.314e+05   2.83e+04     -4.646      0.000   -1.87e+05    -7.6e+04\n",
      "Other,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -2.531e+05   8.86e+04     -2.858      0.004   -4.27e+05   -7.95e+04\n",
      "Other,SplitFoyer,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.448e+04   1.77e+05     -0.082      0.935   -3.61e+05    3.32e+05\n",
      "Other,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -1.602e+05   7.24e+04     -2.214      0.027   -3.02e+05   -1.84e+04\n",
      "Other,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -7.73e+04   4.76e+04     -1.623      0.105   -1.71e+05     1.6e+04\n",
      "Other,Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -0.0018      0.003     -0.570      0.568      -0.008       0.004\n",
      "Other,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1.287e+05   1.25e+05      1.029      0.303   -1.16e+05    3.74e+05\n",
      "Other,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.0010      0.003      0.304      0.761      -0.005       0.007\n",
      "PostAndBeam                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          2.266e+05   4.33e+04      5.234      0.000    1.42e+05    3.11e+05\n",
      "PostAndBeam,Prairie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  9742.3746   1.77e+05      0.055      0.956   -3.37e+05    3.56e+05\n",
      "PostAndBeam,SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -5.684e+05   1.77e+05     -3.215      0.001   -9.15e+05   -2.22e+05\n",
      "Prairie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.132e+05   1.02e+05     -1.108      0.268   -3.14e+05     8.7e+04\n",
      "PreFabricated                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -1.426e+05   4.33e+04     -3.292      0.001   -2.28e+05   -5.77e+04\n",
      "PreFabricated,Other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -1.088e+05   1.77e+05     -0.616      0.538   -4.55e+05    2.38e+05\n",
      "PreFabricated,RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -2.675e+05   1.25e+05     -2.138      0.032   -5.13e+05   -2.23e+04\n",
      "PreFabricated,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.351e+05   7.24e+04     -1.866      0.062   -2.77e+05    6782.145\n",
      "RaisedRanch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1.039e+05   8868.576    -11.710      0.000   -1.21e+05   -8.65e+04\n",
      "RaisedRanch,Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -9.513e+04    1.4e+04     -6.789      0.000   -1.23e+05   -6.77e+04\n",
      "RaisedRanch,Rancher,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -4.242e+04   1.25e+05     -0.339      0.734   -2.88e+05    2.03e+05\n",
      "RaisedRanch,Rancher,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -1.483e+05   1.77e+05     -0.839      0.401   -4.95e+05    1.98e+05\n",
      "RaisedRanch,Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.41e+05   1.77e+05     -0.798      0.425   -4.88e+05    2.05e+05\n",
      "RaisedRanch,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.198e+05   5.62e+04     -2.131      0.033    -2.3e+05   -9593.151\n",
      "RaisedRanch,SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -1.414e+05   1.25e+05     -1.131      0.258   -3.87e+05    1.04e+05\n",
      "RaisedRanch,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -7.67e+04   1.25e+05     -0.613      0.540   -3.22e+05    1.68e+05\n",
      "RaisedRanch,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -9.447e+04   1.77e+05     -0.534      0.593   -4.41e+05    2.52e+05\n",
      "RaisedRanch,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.004e+05   8.85e+04     -1.134      0.257   -2.74e+05    7.31e+04\n",
      "Rancher                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -8.091e+04   6426.811    -12.589      0.000   -9.35e+04   -6.83e+04\n",
      "Rancher,Reverse                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     -8.855e+04   1.25e+05     -0.708      0.479   -3.34e+05    1.57e+05\n",
      "Rancher,SidebySide                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -9.503e+04   1.77e+05     -0.538      0.591   -4.41e+05    2.51e+05\n",
      "Rancher,Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.0037      0.003      1.146      0.252      -0.003       0.010\n",
      "Rancher,SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -1.129e+05   1.77e+05     -0.639      0.523   -4.59e+05    2.34e+05\n",
      "Rancher,SplitFoyer,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      -1.937e+05   1.77e+05     -1.096      0.273    -5.4e+05    1.53e+05\n",
      "Rancher,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    -4.5e+04   1.02e+05     -0.440      0.660   -2.45e+05    1.55e+05\n",
      "Rancher,StraightThru                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -1.311e+05   1.77e+05     -0.742      0.458   -4.78e+05    2.15e+05\n",
      "Rancher,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 -5.813e+04   2.86e+04     -2.031      0.042   -1.14e+05   -2034.122\n",
      "Rancher,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -4.953e+04   8.86e+04     -0.559      0.576   -2.23e+05    1.24e+05\n",
      "Rancher,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.0007      0.003      0.247      0.805      -0.005       0.007\n",
      "Rancher,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.0119      0.003      3.701      0.000       0.006       0.018\n",
      "SaltBox                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -3.225e+04   4.94e+04     -0.653      0.514   -1.29e+05    6.45e+04\n",
      "SidebySide                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -2.42e+05   1.77e+05     -1.369      0.171   -5.88e+05    1.04e+05\n",
      "SidebySide,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -7.246e+04   1.77e+05     -0.410      0.682   -4.19e+05    2.74e+05\n",
      "Spanish                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1.492e+05   5.62e+04      2.655      0.008     3.9e+04    2.59e+05\n",
      "SplitFoyer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -1.272e+05   6861.440    -18.543      0.000   -1.41e+05   -1.14e+05\n",
      "SplitFoyer,SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -1.321e+05   2.22e+04     -5.962      0.000   -1.76e+05   -8.87e+04\n",
      "SplitFoyer,SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -8.662e+04   1.25e+05     -0.693      0.489   -3.32e+05    1.59e+05\n",
      "SplitFoyer,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -1.828e+05   4.94e+04     -3.702      0.000    -2.8e+05    -8.6e+04\n",
      "SplitFoyer,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.0054      0.003      1.721      0.085      -0.001       0.012\n",
      "SplitLevel                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -9.397e+04   6894.448    -13.630      0.000   -1.07e+05   -8.05e+04\n",
      "SplitLevel,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              -4.927e+04   3.28e+04     -1.500      0.134   -1.14e+05    1.51e+04\n",
      "SplitLevel,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.507e+05   1.02e+05     -1.475      0.140   -3.51e+05    4.95e+04\n",
      "StraightThru                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -3.063e+05   1.77e+05     -1.733      0.083   -6.53e+05    4.02e+04\n",
      "StraightThru,Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -1.17e+05   1.77e+05     -0.662      0.508   -4.63e+05    2.29e+05\n",
      "Traditional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -1.814e+04   7079.250     -2.563      0.010    -3.2e+04   -4267.578\n",
      "Traditional,CondoUnit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -5.749e+04   1.77e+05     -0.325      0.745   -4.04e+05    2.89e+05\n",
      "Traditional,Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             3.494e+04    3.9e+04      0.895      0.371   -4.16e+04    1.11e+05\n",
      "Traditional,Trinity,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -8.792e+04   4.77e+04     -1.844      0.065   -1.81e+05    5537.523\n",
      "Traditional,Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -5.754e+04   6.28e+04     -0.917      0.359   -1.81e+05    6.55e+04\n",
      "Traditional,Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -1.177e+04   1.25e+05     -0.094      0.925   -2.57e+05    2.33e+05\n",
      "Transitional                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         8.358e+04   1.14e+04      7.308      0.000    6.12e+04    1.06e+05\n",
      "Transitional,Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1806.1676   1.77e+05      0.010      0.992   -3.45e+05    3.48e+05\n",
      "Tudor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1.246e+05   1.95e+04      6.379      0.000    8.63e+04    1.63e+05\n",
      "Victorian                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           -3.522e+04   1.51e+04     -2.335      0.020   -6.48e+04   -5653.816\n",
      "Villa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                3.111e+05   4.33e+04      7.185      0.000    2.26e+05    3.96e+05\n",
      "NoBasement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          -8.544e+06   1.78e+05    -47.868      0.000   -8.89e+06   -8.19e+06\n",
      "HasBasement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -8.534e+06   1.78e+05    -47.818      0.000   -8.88e+06   -8.18e+06\n",
      "NoFireplace                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         -4818.5967   2175.030     -2.215      0.027   -9081.662    -555.531\n",
      "HasFireplace                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5.159e+04   1902.762     27.112      0.000    4.79e+04    5.53e+04\n",
      "NoCentralAir                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -8.551e+06   1.78e+05    -47.927      0.000    -8.9e+06    -8.2e+06\n",
      "HasCentralAir                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -8.528e+06   1.79e+05    -47.759      0.000   -8.88e+06   -8.18e+06\n",
      "NotWaterfront                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -8.74e+06   1.78e+05    -48.969      0.000   -9.09e+06   -8.39e+06\n",
      "IsWaterfront                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        -8.338e+06   1.78e+05    -46.712      0.000   -8.69e+06   -7.99e+06\n",
      "NotNewConstruction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -8.564e+06   1.79e+05    -47.950      0.000   -8.91e+06   -8.21e+06\n",
      "IsNewConstruction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   -8.514e+06   1.78e+05    -47.733      0.000   -8.86e+06   -8.16e+06\n",
      "==============================================================================\n",
      "Omnibus:                    78547.399   Durbin-Watson:                   2.011\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         71352525.230\n",
      "Skew:                           6.562   Prob(JB):                         0.00\n",
      "Kurtosis:                     170.036   Cond. No.                     7.69e+23\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.62e-32. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#set the training and testing data\n",
    "X = X_train.drop('SoldPrice', axis = 1)\n",
    "y = X_train.SoldPrice\n",
    "X = sm.add_constant(X) #add the constant to the model\n",
    "mod = sm.OLS(y, X, hasconst= True) #create the ordinary least squares model\n",
    "res = mod.fit() #fit the model\n",
    "print(res.summary()) #summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(y, y, color = 'red')\n",
    "plt.scatter(res.predict(X), y, color = 'blue')\n",
    "plt.title('Actual Vs. Predicted Sold Price')\n",
    "plt.xlim(0, 5000000)\n",
    "plt.ylim(0, 5000000)\n",
    "plt.xlabel('Predicted Sold Price')\n",
    "plt.ylabel('Actual Sold Price')\n",
    "plt.show()\n",
    "\n",
    "#plot.scatter(xTrain, yTrain, color = 'red')\n",
    "#plot.plot(xTrain, linearRegressor.predict(xTrain), color = 'blue')\n",
    "#plot.title('Salary vs Experience (Training set)')\n",
    "#plot.xlabel('Years of Experience')\n",
    "#plot.ylabel('Salary')\n",
    "#plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the residuals\n",
    "residuals = res.resid\n",
    "\n",
    "y_pred = res.predict(X)\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('residuals')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61000, 24)\n",
      "(15251, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(dfpdForLinearRegressionNoStyle, test_size = 0.2, random_state = 2) # splits the data into two parts with 1:4 ratio\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SoldPrice   R-squared:                       0.609\n",
      "Model:                            OLS   Adj. R-squared:                  0.608\n",
      "Method:                 Least Squares   F-statistic:                     5267.\n",
      "Date:                Tue, 17 May 2022   Prob (F-statistic):               0.00\n",
      "Time:                        20:40:59   Log-Likelihood:            -8.2575e+05\n",
      "No. Observations:               61000   AIC:                         1.652e+06\n",
      "Df Residuals:                   60981   BIC:                         1.652e+06\n",
      "Df Model:                          18                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const              -1.794e+07   3.64e+05    -49.247      0.000   -1.87e+07   -1.72e+07\n",
      "SettledDate           75.8189      1.599     47.422      0.000      72.685      78.953\n",
      "ZipCode              125.7848      5.178     24.291      0.000     115.635     135.934\n",
      "AcresTotal           1.86e+04    268.122     69.383      0.000    1.81e+04    1.91e+04\n",
      "Age                  -31.0015      6.258     -4.954      0.000     -43.268     -18.735\n",
      "InteriorSqFt           1.6140      0.123     13.074      0.000       1.372       1.856\n",
      "Bedrooms            1.667e+04   1079.566     15.442      0.000    1.46e+04    1.88e+04\n",
      "BathsFull           1.341e+05   1075.812    124.660      0.000    1.32e+05    1.36e+05\n",
      "BathsHalf           1.043e+05   1363.465     76.513      0.000    1.02e+05    1.07e+05\n",
      "GarageSpaces        2794.2810    217.696     12.836      0.000    2367.596    3220.967\n",
      "ANNEARUNDELMD      -4.436e+06   9.11e+04    -48.696      0.000   -4.61e+06   -4.26e+06\n",
      "BALTIMOREMD         -4.54e+06   9.11e+04    -49.812      0.000   -4.72e+06   -4.36e+06\n",
      "HARFORDMD          -4.554e+06   9.11e+04    -50.008      0.000   -4.73e+06   -4.38e+06\n",
      "HOWARDMD           -4.409e+06    9.1e+04    -48.445      0.000   -4.59e+06   -4.23e+06\n",
      "NoBasement          -8.97e+06   1.82e+05    -49.246      0.000   -9.33e+06   -8.61e+06\n",
      "HasBasement        -8.968e+06   1.82e+05    -49.246      0.000   -9.33e+06   -8.61e+06\n",
      "NoFireplace        -5118.1014   2233.388     -2.292      0.022   -9495.548    -740.655\n",
      "HasFireplace        5.459e+04   1948.785     28.013      0.000    5.08e+04    5.84e+04\n",
      "NoCentralAir       -8.979e+06   1.82e+05    -49.318      0.000   -9.34e+06   -8.62e+06\n",
      "HasCentralAir      -8.959e+06   1.82e+05    -49.173      0.000   -9.32e+06    -8.6e+06\n",
      "NotWaterfront      -9.186e+06   1.82e+05    -50.443      0.000   -9.54e+06   -8.83e+06\n",
      "IsWaterfront       -8.752e+06   1.82e+05    -48.044      0.000   -9.11e+06   -8.39e+06\n",
      "NotNewConstruction -9.016e+06   1.82e+05    -49.481      0.000   -9.37e+06   -8.66e+06\n",
      "IsNewConstruction  -8.923e+06   1.82e+05    -49.009      0.000   -9.28e+06   -8.57e+06\n",
      "==============================================================================\n",
      "Omnibus:                    77568.856   Durbin-Watson:                   2.011\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         58024146.407\n",
      "Skew:                           6.470   Prob(JB):                         0.00\n",
      "Kurtosis:                     153.538   Cond. No.                     3.62e+22\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.53e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#set the training and testing data\n",
    "X = X_train.drop('SoldPrice', axis = 1)\n",
    "y = X_train.SoldPrice\n",
    "X = sm.add_constant(X) #add the constant to the model\n",
    "mod = sm.OLS(y, X, hasconst= True) #create the ordinary least squares model\n",
    "res = mod.fit() #fit the model\n",
    "print(res.summary()) #summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the residuals\n",
    "residuals = res.resid\n",
    "\n",
    "y_pred = res.predict(X)\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('residuals')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with normalized scaling\n",
    "\n",
    "x = dfpdForLinearRegressionNoStyle.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dfpdForLinearRegressionNoStyleScaled = pd.DataFrame(x_scaled, columns=dfpdForLinearRegressionNoStyle.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61000, 24)\n",
      "(15251, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(dfpdForLinearRegressionNoStyleScaled, test_size = 0.2, random_state = 2) # splits the data into two parts with 1:4 ratio\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SoldPrice   R-squared:                       0.609\n",
      "Model:                            OLS   Adj. R-squared:                  0.608\n",
      "Method:                 Least Squares   F-statistic:                     5267.\n",
      "Date:                Tue, 17 May 2022   Prob (F-statistic):               0.00\n",
      "Time:                        20:40:59   Log-Likelihood:             1.6326e+05\n",
      "No. Observations:               61000   AIC:                        -3.265e+05\n",
      "Df Residuals:                   60981   BIC:                        -3.263e+05\n",
      "Df Model:                          18                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "const                  0.0038      0.000     10.384      0.000       0.003       0.005\n",
      "SettledDate            0.0126      0.000     47.422      0.000       0.012       0.013\n",
      "ZipCode                0.0124      0.001     24.291      0.000       0.011       0.013\n",
      "AcresTotal             0.4173      0.006     69.383      0.000       0.406       0.429\n",
      "Age                   -0.0060      0.001     -4.954      0.000      -0.008      -0.004\n",
      "InteriorSqFt           0.1630      0.012     13.074      0.000       0.139       0.187\n",
      "Bedrooms               0.0485      0.003     15.442      0.000       0.042       0.055\n",
      "BathsFull              0.1341      0.001    124.660      0.000       0.132       0.136\n",
      "BathsHalf              0.0854      0.001     76.513      0.000       0.083       0.088\n",
      "GarageSpaces           0.2058      0.016     12.836      0.000       0.174       0.237\n",
      "ANNEARUNDELMD          0.0054      0.000     37.946      0.000       0.005       0.006\n",
      "BALTIMOREMD           -0.0041      0.000    -27.331      0.000      -0.004      -0.004\n",
      "HARFORDMD             -0.0053      0.000    -30.158      0.000      -0.006      -0.005\n",
      "HOWARDMD               0.0078      0.000     43.072      0.000       0.007       0.008\n",
      "NoBasement             0.0018      0.000      8.825      0.000       0.001       0.002\n",
      "HasBasement            0.0020      0.000      9.300      0.000       0.002       0.002\n",
      "NoFireplace           -0.0005      0.000     -2.292      0.022      -0.001   -6.73e-05\n",
      "HasFireplace           0.0050      0.000     28.013      0.000       0.005       0.005\n",
      "NoCentralAir           0.0010      0.000      4.781      0.000       0.001       0.001\n",
      "HasCentralAir          0.0028      0.000     12.823      0.000       0.002       0.003\n",
      "NotWaterfront         -0.0178      0.000    -72.497      0.000      -0.018      -0.017\n",
      "IsWaterfront           0.0217      0.000     75.711      0.000       0.021       0.022\n",
      "NotNewConstruction    -0.0023      0.000    -10.174      0.000      -0.003      -0.002\n",
      "IsNewConstruction      0.0061      0.000     24.981      0.000       0.006       0.007\n",
      "==============================================================================\n",
      "Omnibus:                    77568.856   Durbin-Watson:                   2.011\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         58024146.407\n",
      "Skew:                           6.470   Prob(JB):                         0.00\n",
      "Kurtosis:                     153.538   Cond. No.                     1.04e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.64e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "#set the training and testing data\n",
    "X = X_train.drop('SoldPrice', axis = 1)\n",
    "y = X_train.SoldPrice\n",
    "X = sm.add_constant(X) #add the constant to the model\n",
    "mod = sm.OLS(y, X, hasconst= True) #create the ordinary least squares model\n",
    "res = mod.fit() #fit the model\n",
    "print(res.summary()) #summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the residuals\n",
    "residuals = res.resid\n",
    "\n",
    "y_pred = res.predict(X)\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('residuals')\n",
    "plt.title('Residuals Plot: Linear Regression')\n",
    "plt.tight_layout()\n",
    "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.savefig(\"../Images/linearRegressionResiduals.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def model_experiment(num_iter = 5, \n",
    "                     models = ['ols', 'ridge', 'lasso'], alpha= 10, \n",
    "                     complexity = 'simple', degree = 3):\n",
    "    \n",
    "    X_train, X_test = train_test_split(dfpdForLinearRegressionNoStyle, test_size = 0.2) # splits the data into two parts with 1:4 ratio\n",
    "    X = X_train.drop('SoldPrice', axis = 1)\n",
    "    y = X_train.SoldPrice\n",
    "    \n",
    "    x_axis = np.arange(num_iter)\n",
    "    y_ols_test = []\n",
    "    y_lasso_test = []\n",
    "    y_ridge_test = []\n",
    "    sample_models = {}\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        if complexity == 'simple':\n",
    "            ## split train_test \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "        elif complexity == 'polynomial':\n",
    "            ## Create higher order terms\n",
    "            poly = PolynomialFeatures(degree=degree)\n",
    "            Xp = poly.fit_transform(X)\n",
    "            ## test-train split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(Xp, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "        ## Standard scale mean = 0, variance = 1\n",
    "        #sd = StandardScaler()\n",
    "\n",
    "        #sd.fit(X_train)\n",
    "\n",
    "        #X_train = sd.transform(X_train)\n",
    "\n",
    "        #X_test = sd.transform(X_test)\n",
    "\n",
    "        ## Be careful about the leakage\n",
    "\n",
    "        ## Vanilla model\n",
    "        if 'ols' in models:\n",
    "            lr = LinearRegression()\n",
    "\n",
    "            lr.fit(X_train, y_train)\n",
    "            \n",
    "            sample_models['ols'] = lr\n",
    "\n",
    "            test_score = lr.score(X_test, y_test)\n",
    "            train_score = lr.score(X_train, y_train)\n",
    "\n",
    "            y_ols_test.append(test_score)\n",
    "\n",
    "    #       print('test score OLS is %.2f and train score is %.2f'%(test_score, train_score))\n",
    "\n",
    "        if 'ridge' in models:\n",
    "            ## Ridge in the simple setting\n",
    "            ridge = Ridge(alpha = alpha, max_iter= 10000)\n",
    "            ridge.fit(X_train, y_train)\n",
    "            sample_models['ridge'] = ridge\n",
    "            y_ridge_test.append(ridge.score(X_test, y_test))\n",
    "    #         print('test score Ridge is %.2f and train score is %.2f'%(ridge.score(X_test, y_test),\n",
    "    #                                                             ridge.score(X_train, y_train)))\n",
    "\n",
    "        if 'lasso' in models:\n",
    "            ## Lasso in the simple setting\n",
    "            lasso = Lasso(alpha = alpha, max_iter= 10000)\n",
    "\n",
    "            lasso.fit(X_train, y_train)\n",
    "            \n",
    "            sample_models['lasso'] = lasso\n",
    "            \n",
    "            y_lasso_test.append(lasso.score(X_test, y_test))\n",
    "    #       print('test score Lasso is %.2f and train score is %.2f'%(lasso.score(X_test, y_test),\n",
    "    #                                                             lasso.score(X_train, y_train)))\n",
    "\n",
    "        i+=1\n",
    "    if 'ols' in models:\n",
    "        plt.plot(y_ols_test, label = 'ols')\n",
    "    if 'ridge' in models:\n",
    "        plt.plot(y_ridge_test, label = 'ridge')\n",
    "    if 'lasso' in models:\n",
    "        plt.plot(y_lasso_test, label = 'lasso')\n",
    "    plt.ylabel('R2 test score')\n",
    "    plt.xlabel('number of iterations')\n",
    "    all_results = y_ols_test + y_lasso_test + y_ridge_test\n",
    "    plt.ylim((np.min(all_results), np.max(all_results)))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(\"../Images/regressionComparison.png\",bbox_inches='tight')\n",
    "    return sample_models\n",
    "\n",
    "trained_models = model_experiment(num_iter=30, alpha = 30,\n",
    "                                   models = ['ols', 'ridge', 'lasso'], \n",
    "                                   complexity= 'simple', degree = 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.74429895e+01  1.18029766e+02  1.75931790e+04 -4.69442900e+01\n",
      "  1.56439862e+00  1.53064701e+04  1.36597159e+05  1.09835173e+05\n",
      "  2.25674040e+03  4.87644785e+04 -5.29581114e+04 -7.00451944e+04\n",
      "  7.42388273e+04  1.63184141e+02 -1.63184141e+02 -5.97549255e+03\n",
      "  5.48154102e+04 -9.78472688e+03  9.78472688e+03 -2.13744673e+05\n",
      "  2.13744673e+05 -4.58748338e+04  4.58748338e+04]\n",
      "[ 7.74035037e+01  1.18081294e+02  1.75960289e+04 -4.69255444e+01\n",
      "  1.56821061e+00  1.53686024e+04  1.36584276e+05  1.09640272e+05\n",
      "  2.26336859e+03  4.88436383e+04 -5.28990664e+04 -6.97864025e+04\n",
      "  7.38418307e+04  2.17254091e+02 -2.17254091e+02 -6.08082550e+03\n",
      "  5.47658795e+04 -9.74320033e+03  9.74320033e+03 -2.11711460e+05\n",
      "  2.11711460e+05 -4.56045423e+04  4.56045423e+04]\n",
      "[ 7.74526396e+01  1.17917764e+02  1.75883905e+04 -4.69337706e+01\n",
      "  1.56634053e+00  1.52864814e+04  1.36641263e+05  1.09783130e+05\n",
      "  2.25843931e+03  6.08477890e+04 -4.07925429e+04 -5.77429142e+04\n",
      "  8.61221846e+04  4.30466621e+01 -0.00000000e+00 -5.80768049e+03\n",
      "  5.47911123e+04 -1.92250926e+04  4.95903078e-11 -4.26586887e+05\n",
      "  2.25217699e-09 -9.11522434e+04  4.04299292e-12]\n"
     ]
    }
   ],
   "source": [
    "# After run model_experiment with complexity == 'polynomial'\n",
    "\n",
    "lr_ols = trained_models['ols']\n",
    "lr_lasso = trained_models['lasso']\n",
    "lr_ridge =trained_models['ridge']\n",
    "\n",
    "# check the coefficients from Lasso\n",
    "\n",
    "print(lr_ols.coef_)\n",
    "print(lr_ridge.coef_)\n",
    "print(lr_lasso.coef_)\n",
    "\n",
    "# compare them with OLS/Ridge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48800, 2600)\n",
      "(12200, 2600)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(dfpdForLinearRegressionNoStyle, test_size = 0.2) # splits the data into two parts with 1:4 ratio\n",
    "X = X_train.drop('SoldPrice', axis = 1)\n",
    "y = X_train.SoldPrice\n",
    "\n",
    "## Create higher order terms\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "Xp = poly.fit_transform(X)\n",
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xp, y, test_size = 0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6613804307184897"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the training and testing data\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "test_score = lr.score(X_test, y_test)\n",
    "train_score = lr.score(X_train, y_train)\n",
    "\n",
    "test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried here above with polynomial regression but got pretty bad results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6100, 24)\n",
      "(1525, 24)\n"
     ]
    }
   ],
   "source": [
    "sample = dfpdForLinearRegressionNoStyle.sample(frac=0.1, replace=False, random_state=1)\n",
    "\n",
    "X_train, X_test = train_test_split(sample, test_size = 0.2, random_state = 2) # splits the data into two parts with 1:4 ratio\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SoldPrice</th>\n",
       "      <th>SettledDate</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>AcresTotal</th>\n",
       "      <th>Age</th>\n",
       "      <th>InteriorSqFt</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>BathsFull</th>\n",
       "      <th>BathsHalf</th>\n",
       "      <th>GarageSpaces</th>\n",
       "      <th>ANNEARUNDELMD</th>\n",
       "      <th>BALTIMOREMD</th>\n",
       "      <th>HARFORDMD</th>\n",
       "      <th>HOWARDMD</th>\n",
       "      <th>NoBasement</th>\n",
       "      <th>HasBasement</th>\n",
       "      <th>NoFireplace</th>\n",
       "      <th>HasFireplace</th>\n",
       "      <th>NoCentralAir</th>\n",
       "      <th>HasCentralAir</th>\n",
       "      <th>NotWaterfront</th>\n",
       "      <th>IsWaterfront</th>\n",
       "      <th>NotNewConstruction</th>\n",
       "      <th>IsNewConstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9511</th>\n",
       "      <td>514550</td>\n",
       "      <td>737455</td>\n",
       "      <td>21108</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>300000</td>\n",
       "      <td>737854</td>\n",
       "      <td>21206</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29299</th>\n",
       "      <td>292650</td>\n",
       "      <td>736853</td>\n",
       "      <td>21061</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29194</th>\n",
       "      <td>220000</td>\n",
       "      <td>736867</td>\n",
       "      <td>21037</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>960.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11006</th>\n",
       "      <td>425000</td>\n",
       "      <td>737633</td>\n",
       "      <td>21090</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SoldPrice  SettledDate  ZipCode  AcresTotal     Age  InteriorSqFt  \\\n",
       "9511      514550       737455    21108        0.00  2020.0        3571.0   \n",
       "1609      300000       737854    21206        0.19  1954.0        1492.0   \n",
       "29299     292650       736853    21061        0.17  1976.0        1608.0   \n",
       "29194     220000       736867    21037        0.24  1968.0         960.0   \n",
       "11006     425000       737633    21090        0.23  1976.0        1944.0   \n",
       "\n",
       "       Bedrooms  BathsFull  BathsHalf  GarageSpaces  ANNEARUNDELMD  \\\n",
       "9511        4.0        2.0        1.0           3.0              1   \n",
       "1609        3.0        2.0        0.0           2.0              0   \n",
       "29299       4.0        2.0        0.0           0.0              1   \n",
       "29194       3.0        1.0        0.0           0.0              1   \n",
       "11006       4.0        2.0        2.0           1.0              1   \n",
       "\n",
       "       BALTIMOREMD  HARFORDMD  HOWARDMD  NoBasement  HasBasement  NoFireplace  \\\n",
       "9511             0          0         0           0            1            1   \n",
       "1609             1          0         0           0            1            0   \n",
       "29299            0          0         0           0            1            0   \n",
       "29194            0          0         0           1            0            1   \n",
       "11006            0          0         0           0            1            0   \n",
       "\n",
       "       HasFireplace  NoCentralAir  HasCentralAir  NotWaterfront  IsWaterfront  \\\n",
       "9511              0             0              1              1             0   \n",
       "1609              1             0              1              1             0   \n",
       "29299             1             0              1              1             0   \n",
       "29194             0             1              0              1             0   \n",
       "11006             1             0              1              1             0   \n",
       "\n",
       "       NotNewConstruction  IsNewConstruction  \n",
       "9511                    0                  1  \n",
       "1609                    1                  0  \n",
       "29299                   1                  0  \n",
       "29194                   1                  0  \n",
       "11006                   1                  0  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=100, gamma=0.001)\n",
      "0.7356147972439364\n",
      "[418519.02094328]\n",
      "[418519.02094328]\n"
     ]
    }
   ],
   "source": [
    "# Now try SVR\n",
    "X = X_train.drop('SoldPrice', axis = 1)\n",
    "y = X_train.SoldPrice\n",
    "y = np.asarray(y)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)\n",
    "\n",
    "regressor = SVR(kernel = 'rbf', C = 100, gamma = .001).fit(X, y)\n",
    "print(regressor)\n",
    "\n",
    "score = regressor.score(X, y)\n",
    "print(score)\n",
    "\n",
    "#print(X[1])\n",
    "\n",
    "data = [[738124, 21144, 1.61, 1995.0, 3806.0, 5.0, 2.0, 1.0, 2.0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]]\n",
    "sc_Xpred = StandardScaler()\n",
    "Xpred = sc_Xpred.fit_transform(data)\n",
    "data2 = [[737928, 21784, 6.00, 1996.0, 4428.0, 4.0, 2.0, 1.0, 4.0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0]]\n",
    "sc_Xpred2 = StandardScaler()\n",
    "Xpred2 = sc_Xpred2.fit_transform(data2)\n",
    "#data = sc_X.fit_transform(data)\n",
    "\n",
    "\n",
    "y_pred = regressor.predict(Xpred)\n",
    "y_pred = sc_y.inverse_transform(y_pred)\n",
    "print(y_pred)\n",
    "\n",
    "y_pred2 = regressor.predict(Xpred2)\n",
    "y_pred2 = sc_y.inverse_transform(y_pred2)\n",
    "print(y_pred2)\n",
    "\n",
    "#Xshaped = np.array(X).reshape(-1,1)\n",
    "#print(len(X))\n",
    "#print(len(y))\n",
    "\n",
    "#plt.scatter(X, y, s=5, color=\"blue\", label=\"original\")\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "#X_grid = np.arange(np.min(X), np.max(X), 0.01) #this step required because data is feature scaled.\n",
    "#X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "#plt.scatter(X, y, color = 'red')\n",
    "#plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n",
    "#plt.title('Truth or Bluff (SVR)')\n",
    "#plt.xlabel('Position level')\n",
    "#plt.ylabel('Salary')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 514550.]\n",
      " [ 300000.]\n",
      " [ 292650.]\n",
      " ...\n",
      " [ 600000.]\n",
      " [ 300000.]\n",
      " [1094000.]]\n",
      "[ 650512.25453872  314126.07933187  303780.53550339 ...  622271.06381872\n",
      "  322160.30943237 1365127.95346773]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sc_y.inverse_transform(y))\n",
    "print(sc_y.inverse_transform(regressor.predict(X)))\n",
    "\n",
    "plt.scatter(sc_y.inverse_transform(regressor.predict(X)), sc_y.inverse_transform(y), color = 'blue')\n",
    "plt.title('Actual Vs. Predicted Sold Price')\n",
    "plt.xlim(0, 5000000)\n",
    "plt.ylim(0, 5000000)\n",
    "plt.xlabel('Predicted Sold Price')\n",
    "plt.ylabel('Actual Sold Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6100,)\n",
      "(6100,)\n",
      "[ 650512.25453872  314126.07933187  303780.53550339 ...  622271.06381872\n",
      "  322160.30943237 1365127.95346773]\n",
      "6100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actualPrices = sc_y.inverse_transform(y)\n",
    "actualPrices2 = np.reshape(actualPrices, (6100,))\n",
    "print(actualPrices2.shape)\n",
    "predictedPrices = np.reshape(sc_y.inverse_transform(regressor.predict(X)), (6100,))\n",
    "print(predictedPrices.shape)\n",
    "#print(actualPrices2)\n",
    "print(predictedPrices)\n",
    "residuals = []\n",
    "for item1, item2 in zip(actualPrices2.tolist(), predictedPrices.tolist()):\n",
    "    residuals.append(item1 - item2)\n",
    "#rediduals = actualPrices2 - predictedPrices\n",
    "#print(residuals.shape)\n",
    "print(len(residuals))\n",
    "#print(residuals)\n",
    "plt.scatter(predictedPrices, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.title('Residual Plot: Support Vector Regression')\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('residuals')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../Images/svrResiduals.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "[CV] C=1, gamma=1e-08, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-08, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-07, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-06, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=1e-05, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] .................... C=1, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=poly ..................................\n",
      "[CV] ................... C=1, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=poly ..................................\n",
      "[CV] ................... C=1, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=poly ..................................\n",
      "[CV] ................... C=1, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=poly ..................................\n",
      "[CV] ................... C=1, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=poly ..................................\n",
      "[CV] ................... C=1, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ..................... C=1, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.001, kernel=poly ...................................\n",
      "[CV] .................... C=1, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ...................... C=1, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=poly ....................................\n",
      "[CV] ..................... C=1, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=poly ....................................\n",
      "[CV] ..................... C=1, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=poly ....................................\n",
      "[CV] ..................... C=1, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=poly ....................................\n",
      "[CV] ..................... C=1, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.01, kernel=poly ....................................\n",
      "[CV] ..................... C=1, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] ....................... C=1, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1, gamma=0.1, kernel=poly .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, gamma=0.1, kernel=poly, total=   0.1s\n",
      "[CV] C=1, gamma=0.1, kernel=poly .....................................\n",
      "[CV] ...................... C=1, gamma=0.1, kernel=poly, total=   0.1s\n",
      "[CV] C=1, gamma=0.1, kernel=poly .....................................\n",
      "[CV] ...................... C=1, gamma=0.1, kernel=poly, total=   0.1s\n",
      "[CV] C=1, gamma=0.1, kernel=poly .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, gamma=0.1, kernel=poly, total=   0.1s\n",
      "[CV] C=1, gamma=0.1, kernel=poly .....................................\n",
      "[CV] ...................... C=1, gamma=0.1, kernel=poly, total=   0.1s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.2s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n",
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.2s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.5s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.3s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.3s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-08, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-07, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-06, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=1e-05, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ................... C=10, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=poly .................................\n",
      "[CV] .................. C=10, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] .................... C=10, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.001, kernel=poly ..................................\n",
      "[CV] ................... C=10, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ..................... C=10, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.01, kernel=poly ...................................\n",
      "[CV] .................... C=10, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ...................... C=10, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.4s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.3s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.2s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.3s\n",
      "[CV] C=10, gamma=0.1, kernel=poly ....................................\n",
      "[CV] ..................... C=10, gamma=0.1, kernel=poly, total=   0.1s\n",
      "[CV] C=100, gamma=1e-08, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=100, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-08, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=100, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-07, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=100, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-06, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=100, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=1e-05, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=100, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=poly ................................\n",
      "[CV] ................. C=100, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=poly ................................\n",
      "[CV] ................. C=100, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=poly ................................\n",
      "[CV] ................. C=100, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=poly ................................\n",
      "[CV] ................. C=100, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.0001, kernel=poly ................................\n",
      "[CV] ................. C=100, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ................... C=100, gamma=0.001, kernel=rbf, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.001, kernel=poly .................................\n",
      "[CV] .................. C=100, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=100, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=100, gamma=0.01, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.01, kernel=rbf, total=   0.2s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.01, kernel=rbf, total=   0.2s\n",
      "[CV] C=100, gamma=0.01, kernel=poly ..................................\n",
      "[CV] ................... C=100, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=poly ..................................\n",
      "[CV] ................... C=100, gamma=0.01, kernel=poly, total=   0.1s\n",
      "[CV] C=100, gamma=0.01, kernel=poly ..................................\n",
      "[CV] ................... C=100, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=poly ..................................\n",
      "[CV] ................... C=100, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.01, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=100, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=100, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=100, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=100, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=100, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=100, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=100, gamma=0.1, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.1, kernel=poly, total=   0.3s\n",
      "[CV] C=100, gamma=0.1, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.1, kernel=poly, total=   0.4s\n",
      "[CV] C=100, gamma=0.1, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.1, kernel=poly, total=   0.6s\n",
      "[CV] C=100, gamma=0.1, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.1, kernel=poly, total=   0.7s\n",
      "[CV] C=100, gamma=0.1, kernel=poly ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=100, gamma=0.1, kernel=poly, total=   0.5s\n",
      "[CV] C=1000, gamma=1e-08, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=1e-08, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-08, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-08, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-07, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=poly ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=1000, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-07, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-07, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=1e-06, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-06, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-06, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=1e-05, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=1e-05, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=1e-05, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] ................. C=1000, gamma=0.0001, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=poly ...............................\n",
      "[CV] ................ C=1000, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=poly ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ C=1000, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=poly ...............................\n",
      "[CV] ................ C=1000, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=poly ...............................\n",
      "[CV] ................ C=1000, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.0001, kernel=poly ...............................\n",
      "[CV] ................ C=1000, gamma=0.0001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   0.3s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   0.2s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   0.2s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   0.2s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=0.001, kernel=rbf, total=   0.2s\n",
      "[CV] C=1000, gamma=0.001, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.001, kernel=poly ................................\n",
      "[CV] ................. C=1000, gamma=0.001, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.01, kernel=rbf, total=   0.8s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.01, kernel=rbf, total=   0.4s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.01, kernel=rbf, total=   0.4s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.01, kernel=rbf, total=   0.4s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.01, kernel=rbf, total=   0.4s\n",
      "[CV] C=1000, gamma=0.01, kernel=poly .................................\n",
      "[CV] .................. C=1000, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=poly .................................\n",
      "[CV] .................. C=1000, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=poly .................................\n",
      "[CV] .................. C=1000, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=poly .................................\n",
      "[CV] .................. C=1000, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.01, kernel=poly .................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=1000, gamma=0.01, kernel=poly, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] .................... C=1000, gamma=0.1, kernel=rbf, total=   0.1s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] .................... C=1000, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] .................... C=1000, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] .................... C=1000, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1000, gamma=0.1, kernel=rbf, total=   0.0s\n",
      "[CV] C=1000, gamma=0.1, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.1, kernel=poly, total=   0.4s\n",
      "[CV] C=1000, gamma=0.1, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.1, kernel=poly, total=   0.5s\n",
      "[CV] C=1000, gamma=0.1, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.1, kernel=poly, total=   0.9s\n",
      "[CV] C=1000, gamma=0.1, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.1, kernel=poly, total=   0.8s\n",
      "[CV] C=1000, gamma=0.1, kernel=poly ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1000, gamma=0.1, kernel=poly, total=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:   26.3s finished\n",
      "/Users/ZachV/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01559925, 0.01092262, 0.01244574, 0.00894423, 0.01670256,\n",
       "        0.01340017, 0.01859298, 0.01111155, 0.01456957, 0.00949898,\n",
       "        0.01189528, 0.00860424, 0.01332474, 0.01213837, 0.02739787,\n",
       "        0.07879696, 0.02139993, 0.02071843, 0.02414427, 0.01295972,\n",
       "        0.0212007 , 0.01898246, 0.0220408 , 0.02081885, 0.02157512,\n",
       "        0.024225  , 0.02920489, 0.02068968, 0.04833961, 0.01584287,\n",
       "        0.03088284, 0.28074284, 0.03257155, 0.02490916, 0.03109117,\n",
       "        0.02752404, 0.0272831 , 0.0220674 , 0.0160624 , 0.02174807,\n",
       "        0.01070223, 0.00931616, 0.0142241 , 0.01104054, 0.04581776,\n",
       "        0.02395439, 0.0450253 , 0.27587266, 0.01450543, 0.0087657 ,\n",
       "        0.01196785, 0.00853281, 0.01152024, 0.00882339, 0.01017909,\n",
       "        0.00966134, 0.01619   , 0.00941353, 0.02441034, 0.00913777,\n",
       "        0.13317142, 0.03574409, 0.0827868 , 0.51299362, 0.02388349,\n",
       "        0.01561799, 0.01983209, 0.01694822, 0.02073555, 0.01464262,\n",
       "        0.02419953, 0.00981388, 0.03020601, 0.01854935, 0.22123132,\n",
       "        0.01608524, 0.50241137, 0.03689632, 0.03779063, 0.62029562]),\n",
       " 'std_fit_time': array([4.99020179e-03, 2.51205412e-03, 1.44901297e-03, 1.00436521e-03,\n",
       "        2.24966519e-03, 2.35379836e-03, 8.70039632e-03, 1.13538153e-03,\n",
       "        2.59303749e-03, 1.18055120e-03, 2.05232611e-03, 1.82090831e-04,\n",
       "        1.98953005e-03, 3.24612314e-03, 5.55400151e-03, 1.57641752e-02,\n",
       "        4.34744204e-03, 2.28720843e-03, 6.24183999e-03, 3.72890364e-03,\n",
       "        2.78245135e-03, 3.03913495e-03, 4.27521697e-03, 2.65353727e-03,\n",
       "        4.44165420e-03, 8.06804622e-03, 1.09810828e-02, 4.47575029e-03,\n",
       "        5.68788386e-03, 4.13704564e-03, 5.58553778e-03, 1.02451665e-01,\n",
       "        8.70401346e-03, 9.54170201e-03, 6.88049040e-03, 4.75928172e-03,\n",
       "        8.12477110e-03, 5.00108364e-03, 2.55081077e-03, 7.12396809e-03,\n",
       "        2.38639892e-03, 9.79346462e-04, 9.46084258e-04, 3.26530207e-03,\n",
       "        5.30144299e-03, 3.13256157e-03, 7.47963760e-03, 8.01647599e-02,\n",
       "        3.05371026e-03, 2.27297225e-04, 7.59772274e-04, 3.07310965e-04,\n",
       "        8.74156349e-04, 2.39052740e-04, 9.25034117e-04, 1.34049561e-03,\n",
       "        6.94317417e-04, 9.89897832e-04, 1.99587356e-03, 5.43146777e-04,\n",
       "        6.45050301e-02, 1.01529011e-02, 2.04822696e-02, 1.26404696e-01,\n",
       "        5.19674864e-03, 2.56798503e-03, 2.16509511e-03, 1.75600933e-03,\n",
       "        6.96305275e-03, 9.43859011e-04, 6.43547426e-03, 1.74012705e-03,\n",
       "        3.99538708e-03, 1.63990129e-03, 2.62668156e-02, 1.06594392e-03,\n",
       "        1.56674613e-01, 3.90245471e-03, 6.61258118e-03, 1.84336638e-01]),\n",
       " 'mean_score_time': array([0.00372028, 0.00254335, 0.00249085, 0.00166492, 0.00380349,\n",
       "        0.00284572, 0.00384164, 0.00291424, 0.00371199, 0.00168285,\n",
       "        0.00208397, 0.00162153, 0.00202169, 0.002139  , 0.00467076,\n",
       "        0.00181007, 0.00532994, 0.00389619, 0.00452676, 0.00396256,\n",
       "        0.00367508, 0.00458407, 0.00393763, 0.0037468 , 0.00671477,\n",
       "        0.0034677 , 0.00564561, 0.0052773 , 0.00550413, 0.00232668,\n",
       "        0.00241857, 0.00431318, 0.00658307, 0.00384159, 0.00642266,\n",
       "        0.00583396, 0.0052434 , 0.00321503, 0.00310073, 0.0028317 ,\n",
       "        0.00195379, 0.00174618, 0.00203724, 0.00237026, 0.00484419,\n",
       "        0.00329905, 0.00456495, 0.00211086, 0.00299258, 0.00182614,\n",
       "        0.00252662, 0.00169711, 0.00234451, 0.00166922, 0.00191689,\n",
       "        0.00211902, 0.00206442, 0.00173917, 0.00217381, 0.001682  ,\n",
       "        0.00359774, 0.0023859 , 0.00586147, 0.00210228, 0.00514655,\n",
       "        0.00421405, 0.00272293, 0.00246038, 0.00382667, 0.00329132,\n",
       "        0.00237894, 0.0016223 , 0.00245957, 0.00524173, 0.00300126,\n",
       "        0.00314798, 0.00219622, 0.00156126, 0.00208931, 0.00184264]),\n",
       " 'std_score_time': array([1.62344365e-03, 9.97073015e-04, 5.80528700e-04, 9.85186882e-05,\n",
       "        1.92591189e-03, 5.73447772e-04, 1.67106791e-03, 1.66021235e-03,\n",
       "        2.49406167e-03, 6.13236686e-05, 4.12246239e-04, 3.15946990e-05,\n",
       "        2.39002828e-04, 4.93410244e-04, 2.19813924e-03, 2.27447676e-04,\n",
       "        1.90058358e-03, 1.97418334e-03, 1.92500945e-03, 2.69639815e-03,\n",
       "        1.08314476e-03, 1.58298174e-03, 1.75038849e-03, 1.63533987e-03,\n",
       "        4.85947233e-03, 1.36614740e-03, 3.21826713e-03, 2.99151147e-03,\n",
       "        3.30464639e-03, 4.14031732e-04, 1.18935882e-04, 3.08806785e-03,\n",
       "        3.53959570e-03, 2.40141802e-03, 3.25828839e-03, 2.43673981e-03,\n",
       "        2.56526499e-03, 1.00994715e-03, 4.22162833e-04, 1.01608147e-03,\n",
       "        1.36190686e-04, 1.07926168e-04, 2.73191257e-04, 9.71895674e-04,\n",
       "        3.47987561e-03, 1.39896672e-03, 2.11112638e-03, 7.21526467e-04,\n",
       "        7.83496522e-04, 1.09902907e-04, 3.77339011e-04, 2.93986254e-05,\n",
       "        3.88457802e-04, 4.80867046e-05, 4.61436971e-05, 4.65748785e-04,\n",
       "        2.37851537e-04, 1.85636362e-04, 5.41314976e-04, 6.18583700e-05,\n",
       "        1.50138671e-03, 4.62035655e-04, 3.96390949e-03, 4.91628004e-04,\n",
       "        2.13076953e-03, 2.89347749e-03, 4.94033176e-04, 9.37147668e-04,\n",
       "        1.84617593e-03, 1.98315940e-03, 7.78393087e-04, 4.63505831e-05,\n",
       "        5.83985222e-04, 3.22616026e-03, 8.54872966e-04, 1.54273034e-03,\n",
       "        1.34884632e-04, 6.08431134e-05, 1.09848151e-04, 2.58118415e-04]),\n",
       " 'param_C': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[1e-08, 1e-08, 1e-07, 1e-07, 1e-06, 1e-06, 1e-05, 1e-05,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1e-08, 1e-08, 1e-07, 1e-07, 1e-06, 1e-06, 1e-05, 1e-05,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1e-08, 1e-08, 1e-07, 1e-07, 1e-06, 1e-06, 1e-05, 1e-05,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1e-08, 1e-08, 1e-07, 1e-07, 1e-06, 1e-06, 1e-05, 1e-05,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1e-08, 1e-08, 1e-07, 1e-07, 1e-06, 1e-06, 1e-05, 1e-05,\n",
       "                    0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_kernel': masked_array(data=['rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf',\n",
       "                    'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly',\n",
       "                    'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf',\n",
       "                    'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly',\n",
       "                    'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf',\n",
       "                    'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly',\n",
       "                    'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf',\n",
       "                    'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly',\n",
       "                    'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf',\n",
       "                    'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly',\n",
       "                    'rbf', 'poly', 'rbf', 'poly', 'rbf', 'poly', 'rbf',\n",
       "                    'poly', 'rbf', 'poly'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1, 'gamma': 1e-08, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 1e-08, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 1e-07, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 1e-07, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 1e-06, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 1e-06, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 1e-05, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 1e-05, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.0001, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.001, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.01, 'kernel': 'poly'},\n",
       "  {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.1, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-08, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-08, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-07, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-06, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-05, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.01, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.1, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-08, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-08, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-07, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-06, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 1e-05, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.01, 'kernel': 'poly'},\n",
       "  {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.1, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 1e-08, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 1e-08, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 1e-07, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 1e-07, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 1e-06, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 1e-06, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 1e-05, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.0001, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.001, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.01, 'kernel': 'poly'},\n",
       "  {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.1, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 1e-08, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 1e-08, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 1e-07, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 1e-07, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 1e-06, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 1e-06, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 1e-05, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.0001, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.01, 'kernel': 'poly'},\n",
       "  {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}],\n",
       " 'split0_test_score': array([-0.07517222, -0.07520895, -0.07483932, -0.07520895, -0.0710784 ,\n",
       "        -0.07520895, -0.04068702, -0.07520895,  0.20782803, -0.07520793,\n",
       "         0.58883541, -0.07418324,  0.65400469,  0.46286075,  0.35572928,\n",
       "         0.20717897, -0.0748417 , -0.07520895, -0.07107872, -0.07520895,\n",
       "        -0.04066154, -0.07520895,  0.20925188, -0.07520894,  0.60058233,\n",
       "        -0.07519869,  0.70267738, -0.06468687,  0.72556266,  0.73985929,\n",
       "         0.43520376, -0.62620713, -0.0748417 , -0.07520895, -0.07107872,\n",
       "        -0.07520895, -0.04066154, -0.07520895,  0.20925188, -0.07520894,\n",
       "         0.60058233, -0.07519869,  0.70267738, -0.06468687,  0.72556266,\n",
       "         0.73985929,  0.43520376, -0.62620713, -0.07110343, -0.07520895,\n",
       "        -0.0406738 , -0.07520895,  0.20929032, -0.07520895,  0.60170423,\n",
       "        -0.07520885,  0.6995711 , -0.07510635,  0.76407719,  0.02246031,\n",
       "         0.87259777,  0.71550001,  0.42870455, -5.51296266, -0.0406104 ,\n",
       "        -0.07520895,  0.20926203, -0.07520895,  0.60174688, -0.07520895,\n",
       "         0.69971677, -0.07520793,  0.73103324, -0.07418324,  0.82006666,\n",
       "         0.46287143,  0.78962105,  0.2072365 ,  0.42834912, -9.85748188]),\n",
       " 'split1_test_score': array([-0.02337   , -0.02342012, -0.02291944, -0.02342012, -0.01842474,\n",
       "        -0.02342012,  0.0187454 , -0.02342012,  0.30607457, -0.02341917,\n",
       "         0.62691501, -0.02246506,  0.70022389,  0.40600894,  0.63957399,\n",
       "         0.19971819, -0.02291896, -0.02342012, -0.01842376, -0.02342012,\n",
       "         0.01876645, -0.02342012,  0.30733021, -0.02342011,  0.62441271,\n",
       "        -0.02341057,  0.67078647, -0.01482114,  0.59414216,  0.50618849,\n",
       "         0.60977164, -0.4978964 , -0.02291896, -0.02342012, -0.01842376,\n",
       "        -0.02342012,  0.01876645, -0.02342012,  0.30733021, -0.02342011,\n",
       "         0.62441271, -0.02341057,  0.67078647, -0.01482114,  0.59414216,\n",
       "         0.50618849,  0.60977164, -0.4978964 , -0.01841896, -0.02342012,\n",
       "         0.01876461, -0.02342012,  0.30748647, -0.02342012,  0.62413317,\n",
       "        -0.02342003,  0.66013118, -0.02332457,  0.6928465 ,  0.06373789,\n",
       "         0.2181698 ,  0.38193377,  0.56568371, -2.51479011,  0.01859719,\n",
       "        -0.02342012,  0.30765633, -0.02342012,  0.6242564 , -0.02342012,\n",
       "         0.65808908, -0.02341917,  0.66987994, -0.02246506,  0.53661605,\n",
       "         0.40600894,  0.12706258,  0.19973266,  0.54743341, -3.24265185]),\n",
       " 'split2_test_score': array([-4.03064182e-02, -4.03479472e-02, -3.99346433e-02, -4.03479472e-02,\n",
       "        -3.58713826e-02, -4.03479472e-02,  8.56116208e-03, -4.03479465e-02,\n",
       "         3.17609327e-01, -4.03472576e-02,  6.15746545e-01, -3.96589356e-02,\n",
       "         6.65418101e-01,  2.36573054e-01,  5.84508106e-01, -6.65646875e-01,\n",
       "        -3.99328159e-02, -4.03479472e-02, -3.58706025e-02, -4.03479472e-02,\n",
       "         8.58414287e-03, -4.03479472e-02,  3.18505314e-01, -4.03479403e-02,\n",
       "         6.10045867e-01, -4.03410518e-02,  6.42161718e-01, -3.33825433e-02,\n",
       "         6.79678747e-01,  3.27066092e-01,  5.32279890e-01, -8.22031490e+00,\n",
       "        -3.99328159e-02, -4.03479472e-02, -3.58706025e-02, -4.03479472e-02,\n",
       "         8.58414287e-03, -4.03479472e-02,  3.18505314e-01, -4.03479403e-02,\n",
       "         6.10045867e-01, -4.03410518e-02,  6.42161718e-01, -3.33825433e-02,\n",
       "         6.79678747e-01,  3.27066092e-01,  5.32279890e-01, -8.22031490e+00,\n",
       "        -3.58527943e-02, -4.03479472e-02,  8.61020141e-03, -4.03479472e-02,\n",
       "         3.18586553e-01, -4.03479471e-02,  6.09520297e-01, -4.03478782e-02,\n",
       "         6.26811452e-01, -4.02789980e-02,  6.78289205e-01,  2.48847470e-02,\n",
       "         4.28369439e-01,  1.74700780e-01,  5.05443989e-01, -6.71272862e+00,\n",
       "         8.50148098e-03, -4.03479472e-02,  3.18282705e-01, -4.03479472e-02,\n",
       "         6.09495499e-01, -4.03479465e-02,  6.25292488e-01, -4.03472576e-02,\n",
       "         6.45727506e-01, -3.96589356e-02,  5.81822335e-01,  2.36579169e-01,\n",
       "         1.03673648e-01, -6.65923146e-01,  5.05443989e-01, -2.78004295e+01]),\n",
       " 'split3_test_score': array([-0.08360682, -0.08364786, -0.08323913, -0.08364786, -0.07995061,\n",
       "        -0.08364786, -0.03891324, -0.08364786,  0.28461976, -0.08364678,\n",
       "         0.68500786, -0.08256847,  0.77787049,  0.35881403,  0.66428177,\n",
       "         0.43706267, -0.08323756, -0.08364786, -0.07995045, -0.08364786,\n",
       "        -0.03889017, -0.08364786,  0.28601691, -0.08364785,  0.68788673,\n",
       "        -0.08363705,  0.76605353, -0.07307727,  0.75313673,  0.54082094,\n",
       "         0.67079398, -0.41297166, -0.08323756, -0.08364786, -0.07995045,\n",
       "        -0.08364786, -0.03889017, -0.08364786,  0.28601691, -0.08364785,\n",
       "         0.68788673, -0.08363705,  0.76605353, -0.07307727,  0.75313673,\n",
       "         0.54082094,  0.67079398, -0.41297166, -0.07993583, -0.08364786,\n",
       "        -0.03890455, -0.08364786,  0.28617811, -0.08364786,  0.68812443,\n",
       "        -0.08364775,  0.76686203, -0.08353985,  0.74132847,  0.01233169,\n",
       "         0.6511119 ,  0.6849358 ,  0.60629008, -2.72908176, -0.03875603,\n",
       "        -0.08364786,  0.28645222, -0.08364786,  0.68822057, -0.08364786,\n",
       "         0.76512531, -0.08364678,  0.76526604, -0.08256847,  0.67495286,\n",
       "         0.35874914,  0.55775293,  0.43696695,  0.61447525, -4.68435326]),\n",
       " 'split4_test_score': array([-2.17046236e-02, -2.17349092e-02, -2.14328381e-02, -2.17349092e-02,\n",
       "        -1.87189609e-02, -2.17349092e-02,  4.62365151e-03, -2.17349082e-02,\n",
       "         1.85961406e-01, -2.17339192e-02,  4.80609132e-01, -2.07453716e-02,\n",
       "         5.49901859e-01,  4.46390641e-01,  3.86279868e-01, -4.20810678e-01,\n",
       "        -2.14320999e-02, -2.17349092e-02, -1.87187717e-02, -2.17349092e-02,\n",
       "         4.64024831e-03, -2.17349092e-02,  1.87002087e-01, -2.17348993e-02,\n",
       "         4.87511167e-01, -2.17250094e-02,  5.52380565e-01, -1.20531514e-02,\n",
       "         6.31960049e-01,  6.17179507e-01,  3.90332083e-01, -4.35440150e+00,\n",
       "        -2.14320999e-02, -2.17349092e-02, -1.87187717e-02, -2.17349092e-02,\n",
       "         4.64024831e-03, -2.17349092e-02,  1.87002087e-01, -2.17348993e-02,\n",
       "         4.87511167e-01, -2.17250094e-02,  5.52380565e-01, -1.20531514e-02,\n",
       "         6.31960049e-01,  6.17179507e-01,  3.90332083e-01, -4.35440150e+00,\n",
       "        -1.87114216e-02, -2.17349092e-02,  4.64003718e-03, -2.17349092e-02,\n",
       "         1.86996709e-01, -2.17349091e-02,  4.88595669e-01, -2.17348102e-02,\n",
       "         5.40194046e-01, -2.16359157e-02,  5.98728197e-01,  7.09321443e-02,\n",
       "         6.50915711e-01,  5.89427867e-01,  3.64684283e-01, -3.62766221e+00,\n",
       "         4.71727150e-03, -2.17349092e-02,  1.87067007e-01, -2.17349092e-02,\n",
       "         4.88624952e-01, -2.17349082e-02,  5.37911435e-01, -2.17339192e-02,\n",
       "         5.65813057e-01, -2.07453716e-02,  6.59863453e-01,  4.46391143e-01,\n",
       "         4.65256264e-01, -4.20997028e-01,  3.61878530e-01, -1.17236147e+01]),\n",
       " 'mean_test_score': array([-4.88320158e-02, -4.88719580e-02, -4.84730756e-02, -4.88719580e-02,\n",
       "        -4.48088201e-02, -4.88719580e-02, -9.53400981e-03, -4.88719570e-02,\n",
       "         2.60418621e-01, -4.88710097e-02,  5.99422794e-01, -4.79242157e-02,\n",
       "         6.69483806e-01,  3.82129485e-01,  5.26074601e-01, -4.84995453e-02,\n",
       "        -4.84726271e-02, -4.88719580e-02, -4.48084607e-02, -4.88719580e-02,\n",
       "        -9.51217211e-03, -4.88719580e-02,  2.61621279e-01, -4.88719485e-02,\n",
       "         6.02087761e-01, -4.88624754e-02,  6.66811932e-01, -3.96041960e-02,\n",
       "         6.76896071e-01,  5.46222863e-01,  5.27676270e-01, -2.82235832e+00,\n",
       "        -4.84726271e-02, -4.88719580e-02, -4.48084607e-02, -4.88719580e-02,\n",
       "        -9.51217211e-03, -4.88719580e-02,  2.61621279e-01, -4.88719485e-02,\n",
       "         6.02087761e-01, -4.88624754e-02,  6.66811932e-01, -3.96041960e-02,\n",
       "         6.76896071e-01,  5.46222863e-01,  5.27676270e-01, -2.82235832e+00,\n",
       "        -4.48044877e-02, -4.88719580e-02, -9.51269984e-03, -4.88719580e-02,\n",
       "         2.61707633e-01, -4.88719579e-02,  6.02415560e-01, -4.88718631e-02,\n",
       "         6.58713962e-01, -4.87771365e-02,  6.95053913e-01,  3.88693559e-02,\n",
       "         5.64232924e-01,  5.09299647e-01,  4.94161321e-01, -4.21944507e+00,\n",
       "        -9.51009852e-03, -4.88719580e-02,  2.61744057e-01, -4.88719580e-02,\n",
       "         6.02468858e-01, -4.88719570e-02,  6.57227015e-01, -4.88710097e-02,\n",
       "         6.75543957e-01, -4.79242157e-02,  6.54664272e-01,  3.82119963e-01,\n",
       "         4.08673295e-01, -4.85968128e-02,  4.91516060e-01, -1.14617062e+01]),\n",
       " 'std_test_score': array([0.02592292, 0.02592248, 0.02592653, 0.02592248, 0.02600659,\n",
       "        0.02592248, 0.02514458, 0.02592248, 0.05338649, 0.02592242,\n",
       "        0.06719504, 0.02586478, 0.07382222, 0.08117579, 0.12958233,\n",
       "        0.42005729, 0.02592696, 0.02592248, 0.0260069 , 0.02592248,\n",
       "        0.02514278, 0.02592248, 0.05334946, 0.02592248, 0.06488954,\n",
       "        0.0259219 , 0.07050276, 0.0251458 , 0.05840595, 0.13578773,\n",
       "        0.1045544 , 3.0827403 , 0.02592696, 0.02592248, 0.0260069 ,\n",
       "        0.02592248, 0.02514278, 0.02592248, 0.05334946, 0.02592248,\n",
       "        0.06488954, 0.0259219 , 0.07050276, 0.0251458 , 0.05840595,\n",
       "        0.13578773, 0.1045544 , 3.0827403 , 0.02601162, 0.02592248,\n",
       "        0.0251525 , 0.02592248, 0.05340227, 0.02592248, 0.06453467,\n",
       "        0.02592247, 0.0754083 , 0.02591668, 0.05739359, 0.0237299 ,\n",
       "        0.22287634, 0.20394394, 0.08816405, 1.63504672, 0.02505757,\n",
       "        0.02592248, 0.05337803, 0.02592248, 0.06455754, 0.02592248,\n",
       "        0.07576831, 0.02592242, 0.06941338, 0.02586478, 0.09699272,\n",
       "        0.08117954, 0.26186903, 0.42015809, 0.08858403, 8.75307236]),\n",
       " 'rank_test_score': array([53, 74, 49, 71, 44, 66, 38, 61, 32, 57, 15, 45,  5, 26, 21, 50, 47,\n",
       "        74, 42, 69, 35, 64, 30, 59, 13, 54,  6, 39,  2, 17, 19, 77, 47, 74,\n",
       "        42, 69, 35, 64, 30, 59, 13, 54,  6, 39,  2, 17, 19, 77, 41, 73, 37,\n",
       "        68, 29, 63, 12, 58,  8, 52,  1, 33, 16, 22, 23, 79, 34, 71, 28, 66,\n",
       "        11, 61,  9, 56,  4, 46, 10, 27, 25, 51, 24, 80], dtype=int32),\n",
       " 'split0_train_score': array([-0.04620157, -0.04624157, -0.04583944, -0.04624157, -0.04181101,\n",
       "        -0.04624157, -0.00871477, -0.04624156,  0.24685196, -0.04623881,\n",
       "         0.58769216, -0.04358386,  0.69204779,  0.39957098,  0.73979682,\n",
       "         0.93662892, -0.0458417 , -0.04624157, -0.04181131, -0.04624157,\n",
       "        -0.00869177, -0.04624157,  0.24799012, -0.04624154,  0.59171759,\n",
       "        -0.04621405,  0.66359503, -0.0271169 ,  0.77501876,  0.63796649,\n",
       "         0.9714445 ,  0.97164196, -0.0458417 , -0.04624157, -0.04181131,\n",
       "        -0.04624157, -0.00869177, -0.04624157,  0.24799012, -0.04624154,\n",
       "         0.59171759, -0.04621405,  0.66359503, -0.0271169 ,  0.77501876,\n",
       "         0.63796649,  0.9714445 ,  0.97164196, -0.04183481, -0.04624157,\n",
       "        -0.00875644, -0.04624157,  0.24800959, -0.04624156,  0.59219086,\n",
       "        -0.04624129,  0.64982038, -0.04596726,  0.71524295,  0.07812657,\n",
       "         0.91048453,  0.86993264,  0.98838055,  0.98817087, -0.00869535,\n",
       "        -0.04624157,  0.2479778 , -0.04624157,  0.59222747, -0.04624156,\n",
       "         0.64837188, -0.04623881,  0.66813007, -0.04358386,  0.76979138,\n",
       "         0.39957759,  0.95171048,  0.93663029,  0.99053035,  0.99000553]),\n",
       " 'split1_train_score': array([-0.03837137, -0.03841412, -0.03798722, -0.03841412, -0.03415362,\n",
       "        -0.03841412, -0.00306835, -0.03841411,  0.24838876, -0.03841167,\n",
       "         0.58306984, -0.03604559,  0.69730194,  0.45864555,  0.6706902 ,\n",
       "         0.96084789, -0.03798675, -0.03841412, -0.03415265, -0.03841412,\n",
       "        -0.00304668, -0.03841412,  0.2498576 , -0.03841409,  0.58517672,\n",
       "        -0.03838964,  0.67972551, -0.02195651,  0.8660661 ,  0.72888082,\n",
       "         0.98037199,  0.981128  , -0.03798675, -0.03841412, -0.03415265,\n",
       "        -0.03841412, -0.00304668, -0.03841412,  0.2498576 , -0.03841409,\n",
       "         0.58517672, -0.03838964,  0.67972551, -0.02195651,  0.8660661 ,\n",
       "         0.72888082,  0.98037199,  0.981128  , -0.03414797, -0.03841412,\n",
       "        -0.00304825, -0.03841412,  0.25003347, -0.03841411,  0.58499887,\n",
       "        -0.03841387,  0.66788366, -0.03817012,  0.76390261,  0.08189258,\n",
       "         0.93864884,  0.90578547,  0.99150105,  0.99088758, -0.00320676,\n",
       "        -0.03841412,  0.25018838, -0.03841412,  0.58505225, -0.03841411,\n",
       "         0.66621736, -0.03841167,  0.68799726, -0.03604559,  0.86711722,\n",
       "         0.45864555,  0.96655994,  0.96084929,  0.99208539,  0.99167853]),\n",
       " 'split2_train_score': array([-0.04547162, -0.04550116, -0.04520738, -0.04550116, -0.04227192,\n",
       "        -0.04550116, -0.00941117, -0.04550116,  0.235399  , -0.04549955,\n",
       "         0.59967124, -0.04388736,  0.68393149,  0.46568632,  0.68368026,\n",
       "         0.95922094, -0.04520585, -0.04550116, -0.04227125, -0.04550116,\n",
       "        -0.00938972, -0.04550116,  0.23663318, -0.04550115,  0.60443141,\n",
       "        -0.045485  ,  0.68274467, -0.02946881,  0.8425864 ,  0.73522216,\n",
       "         0.98690931,  0.98377421, -0.04520585, -0.04550116, -0.04227125,\n",
       "        -0.04550116, -0.00938972, -0.04550116,  0.23663318, -0.04550115,\n",
       "         0.60443141, -0.045485  ,  0.68274467, -0.02946881,  0.8425864 ,\n",
       "         0.73522216,  0.98690931,  0.98377421, -0.04225637, -0.04550116,\n",
       "        -0.00936795, -0.04550116,  0.23675063, -0.04550116,  0.60501904,\n",
       "        -0.045501  ,  0.67332829, -0.04533957,  0.75927553,  0.09423147,\n",
       "         0.94986545,  0.91423327,  0.99226781,  0.99128314, -0.00945768,\n",
       "        -0.04550116,  0.23652975, -0.04550116,  0.60503303, -0.04550116,\n",
       "         0.67135711, -0.04549955,  0.69127409, -0.04388736,  0.84815421,\n",
       "         0.46569203,  0.96907356,  0.95922115,  0.99226781,  0.99166993]),\n",
       " 'split3_train_score': array([-0.03813574, -0.03816833, -0.03784335, -0.03816833, -0.03513737,\n",
       "        -0.03816833, -0.00382536, -0.03816833,  0.23580066, -0.03816602,\n",
       "         0.54475897, -0.03593867,  0.67469131,  0.46125437,  0.66780264,\n",
       "         0.94875954, -0.03784249, -0.03816833, -0.0351372 , -0.03816833,\n",
       "        -0.00380458, -0.03816833,  0.23708861, -0.0381683 ,  0.55040845,\n",
       "        -0.03814524,  0.65501286, -0.02242933,  0.83182577,  0.71334901,\n",
       "         0.97567848,  0.97268038, -0.03784249, -0.03816833, -0.0351372 ,\n",
       "        -0.03816833, -0.00380458, -0.03816833,  0.23708861, -0.0381683 ,\n",
       "         0.55040845, -0.03814524,  0.65501286, -0.02242933,  0.83182577,\n",
       "         0.71334901,  0.97567848,  0.97268038, -0.03512913, -0.03816833,\n",
       "        -0.00381185, -0.03816833,  0.23723106, -0.03816833,  0.55109486,\n",
       "        -0.0381681 ,  0.63639504, -0.03793816,  0.75809371,  0.07061444,\n",
       "         0.93424241,  0.89588862,  0.99018458,  0.99029987, -0.00372862,\n",
       "        -0.03816833,  0.23740702, -0.03816833,  0.55118781, -0.03816833,\n",
       "         0.63212436, -0.03816602,  0.66952978, -0.03593867,  0.83272894,\n",
       "         0.46120239,  0.95605365,  0.94876451,  0.99181976,  0.99121676]),\n",
       " 'split4_train_score': array([-0.04827127, -0.04831987, -0.04783551, -0.04831987, -0.04348535,\n",
       "        -0.04831987, -0.00756198, -0.04831987,  0.26997435, -0.04831743,\n",
       "         0.64912736, -0.04598058,  0.74639925,  0.43369871,  0.76385846,\n",
       "         0.95061651, -0.04783399, -0.04831987, -0.04348511, -0.04831987,\n",
       "        -0.00754216, -0.04831987,  0.27105376, -0.04831984,  0.65145046,\n",
       "        -0.04829548,  0.73309647, -0.03336617,  0.85724591,  0.71178523,\n",
       "         0.98148342,  0.98087272, -0.04783399, -0.04831987, -0.04348511,\n",
       "        -0.04831987, -0.00754216, -0.04831987,  0.27105376, -0.04831984,\n",
       "         0.65145046, -0.04829548,  0.73309647, -0.03336617,  0.85724591,\n",
       "         0.71178523,  0.98148342,  0.98087272, -0.04346992, -0.04831987,\n",
       "        -0.00754399, -0.04831987,  0.27099751, -0.04831987,  0.65182692,\n",
       "        -0.04831962,  0.71560573, -0.04807689,  0.7981582 ,  0.05375119,\n",
       "         0.92104379,  0.88138802,  0.98969387,  0.98882886, -0.00740829,\n",
       "        -0.04831987,  0.27112478, -0.04831987,  0.6518437 , -0.04831987,\n",
       "         0.71286934, -0.04831743,  0.74559281, -0.04598058,  0.86802403,\n",
       "         0.43369953,  0.96241372,  0.95061735,  0.98984231,  0.98904926]),\n",
       " 'mean_train_score': array([-0.04329031, -0.04332901, -0.04294258, -0.04332901, -0.03937185,\n",
       "        -0.04332901, -0.00651633, -0.04332901,  0.24728295, -0.04332669,\n",
       "         0.59286391, -0.04108721,  0.69887435,  0.44377119,  0.70516567,\n",
       "         0.95121476, -0.04294215, -0.04332901, -0.0393715 , -0.04332901,\n",
       "        -0.00649498, -0.04332901,  0.24852465, -0.04332898,  0.59663693,\n",
       "        -0.04330588,  0.68283491, -0.02686754,  0.83454859,  0.70544074,\n",
       "         0.97917754,  0.97801945, -0.04294215, -0.04332901, -0.0393715 ,\n",
       "        -0.04332901, -0.00649498, -0.04332901,  0.24852465, -0.04332898,\n",
       "         0.59663693, -0.04330588,  0.68283491, -0.02686754,  0.83454859,\n",
       "         0.70544074,  0.97917754,  0.97801945, -0.03936764, -0.04332901,\n",
       "        -0.0065057 , -0.04332901,  0.24860445, -0.04332901,  0.59702611,\n",
       "        -0.04332878,  0.66860662, -0.0430984 ,  0.7589346 ,  0.07572325,\n",
       "         0.930857  ,  0.8934456 ,  0.99040557,  0.98989407, -0.00649934,\n",
       "        -0.04332901,  0.24864555, -0.04332901,  0.59706885, -0.04332901,\n",
       "         0.66618801, -0.04332669,  0.6925048 , -0.04108721,  0.83716316,\n",
       "         0.44376342,  0.96116227,  0.95121652,  0.99130912,  0.990724  ]),\n",
       " 'std_train_score': array([0.00421448, 0.0042166 , 0.0041957 , 0.0042166 , 0.00391003,\n",
       "        0.0042166 , 0.00258598, 0.0042166 , 0.01256487, 0.00421661,\n",
       "        0.03361152, 0.00424133, 0.02496317, 0.02474791, 0.0392182 ,\n",
       "        0.00867213, 0.00419582, 0.0042166 , 0.00391021, 0.0042166 ,\n",
       "        0.00258577, 0.0042166 , 0.01250451, 0.0042166 , 0.03273495,\n",
       "        0.00421672, 0.02712984, 0.00430992, 0.03201357, 0.03490541,\n",
       "        0.00526436, 0.00490101, 0.00419582, 0.0042166 , 0.00391021,\n",
       "        0.0042166 , 0.00258577, 0.0042166 , 0.01250451, 0.0042166 ,\n",
       "        0.03273495, 0.00421672, 0.02712984, 0.00430992, 0.03201357,\n",
       "        0.03490541, 0.00526436, 0.00490101, 0.00391076, 0.0042166 ,\n",
       "        0.00259026, 0.0042166 , 0.01243955, 0.0042166 , 0.03269474,\n",
       "        0.0042166 , 0.02691558, 0.00421793, 0.02635509, 0.01338105,\n",
       "        0.0137465 , 0.01605374, 0.00136558, 0.00119878, 0.00256587,\n",
       "        0.0042166 , 0.01249991, 0.0042166 , 0.03266992, 0.0042166 ,\n",
       "        0.02714264, 0.00421661, 0.02815013, 0.00424133, 0.03613602,\n",
       "        0.02473915, 0.00646102, 0.00867172, 0.00095294, 0.00103631])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grid = {\n",
    "    \"kernel\": [\"rbf\", \"poly\"],\n",
    "    \"C\": [1,10,10,100,1000],\n",
    "    \"gamma\": [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "X = X_train.drop('SoldPrice', axis = 1)\n",
    "y = X_train.SoldPrice\n",
    "y = np.asarray(y)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = sc_y.fit_transform(y)\n",
    "\n",
    "regressor = SVR()\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator = regressor, param_grid= grid, cv =5, return_train_score= True, verbose= 2)\n",
    "grid_search_cv.fit(X, y)\n",
    "grid_search_cv.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>58</th>\n",
       "      <th>44</th>\n",
       "      <th>28</th>\n",
       "      <th>72</th>\n",
       "      <th>12</th>\n",
       "      <th>26</th>\n",
       "      <th>42</th>\n",
       "      <th>56</th>\n",
       "      <th>70</th>\n",
       "      <th>74</th>\n",
       "      <th>68</th>\n",
       "      <th>54</th>\n",
       "      <th>40</th>\n",
       "      <th>24</th>\n",
       "      <th>10</th>\n",
       "      <th>60</th>\n",
       "      <th>45</th>\n",
       "      <th>29</th>\n",
       "      <th>46</th>\n",
       "      <th>30</th>\n",
       "      <th>14</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>78</th>\n",
       "      <th>76</th>\n",
       "      <th>13</th>\n",
       "      <th>75</th>\n",
       "      <th>66</th>\n",
       "      <th>52</th>\n",
       "      <th>22</th>\n",
       "      <th>38</th>\n",
       "      <th>8</th>\n",
       "      <th>59</th>\n",
       "      <th>64</th>\n",
       "      <th>20</th>\n",
       "      <th>36</th>\n",
       "      <th>50</th>\n",
       "      <th>6</th>\n",
       "      <th>27</th>\n",
       "      <th>43</th>\n",
       "      <th>48</th>\n",
       "      <th>34</th>\n",
       "      <th>18</th>\n",
       "      <th>4</th>\n",
       "      <th>11</th>\n",
       "      <th>73</th>\n",
       "      <th>16</th>\n",
       "      <th>32</th>\n",
       "      <th>2</th>\n",
       "      <th>15</th>\n",
       "      <th>77</th>\n",
       "      <th>57</th>\n",
       "      <th>0</th>\n",
       "      <th>41</th>\n",
       "      <th>25</th>\n",
       "      <th>71</th>\n",
       "      <th>9</th>\n",
       "      <th>55</th>\n",
       "      <th>23</th>\n",
       "      <th>39</th>\n",
       "      <th>7</th>\n",
       "      <th>69</th>\n",
       "      <th>53</th>\n",
       "      <th>37</th>\n",
       "      <th>21</th>\n",
       "      <th>5</th>\n",
       "      <th>67</th>\n",
       "      <th>51</th>\n",
       "      <th>35</th>\n",
       "      <th>19</th>\n",
       "      <th>65</th>\n",
       "      <th>3</th>\n",
       "      <th>49</th>\n",
       "      <th>1</th>\n",
       "      <th>17</th>\n",
       "      <th>33</th>\n",
       "      <th>31</th>\n",
       "      <th>47</th>\n",
       "      <th>63</th>\n",
       "      <th>79</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.0244103</td>\n",
       "      <td>0.0458178</td>\n",
       "      <td>0.0483396</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.0133247</td>\n",
       "      <td>0.0292049</td>\n",
       "      <td>0.0142241</td>\n",
       "      <td>0.01619</td>\n",
       "      <td>0.0241995</td>\n",
       "      <td>0.221231</td>\n",
       "      <td>0.0207355</td>\n",
       "      <td>0.0101791</td>\n",
       "      <td>0.0107022</td>\n",
       "      <td>0.0215751</td>\n",
       "      <td>0.0118953</td>\n",
       "      <td>0.133171</td>\n",
       "      <td>0.0239544</td>\n",
       "      <td>0.0158429</td>\n",
       "      <td>0.0450253</td>\n",
       "      <td>0.0308828</td>\n",
       "      <td>0.0273979</td>\n",
       "      <td>0.0357441</td>\n",
       "      <td>0.0827868</td>\n",
       "      <td>0.0377906</td>\n",
       "      <td>0.502411</td>\n",
       "      <td>0.0121384</td>\n",
       "      <td>0.0160852</td>\n",
       "      <td>0.0198321</td>\n",
       "      <td>0.0115202</td>\n",
       "      <td>0.0220408</td>\n",
       "      <td>0.0160624</td>\n",
       "      <td>0.0145696</td>\n",
       "      <td>0.00913777</td>\n",
       "      <td>0.0238835</td>\n",
       "      <td>0.0212007</td>\n",
       "      <td>0.0272831</td>\n",
       "      <td>0.0119678</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.0206897</td>\n",
       "      <td>0.0110405</td>\n",
       "      <td>0.0145054</td>\n",
       "      <td>0.0310912</td>\n",
       "      <td>0.0241443</td>\n",
       "      <td>0.0167026</td>\n",
       "      <td>0.00860424</td>\n",
       "      <td>0.0185493</td>\n",
       "      <td>0.0213999</td>\n",
       "      <td>0.0325716</td>\n",
       "      <td>0.0124457</td>\n",
       "      <td>0.078797</td>\n",
       "      <td>0.0368963</td>\n",
       "      <td>0.00941353</td>\n",
       "      <td>0.0155993</td>\n",
       "      <td>0.00931616</td>\n",
       "      <td>0.024225</td>\n",
       "      <td>0.00981388</td>\n",
       "      <td>0.00949898</td>\n",
       "      <td>0.00966134</td>\n",
       "      <td>0.0208189</td>\n",
       "      <td>0.0217481</td>\n",
       "      <td>0.0111115</td>\n",
       "      <td>0.0146426</td>\n",
       "      <td>0.00882339</td>\n",
       "      <td>0.0220674</td>\n",
       "      <td>0.0189825</td>\n",
       "      <td>0.0134002</td>\n",
       "      <td>0.0169482</td>\n",
       "      <td>0.00853281</td>\n",
       "      <td>0.027524</td>\n",
       "      <td>0.0129597</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.00894423</td>\n",
       "      <td>0.0087657</td>\n",
       "      <td>0.0109226</td>\n",
       "      <td>0.0207184</td>\n",
       "      <td>0.0249092</td>\n",
       "      <td>0.280743</td>\n",
       "      <td>0.275873</td>\n",
       "      <td>0.512994</td>\n",
       "      <td>0.620296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.00199587</td>\n",
       "      <td>0.00530144</td>\n",
       "      <td>0.00568788</td>\n",
       "      <td>0.00399539</td>\n",
       "      <td>0.00198953</td>\n",
       "      <td>0.0109811</td>\n",
       "      <td>0.000946084</td>\n",
       "      <td>0.000694317</td>\n",
       "      <td>0.00643547</td>\n",
       "      <td>0.0262668</td>\n",
       "      <td>0.00696305</td>\n",
       "      <td>0.000925034</td>\n",
       "      <td>0.0023864</td>\n",
       "      <td>0.00444165</td>\n",
       "      <td>0.00205233</td>\n",
       "      <td>0.064505</td>\n",
       "      <td>0.00313256</td>\n",
       "      <td>0.00413705</td>\n",
       "      <td>0.00747964</td>\n",
       "      <td>0.00558554</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.0101529</td>\n",
       "      <td>0.0204823</td>\n",
       "      <td>0.00661258</td>\n",
       "      <td>0.156675</td>\n",
       "      <td>0.00324612</td>\n",
       "      <td>0.00106594</td>\n",
       "      <td>0.0021651</td>\n",
       "      <td>0.000874156</td>\n",
       "      <td>0.00427522</td>\n",
       "      <td>0.00255081</td>\n",
       "      <td>0.00259304</td>\n",
       "      <td>0.000543147</td>\n",
       "      <td>0.00519675</td>\n",
       "      <td>0.00278245</td>\n",
       "      <td>0.00812477</td>\n",
       "      <td>0.000759772</td>\n",
       "      <td>0.0087004</td>\n",
       "      <td>0.00447575</td>\n",
       "      <td>0.0032653</td>\n",
       "      <td>0.00305371</td>\n",
       "      <td>0.00688049</td>\n",
       "      <td>0.00624184</td>\n",
       "      <td>0.00224967</td>\n",
       "      <td>0.000182091</td>\n",
       "      <td>0.0016399</td>\n",
       "      <td>0.00434744</td>\n",
       "      <td>0.00870401</td>\n",
       "      <td>0.00144901</td>\n",
       "      <td>0.0157642</td>\n",
       "      <td>0.00390245</td>\n",
       "      <td>0.000989898</td>\n",
       "      <td>0.0049902</td>\n",
       "      <td>0.000979346</td>\n",
       "      <td>0.00806805</td>\n",
       "      <td>0.00174013</td>\n",
       "      <td>0.00118055</td>\n",
       "      <td>0.0013405</td>\n",
       "      <td>0.00265354</td>\n",
       "      <td>0.00712397</td>\n",
       "      <td>0.00113538</td>\n",
       "      <td>0.000943859</td>\n",
       "      <td>0.000239053</td>\n",
       "      <td>0.00500108</td>\n",
       "      <td>0.00303913</td>\n",
       "      <td>0.0023538</td>\n",
       "      <td>0.00175601</td>\n",
       "      <td>0.000307311</td>\n",
       "      <td>0.00475928</td>\n",
       "      <td>0.0037289</td>\n",
       "      <td>0.00256799</td>\n",
       "      <td>0.00100437</td>\n",
       "      <td>0.000227297</td>\n",
       "      <td>0.00251205</td>\n",
       "      <td>0.00228721</td>\n",
       "      <td>0.0095417</td>\n",
       "      <td>0.102452</td>\n",
       "      <td>0.0801648</td>\n",
       "      <td>0.126405</td>\n",
       "      <td>0.184337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.00217381</td>\n",
       "      <td>0.00484419</td>\n",
       "      <td>0.00550413</td>\n",
       "      <td>0.00245957</td>\n",
       "      <td>0.00202169</td>\n",
       "      <td>0.00564561</td>\n",
       "      <td>0.00203724</td>\n",
       "      <td>0.00206442</td>\n",
       "      <td>0.00237894</td>\n",
       "      <td>0.00300126</td>\n",
       "      <td>0.00382667</td>\n",
       "      <td>0.00191689</td>\n",
       "      <td>0.00195379</td>\n",
       "      <td>0.00671477</td>\n",
       "      <td>0.00208397</td>\n",
       "      <td>0.00359774</td>\n",
       "      <td>0.00329905</td>\n",
       "      <td>0.00232668</td>\n",
       "      <td>0.00456495</td>\n",
       "      <td>0.00241857</td>\n",
       "      <td>0.00467076</td>\n",
       "      <td>0.0023859</td>\n",
       "      <td>0.00586147</td>\n",
       "      <td>0.00208931</td>\n",
       "      <td>0.00219622</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.00314798</td>\n",
       "      <td>0.00272293</td>\n",
       "      <td>0.00234451</td>\n",
       "      <td>0.00393763</td>\n",
       "      <td>0.00310073</td>\n",
       "      <td>0.00371199</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.00514655</td>\n",
       "      <td>0.00367508</td>\n",
       "      <td>0.0052434</td>\n",
       "      <td>0.00252662</td>\n",
       "      <td>0.00384164</td>\n",
       "      <td>0.0052773</td>\n",
       "      <td>0.00237026</td>\n",
       "      <td>0.00299258</td>\n",
       "      <td>0.00642266</td>\n",
       "      <td>0.00452676</td>\n",
       "      <td>0.00380349</td>\n",
       "      <td>0.00162153</td>\n",
       "      <td>0.00524173</td>\n",
       "      <td>0.00532994</td>\n",
       "      <td>0.00658307</td>\n",
       "      <td>0.00249085</td>\n",
       "      <td>0.00181007</td>\n",
       "      <td>0.00156126</td>\n",
       "      <td>0.00173917</td>\n",
       "      <td>0.00372028</td>\n",
       "      <td>0.00174618</td>\n",
       "      <td>0.0034677</td>\n",
       "      <td>0.0016223</td>\n",
       "      <td>0.00168285</td>\n",
       "      <td>0.00211902</td>\n",
       "      <td>0.0037468</td>\n",
       "      <td>0.0028317</td>\n",
       "      <td>0.00291424</td>\n",
       "      <td>0.00329132</td>\n",
       "      <td>0.00166922</td>\n",
       "      <td>0.00321503</td>\n",
       "      <td>0.00458407</td>\n",
       "      <td>0.00284572</td>\n",
       "      <td>0.00246038</td>\n",
       "      <td>0.00169711</td>\n",
       "      <td>0.00583396</td>\n",
       "      <td>0.00396256</td>\n",
       "      <td>0.00421405</td>\n",
       "      <td>0.00166492</td>\n",
       "      <td>0.00182614</td>\n",
       "      <td>0.00254335</td>\n",
       "      <td>0.00389619</td>\n",
       "      <td>0.00384159</td>\n",
       "      <td>0.00431318</td>\n",
       "      <td>0.00211086</td>\n",
       "      <td>0.00210228</td>\n",
       "      <td>0.00184264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000541315</td>\n",
       "      <td>0.00347988</td>\n",
       "      <td>0.00330465</td>\n",
       "      <td>0.000583985</td>\n",
       "      <td>0.000239003</td>\n",
       "      <td>0.00321827</td>\n",
       "      <td>0.000273191</td>\n",
       "      <td>0.000237852</td>\n",
       "      <td>0.000778393</td>\n",
       "      <td>0.000854873</td>\n",
       "      <td>0.00184618</td>\n",
       "      <td>4.61437e-05</td>\n",
       "      <td>0.000136191</td>\n",
       "      <td>0.00485947</td>\n",
       "      <td>0.000412246</td>\n",
       "      <td>0.00150139</td>\n",
       "      <td>0.00139897</td>\n",
       "      <td>0.000414032</td>\n",
       "      <td>0.00211113</td>\n",
       "      <td>0.000118936</td>\n",
       "      <td>0.00219814</td>\n",
       "      <td>0.000462036</td>\n",
       "      <td>0.00396391</td>\n",
       "      <td>0.000109848</td>\n",
       "      <td>0.000134885</td>\n",
       "      <td>0.00049341</td>\n",
       "      <td>0.00154273</td>\n",
       "      <td>0.000494033</td>\n",
       "      <td>0.000388458</td>\n",
       "      <td>0.00175039</td>\n",
       "      <td>0.000422163</td>\n",
       "      <td>0.00249406</td>\n",
       "      <td>6.18584e-05</td>\n",
       "      <td>0.00213077</td>\n",
       "      <td>0.00108314</td>\n",
       "      <td>0.00256526</td>\n",
       "      <td>0.000377339</td>\n",
       "      <td>0.00167107</td>\n",
       "      <td>0.00299151</td>\n",
       "      <td>0.000971896</td>\n",
       "      <td>0.000783497</td>\n",
       "      <td>0.00325829</td>\n",
       "      <td>0.00192501</td>\n",
       "      <td>0.00192591</td>\n",
       "      <td>3.15947e-05</td>\n",
       "      <td>0.00322616</td>\n",
       "      <td>0.00190058</td>\n",
       "      <td>0.0035396</td>\n",
       "      <td>0.000580529</td>\n",
       "      <td>0.000227448</td>\n",
       "      <td>6.08431e-05</td>\n",
       "      <td>0.000185636</td>\n",
       "      <td>0.00162344</td>\n",
       "      <td>0.000107926</td>\n",
       "      <td>0.00136615</td>\n",
       "      <td>4.63506e-05</td>\n",
       "      <td>6.13237e-05</td>\n",
       "      <td>0.000465749</td>\n",
       "      <td>0.00163534</td>\n",
       "      <td>0.00101608</td>\n",
       "      <td>0.00166021</td>\n",
       "      <td>0.00198316</td>\n",
       "      <td>4.80867e-05</td>\n",
       "      <td>0.00100995</td>\n",
       "      <td>0.00158298</td>\n",
       "      <td>0.000573448</td>\n",
       "      <td>0.000937148</td>\n",
       "      <td>2.93986e-05</td>\n",
       "      <td>0.00243674</td>\n",
       "      <td>0.0026964</td>\n",
       "      <td>0.00289348</td>\n",
       "      <td>9.85187e-05</td>\n",
       "      <td>0.000109903</td>\n",
       "      <td>0.000997073</td>\n",
       "      <td>0.00197418</td>\n",
       "      <td>0.00240142</td>\n",
       "      <td>0.00308807</td>\n",
       "      <td>0.000721526</td>\n",
       "      <td>0.000491628</td>\n",
       "      <td>0.000258118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_C</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_gamma</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_kernel</th>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>rbf</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "      <td>poly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-06, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-07, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-06, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-08, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-07, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-05, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-08, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-06, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.0001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-08, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-08, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-07, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.0001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-08, 'kernel': 'rbf'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.0001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.0001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-05, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1, 'gamma': 0.0001, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-05, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-05, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-05, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-05, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-06, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-06, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-06, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-06, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-06, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-07, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-07, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-07, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-07, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 1e-08, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-07, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 1e-08, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1, 'gamma': 1e-08, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-08, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 1e-08, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 100, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.764077</td>\n",
       "      <td>0.725563</td>\n",
       "      <td>0.725563</td>\n",
       "      <td>0.731033</td>\n",
       "      <td>0.654005</td>\n",
       "      <td>0.702677</td>\n",
       "      <td>0.702677</td>\n",
       "      <td>0.699571</td>\n",
       "      <td>0.699717</td>\n",
       "      <td>0.820067</td>\n",
       "      <td>0.601747</td>\n",
       "      <td>0.601704</td>\n",
       "      <td>0.600582</td>\n",
       "      <td>0.600582</td>\n",
       "      <td>0.588835</td>\n",
       "      <td>0.872598</td>\n",
       "      <td>0.739859</td>\n",
       "      <td>0.739859</td>\n",
       "      <td>0.435204</td>\n",
       "      <td>0.435204</td>\n",
       "      <td>0.355729</td>\n",
       "      <td>0.7155</td>\n",
       "      <td>0.428705</td>\n",
       "      <td>0.428349</td>\n",
       "      <td>0.789621</td>\n",
       "      <td>0.462861</td>\n",
       "      <td>0.462871</td>\n",
       "      <td>0.209262</td>\n",
       "      <td>0.20929</td>\n",
       "      <td>0.209252</td>\n",
       "      <td>0.209252</td>\n",
       "      <td>0.207828</td>\n",
       "      <td>0.0224603</td>\n",
       "      <td>-0.0406104</td>\n",
       "      <td>-0.0406615</td>\n",
       "      <td>-0.0406615</td>\n",
       "      <td>-0.0406738</td>\n",
       "      <td>-0.040687</td>\n",
       "      <td>-0.0646869</td>\n",
       "      <td>-0.0646869</td>\n",
       "      <td>-0.0711034</td>\n",
       "      <td>-0.0710787</td>\n",
       "      <td>-0.0710787</td>\n",
       "      <td>-0.0710784</td>\n",
       "      <td>-0.0741832</td>\n",
       "      <td>-0.0741832</td>\n",
       "      <td>-0.0748417</td>\n",
       "      <td>-0.0748417</td>\n",
       "      <td>-0.0748393</td>\n",
       "      <td>0.207179</td>\n",
       "      <td>0.207236</td>\n",
       "      <td>-0.0751064</td>\n",
       "      <td>-0.0751722</td>\n",
       "      <td>-0.0751987</td>\n",
       "      <td>-0.0751987</td>\n",
       "      <td>-0.0752079</td>\n",
       "      <td>-0.0752079</td>\n",
       "      <td>-0.0752089</td>\n",
       "      <td>-0.0752089</td>\n",
       "      <td>-0.0752089</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.075209</td>\n",
       "      <td>-0.626207</td>\n",
       "      <td>-0.626207</td>\n",
       "      <td>-5.51296</td>\n",
       "      <td>-9.85748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.692847</td>\n",
       "      <td>0.594142</td>\n",
       "      <td>0.594142</td>\n",
       "      <td>0.66988</td>\n",
       "      <td>0.700224</td>\n",
       "      <td>0.670786</td>\n",
       "      <td>0.670786</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.658089</td>\n",
       "      <td>0.536616</td>\n",
       "      <td>0.624256</td>\n",
       "      <td>0.624133</td>\n",
       "      <td>0.624413</td>\n",
       "      <td>0.624413</td>\n",
       "      <td>0.626915</td>\n",
       "      <td>0.21817</td>\n",
       "      <td>0.506188</td>\n",
       "      <td>0.506188</td>\n",
       "      <td>0.609772</td>\n",
       "      <td>0.609772</td>\n",
       "      <td>0.639574</td>\n",
       "      <td>0.381934</td>\n",
       "      <td>0.565684</td>\n",
       "      <td>0.547433</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.406009</td>\n",
       "      <td>0.406009</td>\n",
       "      <td>0.307656</td>\n",
       "      <td>0.307486</td>\n",
       "      <td>0.30733</td>\n",
       "      <td>0.30733</td>\n",
       "      <td>0.306075</td>\n",
       "      <td>0.0637379</td>\n",
       "      <td>0.0185972</td>\n",
       "      <td>0.0187665</td>\n",
       "      <td>0.0187665</td>\n",
       "      <td>0.0187646</td>\n",
       "      <td>0.0187454</td>\n",
       "      <td>-0.0148211</td>\n",
       "      <td>-0.0148211</td>\n",
       "      <td>-0.018419</td>\n",
       "      <td>-0.0184238</td>\n",
       "      <td>-0.0184238</td>\n",
       "      <td>-0.0184247</td>\n",
       "      <td>-0.0224651</td>\n",
       "      <td>-0.0224651</td>\n",
       "      <td>-0.022919</td>\n",
       "      <td>-0.022919</td>\n",
       "      <td>-0.0229194</td>\n",
       "      <td>0.199718</td>\n",
       "      <td>0.199733</td>\n",
       "      <td>-0.0233246</td>\n",
       "      <td>-0.02337</td>\n",
       "      <td>-0.0234106</td>\n",
       "      <td>-0.0234106</td>\n",
       "      <td>-0.0234192</td>\n",
       "      <td>-0.0234192</td>\n",
       "      <td>-0.02342</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.0234201</td>\n",
       "      <td>-0.497896</td>\n",
       "      <td>-0.497896</td>\n",
       "      <td>-2.51479</td>\n",
       "      <td>-3.24265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.678289</td>\n",
       "      <td>0.679679</td>\n",
       "      <td>0.679679</td>\n",
       "      <td>0.645728</td>\n",
       "      <td>0.665418</td>\n",
       "      <td>0.642162</td>\n",
       "      <td>0.642162</td>\n",
       "      <td>0.626811</td>\n",
       "      <td>0.625292</td>\n",
       "      <td>0.581822</td>\n",
       "      <td>0.609495</td>\n",
       "      <td>0.60952</td>\n",
       "      <td>0.610046</td>\n",
       "      <td>0.610046</td>\n",
       "      <td>0.615747</td>\n",
       "      <td>0.428369</td>\n",
       "      <td>0.327066</td>\n",
       "      <td>0.327066</td>\n",
       "      <td>0.53228</td>\n",
       "      <td>0.53228</td>\n",
       "      <td>0.584508</td>\n",
       "      <td>0.174701</td>\n",
       "      <td>0.505444</td>\n",
       "      <td>0.505444</td>\n",
       "      <td>0.103674</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.236579</td>\n",
       "      <td>0.318283</td>\n",
       "      <td>0.318587</td>\n",
       "      <td>0.318505</td>\n",
       "      <td>0.318505</td>\n",
       "      <td>0.317609</td>\n",
       "      <td>0.0248847</td>\n",
       "      <td>0.00850148</td>\n",
       "      <td>0.00858414</td>\n",
       "      <td>0.00858414</td>\n",
       "      <td>0.0086102</td>\n",
       "      <td>0.00856116</td>\n",
       "      <td>-0.0333825</td>\n",
       "      <td>-0.0333825</td>\n",
       "      <td>-0.0358528</td>\n",
       "      <td>-0.0358706</td>\n",
       "      <td>-0.0358706</td>\n",
       "      <td>-0.0358714</td>\n",
       "      <td>-0.0396589</td>\n",
       "      <td>-0.0396589</td>\n",
       "      <td>-0.0399328</td>\n",
       "      <td>-0.0399328</td>\n",
       "      <td>-0.0399346</td>\n",
       "      <td>-0.665647</td>\n",
       "      <td>-0.665923</td>\n",
       "      <td>-0.040279</td>\n",
       "      <td>-0.0403064</td>\n",
       "      <td>-0.0403411</td>\n",
       "      <td>-0.0403411</td>\n",
       "      <td>-0.0403473</td>\n",
       "      <td>-0.0403473</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-0.0403479</td>\n",
       "      <td>-8.22031</td>\n",
       "      <td>-8.22031</td>\n",
       "      <td>-6.71273</td>\n",
       "      <td>-27.8004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.741328</td>\n",
       "      <td>0.753137</td>\n",
       "      <td>0.753137</td>\n",
       "      <td>0.765266</td>\n",
       "      <td>0.77787</td>\n",
       "      <td>0.766054</td>\n",
       "      <td>0.766054</td>\n",
       "      <td>0.766862</td>\n",
       "      <td>0.765125</td>\n",
       "      <td>0.674953</td>\n",
       "      <td>0.688221</td>\n",
       "      <td>0.688124</td>\n",
       "      <td>0.687887</td>\n",
       "      <td>0.687887</td>\n",
       "      <td>0.685008</td>\n",
       "      <td>0.651112</td>\n",
       "      <td>0.540821</td>\n",
       "      <td>0.540821</td>\n",
       "      <td>0.670794</td>\n",
       "      <td>0.670794</td>\n",
       "      <td>0.664282</td>\n",
       "      <td>0.684936</td>\n",
       "      <td>0.60629</td>\n",
       "      <td>0.614475</td>\n",
       "      <td>0.557753</td>\n",
       "      <td>0.358814</td>\n",
       "      <td>0.358749</td>\n",
       "      <td>0.286452</td>\n",
       "      <td>0.286178</td>\n",
       "      <td>0.286017</td>\n",
       "      <td>0.286017</td>\n",
       "      <td>0.28462</td>\n",
       "      <td>0.0123317</td>\n",
       "      <td>-0.038756</td>\n",
       "      <td>-0.0388902</td>\n",
       "      <td>-0.0388902</td>\n",
       "      <td>-0.0389045</td>\n",
       "      <td>-0.0389132</td>\n",
       "      <td>-0.0730773</td>\n",
       "      <td>-0.0730773</td>\n",
       "      <td>-0.0799358</td>\n",
       "      <td>-0.0799505</td>\n",
       "      <td>-0.0799505</td>\n",
       "      <td>-0.0799506</td>\n",
       "      <td>-0.0825685</td>\n",
       "      <td>-0.0825685</td>\n",
       "      <td>-0.0832376</td>\n",
       "      <td>-0.0832376</td>\n",
       "      <td>-0.0832391</td>\n",
       "      <td>0.437063</td>\n",
       "      <td>0.436967</td>\n",
       "      <td>-0.0835398</td>\n",
       "      <td>-0.0836068</td>\n",
       "      <td>-0.0836371</td>\n",
       "      <td>-0.0836371</td>\n",
       "      <td>-0.0836468</td>\n",
       "      <td>-0.0836468</td>\n",
       "      <td>-0.0836477</td>\n",
       "      <td>-0.0836478</td>\n",
       "      <td>-0.0836478</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.0836479</td>\n",
       "      <td>-0.412972</td>\n",
       "      <td>-0.412972</td>\n",
       "      <td>-2.72908</td>\n",
       "      <td>-4.68435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.598728</td>\n",
       "      <td>0.63196</td>\n",
       "      <td>0.63196</td>\n",
       "      <td>0.565813</td>\n",
       "      <td>0.549902</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.540194</td>\n",
       "      <td>0.537911</td>\n",
       "      <td>0.659863</td>\n",
       "      <td>0.488625</td>\n",
       "      <td>0.488596</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>0.487511</td>\n",
       "      <td>0.480609</td>\n",
       "      <td>0.650916</td>\n",
       "      <td>0.61718</td>\n",
       "      <td>0.61718</td>\n",
       "      <td>0.390332</td>\n",
       "      <td>0.390332</td>\n",
       "      <td>0.38628</td>\n",
       "      <td>0.589428</td>\n",
       "      <td>0.364684</td>\n",
       "      <td>0.361879</td>\n",
       "      <td>0.465256</td>\n",
       "      <td>0.446391</td>\n",
       "      <td>0.446391</td>\n",
       "      <td>0.187067</td>\n",
       "      <td>0.186997</td>\n",
       "      <td>0.187002</td>\n",
       "      <td>0.187002</td>\n",
       "      <td>0.185961</td>\n",
       "      <td>0.0709321</td>\n",
       "      <td>0.00471727</td>\n",
       "      <td>0.00464025</td>\n",
       "      <td>0.00464025</td>\n",
       "      <td>0.00464004</td>\n",
       "      <td>0.00462365</td>\n",
       "      <td>-0.0120532</td>\n",
       "      <td>-0.0120532</td>\n",
       "      <td>-0.0187114</td>\n",
       "      <td>-0.0187188</td>\n",
       "      <td>-0.0187188</td>\n",
       "      <td>-0.018719</td>\n",
       "      <td>-0.0207454</td>\n",
       "      <td>-0.0207454</td>\n",
       "      <td>-0.0214321</td>\n",
       "      <td>-0.0214321</td>\n",
       "      <td>-0.0214328</td>\n",
       "      <td>-0.420811</td>\n",
       "      <td>-0.420997</td>\n",
       "      <td>-0.0216359</td>\n",
       "      <td>-0.0217046</td>\n",
       "      <td>-0.021725</td>\n",
       "      <td>-0.021725</td>\n",
       "      <td>-0.0217339</td>\n",
       "      <td>-0.0217339</td>\n",
       "      <td>-0.0217348</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-0.0217349</td>\n",
       "      <td>-4.3544</td>\n",
       "      <td>-4.3544</td>\n",
       "      <td>-3.62766</td>\n",
       "      <td>-11.7236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.695054</td>\n",
       "      <td>0.676896</td>\n",
       "      <td>0.676896</td>\n",
       "      <td>0.675544</td>\n",
       "      <td>0.669484</td>\n",
       "      <td>0.666812</td>\n",
       "      <td>0.666812</td>\n",
       "      <td>0.658714</td>\n",
       "      <td>0.657227</td>\n",
       "      <td>0.654664</td>\n",
       "      <td>0.602469</td>\n",
       "      <td>0.602416</td>\n",
       "      <td>0.602088</td>\n",
       "      <td>0.602088</td>\n",
       "      <td>0.599423</td>\n",
       "      <td>0.564233</td>\n",
       "      <td>0.546223</td>\n",
       "      <td>0.546223</td>\n",
       "      <td>0.527676</td>\n",
       "      <td>0.527676</td>\n",
       "      <td>0.526075</td>\n",
       "      <td>0.5093</td>\n",
       "      <td>0.494161</td>\n",
       "      <td>0.491516</td>\n",
       "      <td>0.408673</td>\n",
       "      <td>0.382129</td>\n",
       "      <td>0.38212</td>\n",
       "      <td>0.261744</td>\n",
       "      <td>0.261708</td>\n",
       "      <td>0.261621</td>\n",
       "      <td>0.261621</td>\n",
       "      <td>0.260419</td>\n",
       "      <td>0.0388694</td>\n",
       "      <td>-0.0095101</td>\n",
       "      <td>-0.00951217</td>\n",
       "      <td>-0.00951217</td>\n",
       "      <td>-0.0095127</td>\n",
       "      <td>-0.00953401</td>\n",
       "      <td>-0.0396042</td>\n",
       "      <td>-0.0396042</td>\n",
       "      <td>-0.0448045</td>\n",
       "      <td>-0.0448085</td>\n",
       "      <td>-0.0448085</td>\n",
       "      <td>-0.0448088</td>\n",
       "      <td>-0.0479242</td>\n",
       "      <td>-0.0479242</td>\n",
       "      <td>-0.0484726</td>\n",
       "      <td>-0.0484726</td>\n",
       "      <td>-0.0484731</td>\n",
       "      <td>-0.0484995</td>\n",
       "      <td>-0.0485968</td>\n",
       "      <td>-0.0487771</td>\n",
       "      <td>-0.048832</td>\n",
       "      <td>-0.0488625</td>\n",
       "      <td>-0.0488625</td>\n",
       "      <td>-0.048871</td>\n",
       "      <td>-0.048871</td>\n",
       "      <td>-0.0488719</td>\n",
       "      <td>-0.0488719</td>\n",
       "      <td>-0.0488719</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-0.048872</td>\n",
       "      <td>-2.82236</td>\n",
       "      <td>-2.82236</td>\n",
       "      <td>-4.21945</td>\n",
       "      <td>-11.4617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.0573936</td>\n",
       "      <td>0.0584059</td>\n",
       "      <td>0.0584059</td>\n",
       "      <td>0.0694134</td>\n",
       "      <td>0.0738222</td>\n",
       "      <td>0.0705028</td>\n",
       "      <td>0.0705028</td>\n",
       "      <td>0.0754083</td>\n",
       "      <td>0.0757683</td>\n",
       "      <td>0.0969927</td>\n",
       "      <td>0.0645575</td>\n",
       "      <td>0.0645347</td>\n",
       "      <td>0.0648895</td>\n",
       "      <td>0.0648895</td>\n",
       "      <td>0.067195</td>\n",
       "      <td>0.222876</td>\n",
       "      <td>0.135788</td>\n",
       "      <td>0.135788</td>\n",
       "      <td>0.104554</td>\n",
       "      <td>0.104554</td>\n",
       "      <td>0.129582</td>\n",
       "      <td>0.203944</td>\n",
       "      <td>0.0881641</td>\n",
       "      <td>0.088584</td>\n",
       "      <td>0.261869</td>\n",
       "      <td>0.0811758</td>\n",
       "      <td>0.0811795</td>\n",
       "      <td>0.053378</td>\n",
       "      <td>0.0534023</td>\n",
       "      <td>0.0533495</td>\n",
       "      <td>0.0533495</td>\n",
       "      <td>0.0533865</td>\n",
       "      <td>0.0237299</td>\n",
       "      <td>0.0250576</td>\n",
       "      <td>0.0251428</td>\n",
       "      <td>0.0251428</td>\n",
       "      <td>0.0251525</td>\n",
       "      <td>0.0251446</td>\n",
       "      <td>0.0251458</td>\n",
       "      <td>0.0251458</td>\n",
       "      <td>0.0260116</td>\n",
       "      <td>0.0260069</td>\n",
       "      <td>0.0260069</td>\n",
       "      <td>0.0260066</td>\n",
       "      <td>0.0258648</td>\n",
       "      <td>0.0258648</td>\n",
       "      <td>0.025927</td>\n",
       "      <td>0.025927</td>\n",
       "      <td>0.0259265</td>\n",
       "      <td>0.420057</td>\n",
       "      <td>0.420158</td>\n",
       "      <td>0.0259167</td>\n",
       "      <td>0.0259229</td>\n",
       "      <td>0.0259219</td>\n",
       "      <td>0.0259219</td>\n",
       "      <td>0.0259224</td>\n",
       "      <td>0.0259224</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>0.0259225</td>\n",
       "      <td>3.08274</td>\n",
       "      <td>3.08274</td>\n",
       "      <td>1.63505</td>\n",
       "      <td>8.75307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.715243</td>\n",
       "      <td>0.775019</td>\n",
       "      <td>0.775019</td>\n",
       "      <td>0.66813</td>\n",
       "      <td>0.692048</td>\n",
       "      <td>0.663595</td>\n",
       "      <td>0.663595</td>\n",
       "      <td>0.64982</td>\n",
       "      <td>0.648372</td>\n",
       "      <td>0.769791</td>\n",
       "      <td>0.592227</td>\n",
       "      <td>0.592191</td>\n",
       "      <td>0.591718</td>\n",
       "      <td>0.591718</td>\n",
       "      <td>0.587692</td>\n",
       "      <td>0.910485</td>\n",
       "      <td>0.637966</td>\n",
       "      <td>0.637966</td>\n",
       "      <td>0.971445</td>\n",
       "      <td>0.971445</td>\n",
       "      <td>0.739797</td>\n",
       "      <td>0.869933</td>\n",
       "      <td>0.988381</td>\n",
       "      <td>0.99053</td>\n",
       "      <td>0.95171</td>\n",
       "      <td>0.399571</td>\n",
       "      <td>0.399578</td>\n",
       "      <td>0.247978</td>\n",
       "      <td>0.24801</td>\n",
       "      <td>0.24799</td>\n",
       "      <td>0.24799</td>\n",
       "      <td>0.246852</td>\n",
       "      <td>0.0781266</td>\n",
       "      <td>-0.00869535</td>\n",
       "      <td>-0.00869177</td>\n",
       "      <td>-0.00869177</td>\n",
       "      <td>-0.00875644</td>\n",
       "      <td>-0.00871477</td>\n",
       "      <td>-0.0271169</td>\n",
       "      <td>-0.0271169</td>\n",
       "      <td>-0.0418348</td>\n",
       "      <td>-0.0418113</td>\n",
       "      <td>-0.0418113</td>\n",
       "      <td>-0.041811</td>\n",
       "      <td>-0.0435839</td>\n",
       "      <td>-0.0435839</td>\n",
       "      <td>-0.0458417</td>\n",
       "      <td>-0.0458417</td>\n",
       "      <td>-0.0458394</td>\n",
       "      <td>0.936629</td>\n",
       "      <td>0.93663</td>\n",
       "      <td>-0.0459673</td>\n",
       "      <td>-0.0462016</td>\n",
       "      <td>-0.046214</td>\n",
       "      <td>-0.046214</td>\n",
       "      <td>-0.0462388</td>\n",
       "      <td>-0.0462388</td>\n",
       "      <td>-0.0462413</td>\n",
       "      <td>-0.0462415</td>\n",
       "      <td>-0.0462415</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>-0.0462416</td>\n",
       "      <td>0.971642</td>\n",
       "      <td>0.971642</td>\n",
       "      <td>0.988171</td>\n",
       "      <td>0.990006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.763903</td>\n",
       "      <td>0.866066</td>\n",
       "      <td>0.866066</td>\n",
       "      <td>0.687997</td>\n",
       "      <td>0.697302</td>\n",
       "      <td>0.679726</td>\n",
       "      <td>0.679726</td>\n",
       "      <td>0.667884</td>\n",
       "      <td>0.666217</td>\n",
       "      <td>0.867117</td>\n",
       "      <td>0.585052</td>\n",
       "      <td>0.584999</td>\n",
       "      <td>0.585177</td>\n",
       "      <td>0.585177</td>\n",
       "      <td>0.58307</td>\n",
       "      <td>0.938649</td>\n",
       "      <td>0.728881</td>\n",
       "      <td>0.728881</td>\n",
       "      <td>0.980372</td>\n",
       "      <td>0.980372</td>\n",
       "      <td>0.67069</td>\n",
       "      <td>0.905785</td>\n",
       "      <td>0.991501</td>\n",
       "      <td>0.992085</td>\n",
       "      <td>0.96656</td>\n",
       "      <td>0.458646</td>\n",
       "      <td>0.458646</td>\n",
       "      <td>0.250188</td>\n",
       "      <td>0.250033</td>\n",
       "      <td>0.249858</td>\n",
       "      <td>0.249858</td>\n",
       "      <td>0.248389</td>\n",
       "      <td>0.0818926</td>\n",
       "      <td>-0.00320676</td>\n",
       "      <td>-0.00304668</td>\n",
       "      <td>-0.00304668</td>\n",
       "      <td>-0.00304825</td>\n",
       "      <td>-0.00306835</td>\n",
       "      <td>-0.0219565</td>\n",
       "      <td>-0.0219565</td>\n",
       "      <td>-0.034148</td>\n",
       "      <td>-0.0341527</td>\n",
       "      <td>-0.0341527</td>\n",
       "      <td>-0.0341536</td>\n",
       "      <td>-0.0360456</td>\n",
       "      <td>-0.0360456</td>\n",
       "      <td>-0.0379867</td>\n",
       "      <td>-0.0379867</td>\n",
       "      <td>-0.0379872</td>\n",
       "      <td>0.960848</td>\n",
       "      <td>0.960849</td>\n",
       "      <td>-0.0381701</td>\n",
       "      <td>-0.0383714</td>\n",
       "      <td>-0.0383896</td>\n",
       "      <td>-0.0383896</td>\n",
       "      <td>-0.0384117</td>\n",
       "      <td>-0.0384117</td>\n",
       "      <td>-0.0384139</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>-0.0384141</td>\n",
       "      <td>0.981128</td>\n",
       "      <td>0.981128</td>\n",
       "      <td>0.990888</td>\n",
       "      <td>0.991679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.759276</td>\n",
       "      <td>0.842586</td>\n",
       "      <td>0.842586</td>\n",
       "      <td>0.691274</td>\n",
       "      <td>0.683931</td>\n",
       "      <td>0.682745</td>\n",
       "      <td>0.682745</td>\n",
       "      <td>0.673328</td>\n",
       "      <td>0.671357</td>\n",
       "      <td>0.848154</td>\n",
       "      <td>0.605033</td>\n",
       "      <td>0.605019</td>\n",
       "      <td>0.604431</td>\n",
       "      <td>0.604431</td>\n",
       "      <td>0.599671</td>\n",
       "      <td>0.949865</td>\n",
       "      <td>0.735222</td>\n",
       "      <td>0.735222</td>\n",
       "      <td>0.986909</td>\n",
       "      <td>0.986909</td>\n",
       "      <td>0.68368</td>\n",
       "      <td>0.914233</td>\n",
       "      <td>0.992268</td>\n",
       "      <td>0.992268</td>\n",
       "      <td>0.969074</td>\n",
       "      <td>0.465686</td>\n",
       "      <td>0.465692</td>\n",
       "      <td>0.23653</td>\n",
       "      <td>0.236751</td>\n",
       "      <td>0.236633</td>\n",
       "      <td>0.236633</td>\n",
       "      <td>0.235399</td>\n",
       "      <td>0.0942315</td>\n",
       "      <td>-0.00945768</td>\n",
       "      <td>-0.00938972</td>\n",
       "      <td>-0.00938972</td>\n",
       "      <td>-0.00936795</td>\n",
       "      <td>-0.00941117</td>\n",
       "      <td>-0.0294688</td>\n",
       "      <td>-0.0294688</td>\n",
       "      <td>-0.0422564</td>\n",
       "      <td>-0.0422712</td>\n",
       "      <td>-0.0422712</td>\n",
       "      <td>-0.0422719</td>\n",
       "      <td>-0.0438874</td>\n",
       "      <td>-0.0438874</td>\n",
       "      <td>-0.0452058</td>\n",
       "      <td>-0.0452058</td>\n",
       "      <td>-0.0452074</td>\n",
       "      <td>0.959221</td>\n",
       "      <td>0.959221</td>\n",
       "      <td>-0.0453396</td>\n",
       "      <td>-0.0454716</td>\n",
       "      <td>-0.045485</td>\n",
       "      <td>-0.045485</td>\n",
       "      <td>-0.0454995</td>\n",
       "      <td>-0.0454995</td>\n",
       "      <td>-0.045501</td>\n",
       "      <td>-0.0455011</td>\n",
       "      <td>-0.0455011</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>-0.0455012</td>\n",
       "      <td>0.983774</td>\n",
       "      <td>0.983774</td>\n",
       "      <td>0.991283</td>\n",
       "      <td>0.99167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.758094</td>\n",
       "      <td>0.831826</td>\n",
       "      <td>0.831826</td>\n",
       "      <td>0.66953</td>\n",
       "      <td>0.674691</td>\n",
       "      <td>0.655013</td>\n",
       "      <td>0.655013</td>\n",
       "      <td>0.636395</td>\n",
       "      <td>0.632124</td>\n",
       "      <td>0.832729</td>\n",
       "      <td>0.551188</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>0.550408</td>\n",
       "      <td>0.550408</td>\n",
       "      <td>0.544759</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>0.713349</td>\n",
       "      <td>0.713349</td>\n",
       "      <td>0.975678</td>\n",
       "      <td>0.975678</td>\n",
       "      <td>0.667803</td>\n",
       "      <td>0.895889</td>\n",
       "      <td>0.990185</td>\n",
       "      <td>0.99182</td>\n",
       "      <td>0.956054</td>\n",
       "      <td>0.461254</td>\n",
       "      <td>0.461202</td>\n",
       "      <td>0.237407</td>\n",
       "      <td>0.237231</td>\n",
       "      <td>0.237089</td>\n",
       "      <td>0.237089</td>\n",
       "      <td>0.235801</td>\n",
       "      <td>0.0706144</td>\n",
       "      <td>-0.00372862</td>\n",
       "      <td>-0.00380458</td>\n",
       "      <td>-0.00380458</td>\n",
       "      <td>-0.00381185</td>\n",
       "      <td>-0.00382536</td>\n",
       "      <td>-0.0224293</td>\n",
       "      <td>-0.0224293</td>\n",
       "      <td>-0.0351291</td>\n",
       "      <td>-0.0351372</td>\n",
       "      <td>-0.0351372</td>\n",
       "      <td>-0.0351374</td>\n",
       "      <td>-0.0359387</td>\n",
       "      <td>-0.0359387</td>\n",
       "      <td>-0.0378425</td>\n",
       "      <td>-0.0378425</td>\n",
       "      <td>-0.0378434</td>\n",
       "      <td>0.94876</td>\n",
       "      <td>0.948765</td>\n",
       "      <td>-0.0379382</td>\n",
       "      <td>-0.0381357</td>\n",
       "      <td>-0.0381452</td>\n",
       "      <td>-0.0381452</td>\n",
       "      <td>-0.038166</td>\n",
       "      <td>-0.038166</td>\n",
       "      <td>-0.0381681</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>-0.0381683</td>\n",
       "      <td>0.97268</td>\n",
       "      <td>0.97268</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0.991217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.798158</td>\n",
       "      <td>0.857246</td>\n",
       "      <td>0.857246</td>\n",
       "      <td>0.745593</td>\n",
       "      <td>0.746399</td>\n",
       "      <td>0.733096</td>\n",
       "      <td>0.733096</td>\n",
       "      <td>0.715606</td>\n",
       "      <td>0.712869</td>\n",
       "      <td>0.868024</td>\n",
       "      <td>0.651844</td>\n",
       "      <td>0.651827</td>\n",
       "      <td>0.65145</td>\n",
       "      <td>0.65145</td>\n",
       "      <td>0.649127</td>\n",
       "      <td>0.921044</td>\n",
       "      <td>0.711785</td>\n",
       "      <td>0.711785</td>\n",
       "      <td>0.981483</td>\n",
       "      <td>0.981483</td>\n",
       "      <td>0.763858</td>\n",
       "      <td>0.881388</td>\n",
       "      <td>0.989694</td>\n",
       "      <td>0.989842</td>\n",
       "      <td>0.962414</td>\n",
       "      <td>0.433699</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.271125</td>\n",
       "      <td>0.270998</td>\n",
       "      <td>0.271054</td>\n",
       "      <td>0.271054</td>\n",
       "      <td>0.269974</td>\n",
       "      <td>0.0537512</td>\n",
       "      <td>-0.00740829</td>\n",
       "      <td>-0.00754216</td>\n",
       "      <td>-0.00754216</td>\n",
       "      <td>-0.00754399</td>\n",
       "      <td>-0.00756198</td>\n",
       "      <td>-0.0333662</td>\n",
       "      <td>-0.0333662</td>\n",
       "      <td>-0.0434699</td>\n",
       "      <td>-0.0434851</td>\n",
       "      <td>-0.0434851</td>\n",
       "      <td>-0.0434854</td>\n",
       "      <td>-0.0459806</td>\n",
       "      <td>-0.0459806</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.0478355</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>-0.0480769</td>\n",
       "      <td>-0.0482713</td>\n",
       "      <td>-0.0482955</td>\n",
       "      <td>-0.0482955</td>\n",
       "      <td>-0.0483174</td>\n",
       "      <td>-0.0483174</td>\n",
       "      <td>-0.0483196</td>\n",
       "      <td>-0.0483198</td>\n",
       "      <td>-0.0483198</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>-0.0483199</td>\n",
       "      <td>0.980873</td>\n",
       "      <td>0.980873</td>\n",
       "      <td>0.988829</td>\n",
       "      <td>0.989049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.758935</td>\n",
       "      <td>0.834549</td>\n",
       "      <td>0.834549</td>\n",
       "      <td>0.692505</td>\n",
       "      <td>0.698874</td>\n",
       "      <td>0.682835</td>\n",
       "      <td>0.682835</td>\n",
       "      <td>0.668607</td>\n",
       "      <td>0.666188</td>\n",
       "      <td>0.837163</td>\n",
       "      <td>0.597069</td>\n",
       "      <td>0.597026</td>\n",
       "      <td>0.596637</td>\n",
       "      <td>0.596637</td>\n",
       "      <td>0.592864</td>\n",
       "      <td>0.930857</td>\n",
       "      <td>0.705441</td>\n",
       "      <td>0.705441</td>\n",
       "      <td>0.979178</td>\n",
       "      <td>0.979178</td>\n",
       "      <td>0.705166</td>\n",
       "      <td>0.893446</td>\n",
       "      <td>0.990406</td>\n",
       "      <td>0.991309</td>\n",
       "      <td>0.961162</td>\n",
       "      <td>0.443771</td>\n",
       "      <td>0.443763</td>\n",
       "      <td>0.248646</td>\n",
       "      <td>0.248604</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>0.247283</td>\n",
       "      <td>0.0757232</td>\n",
       "      <td>-0.00649934</td>\n",
       "      <td>-0.00649498</td>\n",
       "      <td>-0.00649498</td>\n",
       "      <td>-0.0065057</td>\n",
       "      <td>-0.00651633</td>\n",
       "      <td>-0.0268675</td>\n",
       "      <td>-0.0268675</td>\n",
       "      <td>-0.0393676</td>\n",
       "      <td>-0.0393715</td>\n",
       "      <td>-0.0393715</td>\n",
       "      <td>-0.0393719</td>\n",
       "      <td>-0.0410872</td>\n",
       "      <td>-0.0410872</td>\n",
       "      <td>-0.0429422</td>\n",
       "      <td>-0.0429422</td>\n",
       "      <td>-0.0429426</td>\n",
       "      <td>0.951215</td>\n",
       "      <td>0.951217</td>\n",
       "      <td>-0.0430984</td>\n",
       "      <td>-0.0432903</td>\n",
       "      <td>-0.0433059</td>\n",
       "      <td>-0.0433059</td>\n",
       "      <td>-0.0433267</td>\n",
       "      <td>-0.0433267</td>\n",
       "      <td>-0.0433288</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>-0.043329</td>\n",
       "      <td>0.978019</td>\n",
       "      <td>0.978019</td>\n",
       "      <td>0.989894</td>\n",
       "      <td>0.990724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.0263551</td>\n",
       "      <td>0.0320136</td>\n",
       "      <td>0.0320136</td>\n",
       "      <td>0.0281501</td>\n",
       "      <td>0.0249632</td>\n",
       "      <td>0.0271298</td>\n",
       "      <td>0.0271298</td>\n",
       "      <td>0.0269156</td>\n",
       "      <td>0.0271426</td>\n",
       "      <td>0.036136</td>\n",
       "      <td>0.0326699</td>\n",
       "      <td>0.0326947</td>\n",
       "      <td>0.032735</td>\n",
       "      <td>0.032735</td>\n",
       "      <td>0.0336115</td>\n",
       "      <td>0.0137465</td>\n",
       "      <td>0.0349054</td>\n",
       "      <td>0.0349054</td>\n",
       "      <td>0.00526436</td>\n",
       "      <td>0.00526436</td>\n",
       "      <td>0.0392182</td>\n",
       "      <td>0.0160537</td>\n",
       "      <td>0.00136558</td>\n",
       "      <td>0.00095294</td>\n",
       "      <td>0.00646102</td>\n",
       "      <td>0.0247479</td>\n",
       "      <td>0.0247392</td>\n",
       "      <td>0.0124999</td>\n",
       "      <td>0.0124396</td>\n",
       "      <td>0.0125045</td>\n",
       "      <td>0.0125045</td>\n",
       "      <td>0.0125649</td>\n",
       "      <td>0.013381</td>\n",
       "      <td>0.00256587</td>\n",
       "      <td>0.00258577</td>\n",
       "      <td>0.00258577</td>\n",
       "      <td>0.00259026</td>\n",
       "      <td>0.00258598</td>\n",
       "      <td>0.00430992</td>\n",
       "      <td>0.00430992</td>\n",
       "      <td>0.00391076</td>\n",
       "      <td>0.00391021</td>\n",
       "      <td>0.00391021</td>\n",
       "      <td>0.00391003</td>\n",
       "      <td>0.00424133</td>\n",
       "      <td>0.00424133</td>\n",
       "      <td>0.00419582</td>\n",
       "      <td>0.00419582</td>\n",
       "      <td>0.0041957</td>\n",
       "      <td>0.00867213</td>\n",
       "      <td>0.00867172</td>\n",
       "      <td>0.00421793</td>\n",
       "      <td>0.00421448</td>\n",
       "      <td>0.00421672</td>\n",
       "      <td>0.00421672</td>\n",
       "      <td>0.00421661</td>\n",
       "      <td>0.00421661</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.0042166</td>\n",
       "      <td>0.00490101</td>\n",
       "      <td>0.00490101</td>\n",
       "      <td>0.00119878</td>\n",
       "      <td>0.00103631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             58  \\\n",
       "mean_fit_time                                         0.0244103   \n",
       "std_fit_time                                         0.00199587   \n",
       "mean_score_time                                      0.00217381   \n",
       "std_score_time                                      0.000541315   \n",
       "param_C                                                     100   \n",
       "param_gamma                                               0.001   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                      0.764077   \n",
       "split1_test_score                                      0.692847   \n",
       "split2_test_score                                      0.678289   \n",
       "split3_test_score                                      0.741328   \n",
       "split4_test_score                                      0.598728   \n",
       "mean_test_score                                        0.695054   \n",
       "std_test_score                                        0.0573936   \n",
       "rank_test_score                                               1   \n",
       "split0_train_score                                     0.715243   \n",
       "split1_train_score                                     0.763903   \n",
       "split2_train_score                                     0.759276   \n",
       "split3_train_score                                     0.758094   \n",
       "split4_train_score                                     0.798158   \n",
       "mean_train_score                                       0.758935   \n",
       "std_train_score                                       0.0263551   \n",
       "\n",
       "                                                           44  \\\n",
       "mean_fit_time                                       0.0458178   \n",
       "std_fit_time                                       0.00530144   \n",
       "mean_score_time                                    0.00484419   \n",
       "std_score_time                                     0.00347988   \n",
       "param_C                                                    10   \n",
       "param_gamma                                              0.01   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}   \n",
       "split0_test_score                                    0.725563   \n",
       "split1_test_score                                    0.594142   \n",
       "split2_test_score                                    0.679679   \n",
       "split3_test_score                                    0.753137   \n",
       "split4_test_score                                     0.63196   \n",
       "mean_test_score                                      0.676896   \n",
       "std_test_score                                      0.0584059   \n",
       "rank_test_score                                             2   \n",
       "split0_train_score                                   0.775019   \n",
       "split1_train_score                                   0.866066   \n",
       "split2_train_score                                   0.842586   \n",
       "split3_train_score                                   0.831826   \n",
       "split4_train_score                                   0.857246   \n",
       "mean_train_score                                     0.834549   \n",
       "std_train_score                                     0.0320136   \n",
       "\n",
       "                                                           28  \\\n",
       "mean_fit_time                                       0.0483396   \n",
       "std_fit_time                                       0.00568788   \n",
       "mean_score_time                                    0.00550413   \n",
       "std_score_time                                     0.00330465   \n",
       "param_C                                                    10   \n",
       "param_gamma                                              0.01   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}   \n",
       "split0_test_score                                    0.725563   \n",
       "split1_test_score                                    0.594142   \n",
       "split2_test_score                                    0.679679   \n",
       "split3_test_score                                    0.753137   \n",
       "split4_test_score                                     0.63196   \n",
       "mean_test_score                                      0.676896   \n",
       "std_test_score                                      0.0584059   \n",
       "rank_test_score                                             2   \n",
       "split0_train_score                                   0.775019   \n",
       "split1_train_score                                   0.866066   \n",
       "split2_train_score                                   0.842586   \n",
       "split3_train_score                                   0.831826   \n",
       "split4_train_score                                   0.857246   \n",
       "mean_train_score                                     0.834549   \n",
       "std_train_score                                     0.0320136   \n",
       "\n",
       "                                                               72  \\\n",
       "mean_fit_time                                            0.030206   \n",
       "std_fit_time                                           0.00399539   \n",
       "mean_score_time                                        0.00245957   \n",
       "std_score_time                                        0.000583985   \n",
       "param_C                                                      1000   \n",
       "param_gamma                                                0.0001   \n",
       "param_kernel                                                  rbf   \n",
       "params              {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                        0.731033   \n",
       "split1_test_score                                         0.66988   \n",
       "split2_test_score                                        0.645728   \n",
       "split3_test_score                                        0.765266   \n",
       "split4_test_score                                        0.565813   \n",
       "mean_test_score                                          0.675544   \n",
       "std_test_score                                          0.0694134   \n",
       "rank_test_score                                                 4   \n",
       "split0_train_score                                        0.66813   \n",
       "split1_train_score                                       0.687997   \n",
       "split2_train_score                                       0.691274   \n",
       "split3_train_score                                        0.66953   \n",
       "split4_train_score                                       0.745593   \n",
       "mean_train_score                                         0.692505   \n",
       "std_train_score                                         0.0281501   \n",
       "\n",
       "                                                          12  \\\n",
       "mean_fit_time                                      0.0133247   \n",
       "std_fit_time                                      0.00198953   \n",
       "mean_score_time                                   0.00202169   \n",
       "std_score_time                                   0.000239003   \n",
       "param_C                                                    1   \n",
       "param_gamma                                             0.01   \n",
       "param_kernel                                             rbf   \n",
       "params              {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   0.654005   \n",
       "split1_test_score                                   0.700224   \n",
       "split2_test_score                                   0.665418   \n",
       "split3_test_score                                    0.77787   \n",
       "split4_test_score                                   0.549902   \n",
       "mean_test_score                                     0.669484   \n",
       "std_test_score                                     0.0738222   \n",
       "rank_test_score                                            5   \n",
       "split0_train_score                                  0.692048   \n",
       "split1_train_score                                  0.697302   \n",
       "split2_train_score                                  0.683931   \n",
       "split3_train_score                                  0.674691   \n",
       "split4_train_score                                  0.746399   \n",
       "mean_train_score                                    0.698874   \n",
       "std_train_score                                    0.0249632   \n",
       "\n",
       "                                                            26  \\\n",
       "mean_fit_time                                        0.0292049   \n",
       "std_fit_time                                         0.0109811   \n",
       "mean_score_time                                     0.00564561   \n",
       "std_score_time                                      0.00321827   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              0.001   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.702677   \n",
       "split1_test_score                                     0.670786   \n",
       "split2_test_score                                     0.642162   \n",
       "split3_test_score                                     0.766054   \n",
       "split4_test_score                                     0.552381   \n",
       "mean_test_score                                       0.666812   \n",
       "std_test_score                                       0.0705028   \n",
       "rank_test_score                                              6   \n",
       "split0_train_score                                    0.663595   \n",
       "split1_train_score                                    0.679726   \n",
       "split2_train_score                                    0.682745   \n",
       "split3_train_score                                    0.655013   \n",
       "split4_train_score                                    0.733096   \n",
       "mean_train_score                                      0.682835   \n",
       "std_train_score                                      0.0271298   \n",
       "\n",
       "                                                            42  \\\n",
       "mean_fit_time                                        0.0142241   \n",
       "std_fit_time                                       0.000946084   \n",
       "mean_score_time                                     0.00203724   \n",
       "std_score_time                                     0.000273191   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              0.001   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.702677   \n",
       "split1_test_score                                     0.670786   \n",
       "split2_test_score                                     0.642162   \n",
       "split3_test_score                                     0.766054   \n",
       "split4_test_score                                     0.552381   \n",
       "mean_test_score                                       0.666812   \n",
       "std_test_score                                       0.0705028   \n",
       "rank_test_score                                              6   \n",
       "split0_train_score                                    0.663595   \n",
       "split1_train_score                                    0.679726   \n",
       "split2_train_score                                    0.682745   \n",
       "split3_train_score                                    0.655013   \n",
       "split4_train_score                                    0.733096   \n",
       "mean_train_score                                      0.682835   \n",
       "std_train_score                                      0.0271298   \n",
       "\n",
       "                                                              56  \\\n",
       "mean_fit_time                                            0.01619   \n",
       "std_fit_time                                         0.000694317   \n",
       "mean_score_time                                       0.00206442   \n",
       "std_score_time                                       0.000237852   \n",
       "param_C                                                      100   \n",
       "param_gamma                                               0.0001   \n",
       "param_kernel                                                 rbf   \n",
       "params              {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                       0.699571   \n",
       "split1_test_score                                       0.660131   \n",
       "split2_test_score                                       0.626811   \n",
       "split3_test_score                                       0.766862   \n",
       "split4_test_score                                       0.540194   \n",
       "mean_test_score                                         0.658714   \n",
       "std_test_score                                         0.0754083   \n",
       "rank_test_score                                                8   \n",
       "split0_train_score                                       0.64982   \n",
       "split1_train_score                                      0.667884   \n",
       "split2_train_score                                      0.673328   \n",
       "split3_train_score                                      0.636395   \n",
       "split4_train_score                                      0.715606   \n",
       "mean_train_score                                        0.668607   \n",
       "std_train_score                                        0.0269156   \n",
       "\n",
       "                                                              70  \\\n",
       "mean_fit_time                                          0.0241995   \n",
       "std_fit_time                                          0.00643547   \n",
       "mean_score_time                                       0.00237894   \n",
       "std_score_time                                       0.000778393   \n",
       "param_C                                                     1000   \n",
       "param_gamma                                                1e-05   \n",
       "param_kernel                                                 rbf   \n",
       "params              {'C': 1000, 'gamma': 1e-05, 'kernel': 'rbf'}   \n",
       "split0_test_score                                       0.699717   \n",
       "split1_test_score                                       0.658089   \n",
       "split2_test_score                                       0.625292   \n",
       "split3_test_score                                       0.765125   \n",
       "split4_test_score                                       0.537911   \n",
       "mean_test_score                                         0.657227   \n",
       "std_test_score                                         0.0757683   \n",
       "rank_test_score                                                9   \n",
       "split0_train_score                                      0.648372   \n",
       "split1_train_score                                      0.666217   \n",
       "split2_train_score                                      0.671357   \n",
       "split3_train_score                                      0.632124   \n",
       "split4_train_score                                      0.712869   \n",
       "mean_train_score                                        0.666188   \n",
       "std_train_score                                        0.0271426   \n",
       "\n",
       "                                                              74  \\\n",
       "mean_fit_time                                           0.221231   \n",
       "std_fit_time                                           0.0262668   \n",
       "mean_score_time                                       0.00300126   \n",
       "std_score_time                                       0.000854873   \n",
       "param_C                                                     1000   \n",
       "param_gamma                                                0.001   \n",
       "param_kernel                                                 rbf   \n",
       "params              {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                       0.820067   \n",
       "split1_test_score                                       0.536616   \n",
       "split2_test_score                                       0.581822   \n",
       "split3_test_score                                       0.674953   \n",
       "split4_test_score                                       0.659863   \n",
       "mean_test_score                                         0.654664   \n",
       "std_test_score                                         0.0969927   \n",
       "rank_test_score                                               10   \n",
       "split0_train_score                                      0.769791   \n",
       "split1_train_score                                      0.867117   \n",
       "split2_train_score                                      0.848154   \n",
       "split3_train_score                                      0.832729   \n",
       "split4_train_score                                      0.868024   \n",
       "mean_train_score                                        0.837163   \n",
       "std_train_score                                         0.036136   \n",
       "\n",
       "                                                              68  \\\n",
       "mean_fit_time                                          0.0207355   \n",
       "std_fit_time                                          0.00696305   \n",
       "mean_score_time                                       0.00382667   \n",
       "std_score_time                                        0.00184618   \n",
       "param_C                                                     1000   \n",
       "param_gamma                                                1e-06   \n",
       "param_kernel                                                 rbf   \n",
       "params              {'C': 1000, 'gamma': 1e-06, 'kernel': 'rbf'}   \n",
       "split0_test_score                                       0.601747   \n",
       "split1_test_score                                       0.624256   \n",
       "split2_test_score                                       0.609495   \n",
       "split3_test_score                                       0.688221   \n",
       "split4_test_score                                       0.488625   \n",
       "mean_test_score                                         0.602469   \n",
       "std_test_score                                         0.0645575   \n",
       "rank_test_score                                               11   \n",
       "split0_train_score                                      0.592227   \n",
       "split1_train_score                                      0.585052   \n",
       "split2_train_score                                      0.605033   \n",
       "split3_train_score                                      0.551188   \n",
       "split4_train_score                                      0.651844   \n",
       "mean_train_score                                        0.597069   \n",
       "std_train_score                                        0.0326699   \n",
       "\n",
       "                                                             54  \\\n",
       "mean_fit_time                                         0.0101791   \n",
       "std_fit_time                                        0.000925034   \n",
       "mean_score_time                                      0.00191689   \n",
       "std_score_time                                      4.61437e-05   \n",
       "param_C                                                     100   \n",
       "param_gamma                                               1e-05   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}   \n",
       "split0_test_score                                      0.601704   \n",
       "split1_test_score                                      0.624133   \n",
       "split2_test_score                                       0.60952   \n",
       "split3_test_score                                      0.688124   \n",
       "split4_test_score                                      0.488596   \n",
       "mean_test_score                                        0.602416   \n",
       "std_test_score                                        0.0645347   \n",
       "rank_test_score                                              12   \n",
       "split0_train_score                                     0.592191   \n",
       "split1_train_score                                     0.584999   \n",
       "split2_train_score                                     0.605019   \n",
       "split3_train_score                                     0.551095   \n",
       "split4_train_score                                     0.651827   \n",
       "mean_train_score                                       0.597026   \n",
       "std_train_score                                       0.0326947   \n",
       "\n",
       "                                                             40  \\\n",
       "mean_fit_time                                         0.0107022   \n",
       "std_fit_time                                          0.0023864   \n",
       "mean_score_time                                      0.00195379   \n",
       "std_score_time                                      0.000136191   \n",
       "param_C                                                      10   \n",
       "param_gamma                                              0.0001   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                      0.600582   \n",
       "split1_test_score                                      0.624413   \n",
       "split2_test_score                                      0.610046   \n",
       "split3_test_score                                      0.687887   \n",
       "split4_test_score                                      0.487511   \n",
       "mean_test_score                                        0.602088   \n",
       "std_test_score                                        0.0648895   \n",
       "rank_test_score                                              13   \n",
       "split0_train_score                                     0.591718   \n",
       "split1_train_score                                     0.585177   \n",
       "split2_train_score                                     0.604431   \n",
       "split3_train_score                                     0.550408   \n",
       "split4_train_score                                      0.65145   \n",
       "mean_train_score                                       0.596637   \n",
       "std_train_score                                        0.032735   \n",
       "\n",
       "                                                             24  \\\n",
       "mean_fit_time                                         0.0215751   \n",
       "std_fit_time                                         0.00444165   \n",
       "mean_score_time                                      0.00671477   \n",
       "std_score_time                                       0.00485947   \n",
       "param_C                                                      10   \n",
       "param_gamma                                              0.0001   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                      0.600582   \n",
       "split1_test_score                                      0.624413   \n",
       "split2_test_score                                      0.610046   \n",
       "split3_test_score                                      0.687887   \n",
       "split4_test_score                                      0.487511   \n",
       "mean_test_score                                        0.602088   \n",
       "std_test_score                                        0.0648895   \n",
       "rank_test_score                                              13   \n",
       "split0_train_score                                     0.591718   \n",
       "split1_train_score                                     0.585177   \n",
       "split2_train_score                                     0.604431   \n",
       "split3_train_score                                     0.550408   \n",
       "split4_train_score                                      0.65145   \n",
       "mean_train_score                                       0.596637   \n",
       "std_train_score                                        0.032735   \n",
       "\n",
       "                                                           10  \\\n",
       "mean_fit_time                                       0.0118953   \n",
       "std_fit_time                                       0.00205233   \n",
       "mean_score_time                                    0.00208397   \n",
       "std_score_time                                    0.000412246   \n",
       "param_C                                                     1   \n",
       "param_gamma                                             0.001   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                    0.588835   \n",
       "split1_test_score                                    0.626915   \n",
       "split2_test_score                                    0.615747   \n",
       "split3_test_score                                    0.685008   \n",
       "split4_test_score                                    0.480609   \n",
       "mean_test_score                                      0.599423   \n",
       "std_test_score                                       0.067195   \n",
       "rank_test_score                                            15   \n",
       "split0_train_score                                   0.587692   \n",
       "split1_train_score                                    0.58307   \n",
       "split2_train_score                                   0.599671   \n",
       "split3_train_score                                   0.544759   \n",
       "split4_train_score                                   0.649127   \n",
       "mean_train_score                                     0.592864   \n",
       "std_train_score                                     0.0336115   \n",
       "\n",
       "                                                            60  \\\n",
       "mean_fit_time                                         0.133171   \n",
       "std_fit_time                                          0.064505   \n",
       "mean_score_time                                     0.00359774   \n",
       "std_score_time                                      0.00150139   \n",
       "param_C                                                    100   \n",
       "param_gamma                                               0.01   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.872598   \n",
       "split1_test_score                                      0.21817   \n",
       "split2_test_score                                     0.428369   \n",
       "split3_test_score                                     0.651112   \n",
       "split4_test_score                                     0.650916   \n",
       "mean_test_score                                       0.564233   \n",
       "std_test_score                                        0.222876   \n",
       "rank_test_score                                             16   \n",
       "split0_train_score                                    0.910485   \n",
       "split1_train_score                                    0.938649   \n",
       "split2_train_score                                    0.949865   \n",
       "split3_train_score                                    0.934242   \n",
       "split4_train_score                                    0.921044   \n",
       "mean_train_score                                      0.930857   \n",
       "std_train_score                                      0.0137465   \n",
       "\n",
       "                                                            45  \\\n",
       "mean_fit_time                                        0.0239544   \n",
       "std_fit_time                                        0.00313256   \n",
       "mean_score_time                                     0.00329905   \n",
       "std_score_time                                      0.00139897   \n",
       "param_C                                                     10   \n",
       "param_gamma                                               0.01   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 10, 'gamma': 0.01, 'kernel': 'poly'}   \n",
       "split0_test_score                                     0.739859   \n",
       "split1_test_score                                     0.506188   \n",
       "split2_test_score                                     0.327066   \n",
       "split3_test_score                                     0.540821   \n",
       "split4_test_score                                      0.61718   \n",
       "mean_test_score                                       0.546223   \n",
       "std_test_score                                        0.135788   \n",
       "rank_test_score                                             17   \n",
       "split0_train_score                                    0.637966   \n",
       "split1_train_score                                    0.728881   \n",
       "split2_train_score                                    0.735222   \n",
       "split3_train_score                                    0.713349   \n",
       "split4_train_score                                    0.711785   \n",
       "mean_train_score                                      0.705441   \n",
       "std_train_score                                      0.0349054   \n",
       "\n",
       "                                                            29  \\\n",
       "mean_fit_time                                        0.0158429   \n",
       "std_fit_time                                        0.00413705   \n",
       "mean_score_time                                     0.00232668   \n",
       "std_score_time                                     0.000414032   \n",
       "param_C                                                     10   \n",
       "param_gamma                                               0.01   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 10, 'gamma': 0.01, 'kernel': 'poly'}   \n",
       "split0_test_score                                     0.739859   \n",
       "split1_test_score                                     0.506188   \n",
       "split2_test_score                                     0.327066   \n",
       "split3_test_score                                     0.540821   \n",
       "split4_test_score                                      0.61718   \n",
       "mean_test_score                                       0.546223   \n",
       "std_test_score                                        0.135788   \n",
       "rank_test_score                                             17   \n",
       "split0_train_score                                    0.637966   \n",
       "split1_train_score                                    0.728881   \n",
       "split2_train_score                                    0.735222   \n",
       "split3_train_score                                    0.713349   \n",
       "split4_train_score                                    0.711785   \n",
       "mean_train_score                                      0.705441   \n",
       "std_train_score                                      0.0349054   \n",
       "\n",
       "                                                          46  \\\n",
       "mean_fit_time                                      0.0450253   \n",
       "std_fit_time                                      0.00747964   \n",
       "mean_score_time                                   0.00456495   \n",
       "std_score_time                                    0.00211113   \n",
       "param_C                                                   10   \n",
       "param_gamma                                              0.1   \n",
       "param_kernel                                             rbf   \n",
       "params              {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   0.435204   \n",
       "split1_test_score                                   0.609772   \n",
       "split2_test_score                                    0.53228   \n",
       "split3_test_score                                   0.670794   \n",
       "split4_test_score                                   0.390332   \n",
       "mean_test_score                                     0.527676   \n",
       "std_test_score                                      0.104554   \n",
       "rank_test_score                                           19   \n",
       "split0_train_score                                  0.971445   \n",
       "split1_train_score                                  0.980372   \n",
       "split2_train_score                                  0.986909   \n",
       "split3_train_score                                  0.975678   \n",
       "split4_train_score                                  0.981483   \n",
       "mean_train_score                                    0.979178   \n",
       "std_train_score                                   0.00526436   \n",
       "\n",
       "                                                          30  \\\n",
       "mean_fit_time                                      0.0308828   \n",
       "std_fit_time                                      0.00558554   \n",
       "mean_score_time                                   0.00241857   \n",
       "std_score_time                                   0.000118936   \n",
       "param_C                                                   10   \n",
       "param_gamma                                              0.1   \n",
       "param_kernel                                             rbf   \n",
       "params              {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   0.435204   \n",
       "split1_test_score                                   0.609772   \n",
       "split2_test_score                                    0.53228   \n",
       "split3_test_score                                   0.670794   \n",
       "split4_test_score                                   0.390332   \n",
       "mean_test_score                                     0.527676   \n",
       "std_test_score                                      0.104554   \n",
       "rank_test_score                                           19   \n",
       "split0_train_score                                  0.971445   \n",
       "split1_train_score                                  0.980372   \n",
       "split2_train_score                                  0.986909   \n",
       "split3_train_score                                  0.975678   \n",
       "split4_train_score                                  0.981483   \n",
       "mean_train_score                                    0.979178   \n",
       "std_train_score                                   0.00526436   \n",
       "\n",
       "                                                         14  \\\n",
       "mean_fit_time                                     0.0273979   \n",
       "std_fit_time                                       0.005554   \n",
       "mean_score_time                                  0.00467076   \n",
       "std_score_time                                   0.00219814   \n",
       "param_C                                                   1   \n",
       "param_gamma                                             0.1   \n",
       "param_kernel                                            rbf   \n",
       "params              {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}   \n",
       "split0_test_score                                  0.355729   \n",
       "split1_test_score                                  0.639574   \n",
       "split2_test_score                                  0.584508   \n",
       "split3_test_score                                  0.664282   \n",
       "split4_test_score                                   0.38628   \n",
       "mean_test_score                                    0.526075   \n",
       "std_test_score                                     0.129582   \n",
       "rank_test_score                                          21   \n",
       "split0_train_score                                 0.739797   \n",
       "split1_train_score                                  0.67069   \n",
       "split2_train_score                                  0.68368   \n",
       "split3_train_score                                 0.667803   \n",
       "split4_train_score                                 0.763858   \n",
       "mean_train_score                                   0.705166   \n",
       "std_train_score                                   0.0392182   \n",
       "\n",
       "                                                             61  \\\n",
       "mean_fit_time                                         0.0357441   \n",
       "std_fit_time                                          0.0101529   \n",
       "mean_score_time                                       0.0023859   \n",
       "std_score_time                                      0.000462036   \n",
       "param_C                                                     100   \n",
       "param_gamma                                                0.01   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 100, 'gamma': 0.01, 'kernel': 'poly'}   \n",
       "split0_test_score                                        0.7155   \n",
       "split1_test_score                                      0.381934   \n",
       "split2_test_score                                      0.174701   \n",
       "split3_test_score                                      0.684936   \n",
       "split4_test_score                                      0.589428   \n",
       "mean_test_score                                          0.5093   \n",
       "std_test_score                                         0.203944   \n",
       "rank_test_score                                              22   \n",
       "split0_train_score                                     0.869933   \n",
       "split1_train_score                                     0.905785   \n",
       "split2_train_score                                     0.914233   \n",
       "split3_train_score                                     0.895889   \n",
       "split4_train_score                                     0.881388   \n",
       "mean_train_score                                       0.893446   \n",
       "std_train_score                                       0.0160537   \n",
       "\n",
       "                                                           62  \\\n",
       "mean_fit_time                                       0.0827868   \n",
       "std_fit_time                                        0.0204823   \n",
       "mean_score_time                                    0.00586147   \n",
       "std_score_time                                     0.00396391   \n",
       "param_C                                                   100   \n",
       "param_gamma                                               0.1   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}   \n",
       "split0_test_score                                    0.428705   \n",
       "split1_test_score                                    0.565684   \n",
       "split2_test_score                                    0.505444   \n",
       "split3_test_score                                     0.60629   \n",
       "split4_test_score                                    0.364684   \n",
       "mean_test_score                                      0.494161   \n",
       "std_test_score                                      0.0881641   \n",
       "rank_test_score                                            23   \n",
       "split0_train_score                                   0.988381   \n",
       "split1_train_score                                   0.991501   \n",
       "split2_train_score                                   0.992268   \n",
       "split3_train_score                                   0.990185   \n",
       "split4_train_score                                   0.989694   \n",
       "mean_train_score                                     0.990406   \n",
       "std_train_score                                    0.00136558   \n",
       "\n",
       "                                                            78  \\\n",
       "mean_fit_time                                        0.0377906   \n",
       "std_fit_time                                        0.00661258   \n",
       "mean_score_time                                     0.00208931   \n",
       "std_score_time                                     0.000109848   \n",
       "param_C                                                   1000   \n",
       "param_gamma                                                0.1   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.428349   \n",
       "split1_test_score                                     0.547433   \n",
       "split2_test_score                                     0.505444   \n",
       "split3_test_score                                     0.614475   \n",
       "split4_test_score                                     0.361879   \n",
       "mean_test_score                                       0.491516   \n",
       "std_test_score                                        0.088584   \n",
       "rank_test_score                                             24   \n",
       "split0_train_score                                     0.99053   \n",
       "split1_train_score                                    0.992085   \n",
       "split2_train_score                                    0.992268   \n",
       "split3_train_score                                     0.99182   \n",
       "split4_train_score                                    0.989842   \n",
       "mean_train_score                                      0.991309   \n",
       "std_train_score                                     0.00095294   \n",
       "\n",
       "                                                             76  \\\n",
       "mean_fit_time                                          0.502411   \n",
       "std_fit_time                                           0.156675   \n",
       "mean_score_time                                      0.00219622   \n",
       "std_score_time                                      0.000134885   \n",
       "param_C                                                    1000   \n",
       "param_gamma                                                0.01   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}   \n",
       "split0_test_score                                      0.789621   \n",
       "split1_test_score                                      0.127063   \n",
       "split2_test_score                                      0.103674   \n",
       "split3_test_score                                      0.557753   \n",
       "split4_test_score                                      0.465256   \n",
       "mean_test_score                                        0.408673   \n",
       "std_test_score                                         0.261869   \n",
       "rank_test_score                                              25   \n",
       "split0_train_score                                      0.95171   \n",
       "split1_train_score                                      0.96656   \n",
       "split2_train_score                                     0.969074   \n",
       "split3_train_score                                     0.956054   \n",
       "split4_train_score                                     0.962414   \n",
       "mean_train_score                                       0.961162   \n",
       "std_train_score                                      0.00646102   \n",
       "\n",
       "                                                           13  \\\n",
       "mean_fit_time                                       0.0121384   \n",
       "std_fit_time                                       0.00324612   \n",
       "mean_score_time                                      0.002139   \n",
       "std_score_time                                     0.00049341   \n",
       "param_C                                                     1   \n",
       "param_gamma                                              0.01   \n",
       "param_kernel                                             poly   \n",
       "params              {'C': 1, 'gamma': 0.01, 'kernel': 'poly'}   \n",
       "split0_test_score                                    0.462861   \n",
       "split1_test_score                                    0.406009   \n",
       "split2_test_score                                    0.236573   \n",
       "split3_test_score                                    0.358814   \n",
       "split4_test_score                                    0.446391   \n",
       "mean_test_score                                      0.382129   \n",
       "std_test_score                                      0.0811758   \n",
       "rank_test_score                                            26   \n",
       "split0_train_score                                   0.399571   \n",
       "split1_train_score                                   0.458646   \n",
       "split2_train_score                                   0.465686   \n",
       "split3_train_score                                   0.461254   \n",
       "split4_train_score                                   0.433699   \n",
       "mean_train_score                                     0.443771   \n",
       "std_train_score                                     0.0247479   \n",
       "\n",
       "                                                               75  \\\n",
       "mean_fit_time                                           0.0160852   \n",
       "std_fit_time                                           0.00106594   \n",
       "mean_score_time                                        0.00314798   \n",
       "std_score_time                                         0.00154273   \n",
       "param_C                                                      1000   \n",
       "param_gamma                                                 0.001   \n",
       "param_kernel                                                 poly   \n",
       "params              {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}   \n",
       "split0_test_score                                        0.462871   \n",
       "split1_test_score                                        0.406009   \n",
       "split2_test_score                                        0.236579   \n",
       "split3_test_score                                        0.358749   \n",
       "split4_test_score                                        0.446391   \n",
       "mean_test_score                                           0.38212   \n",
       "std_test_score                                          0.0811795   \n",
       "rank_test_score                                                27   \n",
       "split0_train_score                                       0.399578   \n",
       "split1_train_score                                       0.458646   \n",
       "split2_train_score                                       0.465692   \n",
       "split3_train_score                                       0.461202   \n",
       "split4_train_score                                         0.4337   \n",
       "mean_train_score                                         0.443763   \n",
       "std_train_score                                         0.0247392   \n",
       "\n",
       "                                                              66  \\\n",
       "mean_fit_time                                          0.0198321   \n",
       "std_fit_time                                           0.0021651   \n",
       "mean_score_time                                       0.00272293   \n",
       "std_score_time                                       0.000494033   \n",
       "param_C                                                     1000   \n",
       "param_gamma                                                1e-07   \n",
       "param_kernel                                                 rbf   \n",
       "params              {'C': 1000, 'gamma': 1e-07, 'kernel': 'rbf'}   \n",
       "split0_test_score                                       0.209262   \n",
       "split1_test_score                                       0.307656   \n",
       "split2_test_score                                       0.318283   \n",
       "split3_test_score                                       0.286452   \n",
       "split4_test_score                                       0.187067   \n",
       "mean_test_score                                         0.261744   \n",
       "std_test_score                                          0.053378   \n",
       "rank_test_score                                               28   \n",
       "split0_train_score                                      0.247978   \n",
       "split1_train_score                                      0.250188   \n",
       "split2_train_score                                       0.23653   \n",
       "split3_train_score                                      0.237407   \n",
       "split4_train_score                                      0.271125   \n",
       "mean_train_score                                        0.248646   \n",
       "std_train_score                                        0.0124999   \n",
       "\n",
       "                                                             52  \\\n",
       "mean_fit_time                                         0.0115202   \n",
       "std_fit_time                                        0.000874156   \n",
       "mean_score_time                                      0.00234451   \n",
       "std_score_time                                      0.000388458   \n",
       "param_C                                                     100   \n",
       "param_gamma                                               1e-06   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 100, 'gamma': 1e-06, 'kernel': 'rbf'}   \n",
       "split0_test_score                                       0.20929   \n",
       "split1_test_score                                      0.307486   \n",
       "split2_test_score                                      0.318587   \n",
       "split3_test_score                                      0.286178   \n",
       "split4_test_score                                      0.186997   \n",
       "mean_test_score                                        0.261708   \n",
       "std_test_score                                        0.0534023   \n",
       "rank_test_score                                              29   \n",
       "split0_train_score                                      0.24801   \n",
       "split1_train_score                                     0.250033   \n",
       "split2_train_score                                     0.236751   \n",
       "split3_train_score                                     0.237231   \n",
       "split4_train_score                                     0.270998   \n",
       "mean_train_score                                       0.248604   \n",
       "std_train_score                                       0.0124396   \n",
       "\n",
       "                                                            22  \\\n",
       "mean_fit_time                                        0.0220408   \n",
       "std_fit_time                                        0.00427522   \n",
       "mean_score_time                                     0.00393763   \n",
       "std_score_time                                      0.00175039   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-05   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.209252   \n",
       "split1_test_score                                      0.30733   \n",
       "split2_test_score                                     0.318505   \n",
       "split3_test_score                                     0.286017   \n",
       "split4_test_score                                     0.187002   \n",
       "mean_test_score                                       0.261621   \n",
       "std_test_score                                       0.0533495   \n",
       "rank_test_score                                             30   \n",
       "split0_train_score                                     0.24799   \n",
       "split1_train_score                                    0.249858   \n",
       "split2_train_score                                    0.236633   \n",
       "split3_train_score                                    0.237089   \n",
       "split4_train_score                                    0.271054   \n",
       "mean_train_score                                      0.248525   \n",
       "std_train_score                                      0.0125045   \n",
       "\n",
       "                                                            38  \\\n",
       "mean_fit_time                                        0.0160624   \n",
       "std_fit_time                                        0.00255081   \n",
       "mean_score_time                                     0.00310073   \n",
       "std_score_time                                     0.000422163   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-05   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-05, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.209252   \n",
       "split1_test_score                                      0.30733   \n",
       "split2_test_score                                     0.318505   \n",
       "split3_test_score                                     0.286017   \n",
       "split4_test_score                                     0.187002   \n",
       "mean_test_score                                       0.261621   \n",
       "std_test_score                                       0.0533495   \n",
       "rank_test_score                                             30   \n",
       "split0_train_score                                     0.24799   \n",
       "split1_train_score                                    0.249858   \n",
       "split2_train_score                                    0.236633   \n",
       "split3_train_score                                    0.237089   \n",
       "split4_train_score                                    0.271054   \n",
       "mean_train_score                                      0.248525   \n",
       "std_train_score                                      0.0125045   \n",
       "\n",
       "                                                            8   \\\n",
       "mean_fit_time                                        0.0145696   \n",
       "std_fit_time                                        0.00259304   \n",
       "mean_score_time                                     0.00371199   \n",
       "std_score_time                                      0.00249406   \n",
       "param_C                                                      1   \n",
       "param_gamma                                             0.0001   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     0.207828   \n",
       "split1_test_score                                     0.306075   \n",
       "split2_test_score                                     0.317609   \n",
       "split3_test_score                                      0.28462   \n",
       "split4_test_score                                     0.185961   \n",
       "mean_test_score                                       0.260419   \n",
       "std_test_score                                       0.0533865   \n",
       "rank_test_score                                             32   \n",
       "split0_train_score                                    0.246852   \n",
       "split1_train_score                                    0.248389   \n",
       "split2_train_score                                    0.235399   \n",
       "split3_train_score                                    0.235801   \n",
       "split4_train_score                                    0.269974   \n",
       "mean_train_score                                      0.247283   \n",
       "std_train_score                                      0.0125649   \n",
       "\n",
       "                                                              59  \\\n",
       "mean_fit_time                                         0.00913777   \n",
       "std_fit_time                                         0.000543147   \n",
       "mean_score_time                                         0.001682   \n",
       "std_score_time                                       6.18584e-05   \n",
       "param_C                                                      100   \n",
       "param_gamma                                                0.001   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 100, 'gamma': 0.001, 'kernel': 'poly'}   \n",
       "split0_test_score                                      0.0224603   \n",
       "split1_test_score                                      0.0637379   \n",
       "split2_test_score                                      0.0248847   \n",
       "split3_test_score                                      0.0123317   \n",
       "split4_test_score                                      0.0709321   \n",
       "mean_test_score                                        0.0388694   \n",
       "std_test_score                                         0.0237299   \n",
       "rank_test_score                                               33   \n",
       "split0_train_score                                     0.0781266   \n",
       "split1_train_score                                     0.0818926   \n",
       "split2_train_score                                     0.0942315   \n",
       "split3_train_score                                     0.0706144   \n",
       "split4_train_score                                     0.0537512   \n",
       "mean_train_score                                       0.0757232   \n",
       "std_train_score                                         0.013381   \n",
       "\n",
       "                                                              64  \\\n",
       "mean_fit_time                                          0.0238835   \n",
       "std_fit_time                                          0.00519675   \n",
       "mean_score_time                                       0.00514655   \n",
       "std_score_time                                        0.00213077   \n",
       "param_C                                                     1000   \n",
       "param_gamma                                                1e-08   \n",
       "param_kernel                                                 rbf   \n",
       "params              {'C': 1000, 'gamma': 1e-08, 'kernel': 'rbf'}   \n",
       "split0_test_score                                     -0.0406104   \n",
       "split1_test_score                                      0.0185972   \n",
       "split2_test_score                                     0.00850148   \n",
       "split3_test_score                                      -0.038756   \n",
       "split4_test_score                                     0.00471727   \n",
       "mean_test_score                                       -0.0095101   \n",
       "std_test_score                                         0.0250576   \n",
       "rank_test_score                                               34   \n",
       "split0_train_score                                   -0.00869535   \n",
       "split1_train_score                                   -0.00320676   \n",
       "split2_train_score                                   -0.00945768   \n",
       "split3_train_score                                   -0.00372862   \n",
       "split4_train_score                                   -0.00740829   \n",
       "mean_train_score                                     -0.00649934   \n",
       "std_train_score                                       0.00256587   \n",
       "\n",
       "                                                            20  \\\n",
       "mean_fit_time                                        0.0212007   \n",
       "std_fit_time                                        0.00278245   \n",
       "mean_score_time                                     0.00367508   \n",
       "std_score_time                                      0.00108314   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-06   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.0406615   \n",
       "split1_test_score                                    0.0187665   \n",
       "split2_test_score                                   0.00858414   \n",
       "split3_test_score                                   -0.0388902   \n",
       "split4_test_score                                   0.00464025   \n",
       "mean_test_score                                    -0.00951217   \n",
       "std_test_score                                       0.0251428   \n",
       "rank_test_score                                             35   \n",
       "split0_train_score                                 -0.00869177   \n",
       "split1_train_score                                 -0.00304668   \n",
       "split2_train_score                                 -0.00938972   \n",
       "split3_train_score                                 -0.00380458   \n",
       "split4_train_score                                 -0.00754216   \n",
       "mean_train_score                                   -0.00649498   \n",
       "std_train_score                                     0.00258577   \n",
       "\n",
       "                                                            36  \\\n",
       "mean_fit_time                                        0.0272831   \n",
       "std_fit_time                                        0.00812477   \n",
       "mean_score_time                                      0.0052434   \n",
       "std_score_time                                      0.00256526   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-06   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-06, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.0406615   \n",
       "split1_test_score                                    0.0187665   \n",
       "split2_test_score                                   0.00858414   \n",
       "split3_test_score                                   -0.0388902   \n",
       "split4_test_score                                   0.00464025   \n",
       "mean_test_score                                    -0.00951217   \n",
       "std_test_score                                       0.0251428   \n",
       "rank_test_score                                             35   \n",
       "split0_train_score                                 -0.00869177   \n",
       "split1_train_score                                 -0.00304668   \n",
       "split2_train_score                                 -0.00938972   \n",
       "split3_train_score                                 -0.00380458   \n",
       "split4_train_score                                 -0.00754216   \n",
       "mean_train_score                                   -0.00649498   \n",
       "std_train_score                                     0.00258577   \n",
       "\n",
       "                                                             50  \\\n",
       "mean_fit_time                                         0.0119678   \n",
       "std_fit_time                                        0.000759772   \n",
       "mean_score_time                                      0.00252662   \n",
       "std_score_time                                      0.000377339   \n",
       "param_C                                                     100   \n",
       "param_gamma                                               1e-07   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 100, 'gamma': 1e-07, 'kernel': 'rbf'}   \n",
       "split0_test_score                                    -0.0406738   \n",
       "split1_test_score                                     0.0187646   \n",
       "split2_test_score                                     0.0086102   \n",
       "split3_test_score                                    -0.0389045   \n",
       "split4_test_score                                    0.00464004   \n",
       "mean_test_score                                      -0.0095127   \n",
       "std_test_score                                        0.0251525   \n",
       "rank_test_score                                              37   \n",
       "split0_train_score                                  -0.00875644   \n",
       "split1_train_score                                  -0.00304825   \n",
       "split2_train_score                                  -0.00936795   \n",
       "split3_train_score                                  -0.00381185   \n",
       "split4_train_score                                  -0.00754399   \n",
       "mean_train_score                                     -0.0065057   \n",
       "std_train_score                                      0.00259026   \n",
       "\n",
       "                                                           6   \\\n",
       "mean_fit_time                                        0.018593   \n",
       "std_fit_time                                        0.0087004   \n",
       "mean_score_time                                    0.00384164   \n",
       "std_score_time                                     0.00167107   \n",
       "param_C                                                     1   \n",
       "param_gamma                                             1e-05   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 1, 'gamma': 1e-05, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.040687   \n",
       "split1_test_score                                   0.0187454   \n",
       "split2_test_score                                  0.00856116   \n",
       "split3_test_score                                  -0.0389132   \n",
       "split4_test_score                                  0.00462365   \n",
       "mean_test_score                                   -0.00953401   \n",
       "std_test_score                                      0.0251446   \n",
       "rank_test_score                                            38   \n",
       "split0_train_score                                -0.00871477   \n",
       "split1_train_score                                -0.00306835   \n",
       "split2_train_score                                -0.00941117   \n",
       "split3_train_score                                -0.00382536   \n",
       "split4_train_score                                -0.00756198   \n",
       "mean_train_score                                  -0.00651633   \n",
       "std_train_score                                    0.00258598   \n",
       "\n",
       "                                                             27  \\\n",
       "mean_fit_time                                         0.0206897   \n",
       "std_fit_time                                         0.00447575   \n",
       "mean_score_time                                       0.0052773   \n",
       "std_score_time                                       0.00299151   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               0.001   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.0646869   \n",
       "split1_test_score                                    -0.0148211   \n",
       "split2_test_score                                    -0.0333825   \n",
       "split3_test_score                                    -0.0730773   \n",
       "split4_test_score                                    -0.0120532   \n",
       "mean_test_score                                      -0.0396042   \n",
       "std_test_score                                        0.0251458   \n",
       "rank_test_score                                              39   \n",
       "split0_train_score                                   -0.0271169   \n",
       "split1_train_score                                   -0.0219565   \n",
       "split2_train_score                                   -0.0294688   \n",
       "split3_train_score                                   -0.0224293   \n",
       "split4_train_score                                   -0.0333662   \n",
       "mean_train_score                                     -0.0268675   \n",
       "std_train_score                                      0.00430992   \n",
       "\n",
       "                                                             43  \\\n",
       "mean_fit_time                                         0.0110405   \n",
       "std_fit_time                                          0.0032653   \n",
       "mean_score_time                                      0.00237026   \n",
       "std_score_time                                      0.000971896   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               0.001   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 0.001, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.0646869   \n",
       "split1_test_score                                    -0.0148211   \n",
       "split2_test_score                                    -0.0333825   \n",
       "split3_test_score                                    -0.0730773   \n",
       "split4_test_score                                    -0.0120532   \n",
       "mean_test_score                                      -0.0396042   \n",
       "std_test_score                                        0.0251458   \n",
       "rank_test_score                                              39   \n",
       "split0_train_score                                   -0.0271169   \n",
       "split1_train_score                                   -0.0219565   \n",
       "split2_train_score                                   -0.0294688   \n",
       "split3_train_score                                   -0.0224293   \n",
       "split4_train_score                                   -0.0333662   \n",
       "mean_train_score                                     -0.0268675   \n",
       "std_train_score                                      0.00430992   \n",
       "\n",
       "                                                             48  \\\n",
       "mean_fit_time                                         0.0145054   \n",
       "std_fit_time                                         0.00305371   \n",
       "mean_score_time                                      0.00299258   \n",
       "std_score_time                                      0.000783497   \n",
       "param_C                                                     100   \n",
       "param_gamma                                               1e-08   \n",
       "param_kernel                                                rbf   \n",
       "params              {'C': 100, 'gamma': 1e-08, 'kernel': 'rbf'}   \n",
       "split0_test_score                                    -0.0711034   \n",
       "split1_test_score                                     -0.018419   \n",
       "split2_test_score                                    -0.0358528   \n",
       "split3_test_score                                    -0.0799358   \n",
       "split4_test_score                                    -0.0187114   \n",
       "mean_test_score                                      -0.0448045   \n",
       "std_test_score                                        0.0260116   \n",
       "rank_test_score                                              41   \n",
       "split0_train_score                                   -0.0418348   \n",
       "split1_train_score                                    -0.034148   \n",
       "split2_train_score                                   -0.0422564   \n",
       "split3_train_score                                   -0.0351291   \n",
       "split4_train_score                                   -0.0434699   \n",
       "mean_train_score                                     -0.0393676   \n",
       "std_train_score                                      0.00391076   \n",
       "\n",
       "                                                            34  \\\n",
       "mean_fit_time                                        0.0310912   \n",
       "std_fit_time                                        0.00688049   \n",
       "mean_score_time                                     0.00642266   \n",
       "std_score_time                                      0.00325829   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-07   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.0710787   \n",
       "split1_test_score                                   -0.0184238   \n",
       "split2_test_score                                   -0.0358706   \n",
       "split3_test_score                                   -0.0799505   \n",
       "split4_test_score                                   -0.0187188   \n",
       "mean_test_score                                     -0.0448085   \n",
       "std_test_score                                       0.0260069   \n",
       "rank_test_score                                             42   \n",
       "split0_train_score                                  -0.0418113   \n",
       "split1_train_score                                  -0.0341527   \n",
       "split2_train_score                                  -0.0422712   \n",
       "split3_train_score                                  -0.0351372   \n",
       "split4_train_score                                  -0.0434851   \n",
       "mean_train_score                                    -0.0393715   \n",
       "std_train_score                                     0.00391021   \n",
       "\n",
       "                                                            18  \\\n",
       "mean_fit_time                                        0.0241443   \n",
       "std_fit_time                                        0.00624184   \n",
       "mean_score_time                                     0.00452676   \n",
       "std_score_time                                      0.00192501   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-07   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-07, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.0710787   \n",
       "split1_test_score                                   -0.0184238   \n",
       "split2_test_score                                   -0.0358706   \n",
       "split3_test_score                                   -0.0799505   \n",
       "split4_test_score                                   -0.0187188   \n",
       "mean_test_score                                     -0.0448085   \n",
       "std_test_score                                       0.0260069   \n",
       "rank_test_score                                             42   \n",
       "split0_train_score                                  -0.0418113   \n",
       "split1_train_score                                  -0.0341527   \n",
       "split2_train_score                                  -0.0422712   \n",
       "split3_train_score                                  -0.0351372   \n",
       "split4_train_score                                  -0.0434851   \n",
       "mean_train_score                                    -0.0393715   \n",
       "std_train_score                                     0.00391021   \n",
       "\n",
       "                                                           4   \\\n",
       "mean_fit_time                                       0.0167026   \n",
       "std_fit_time                                       0.00224967   \n",
       "mean_score_time                                    0.00380349   \n",
       "std_score_time                                     0.00192591   \n",
       "param_C                                                     1   \n",
       "param_gamma                                             1e-06   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 1, 'gamma': 1e-06, 'kernel': 'rbf'}   \n",
       "split0_test_score                                  -0.0710784   \n",
       "split1_test_score                                  -0.0184247   \n",
       "split2_test_score                                  -0.0358714   \n",
       "split3_test_score                                  -0.0799506   \n",
       "split4_test_score                                   -0.018719   \n",
       "mean_test_score                                    -0.0448088   \n",
       "std_test_score                                      0.0260066   \n",
       "rank_test_score                                            44   \n",
       "split0_train_score                                  -0.041811   \n",
       "split1_train_score                                 -0.0341536   \n",
       "split2_train_score                                 -0.0422719   \n",
       "split3_train_score                                 -0.0351374   \n",
       "split4_train_score                                 -0.0434854   \n",
       "mean_train_score                                   -0.0393719   \n",
       "std_train_score                                    0.00391003   \n",
       "\n",
       "                                                            11  \\\n",
       "mean_fit_time                                       0.00860424   \n",
       "std_fit_time                                       0.000182091   \n",
       "mean_score_time                                     0.00162153   \n",
       "std_score_time                                     3.15947e-05   \n",
       "param_C                                                      1   \n",
       "param_gamma                                              0.001   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 1, 'gamma': 0.001, 'kernel': 'poly'}   \n",
       "split0_test_score                                   -0.0741832   \n",
       "split1_test_score                                   -0.0224651   \n",
       "split2_test_score                                   -0.0396589   \n",
       "split3_test_score                                   -0.0825685   \n",
       "split4_test_score                                   -0.0207454   \n",
       "mean_test_score                                     -0.0479242   \n",
       "std_test_score                                       0.0258648   \n",
       "rank_test_score                                             45   \n",
       "split0_train_score                                  -0.0435839   \n",
       "split1_train_score                                  -0.0360456   \n",
       "split2_train_score                                  -0.0438874   \n",
       "split3_train_score                                  -0.0359387   \n",
       "split4_train_score                                  -0.0459806   \n",
       "mean_train_score                                    -0.0410872   \n",
       "std_train_score                                     0.00424133   \n",
       "\n",
       "                                                                73  \\\n",
       "mean_fit_time                                            0.0185493   \n",
       "std_fit_time                                             0.0016399   \n",
       "mean_score_time                                         0.00524173   \n",
       "std_score_time                                          0.00322616   \n",
       "param_C                                                       1000   \n",
       "param_gamma                                                 0.0001   \n",
       "param_kernel                                                  poly   \n",
       "params              {'C': 1000, 'gamma': 0.0001, 'kernel': 'poly'}   \n",
       "split0_test_score                                       -0.0741832   \n",
       "split1_test_score                                       -0.0224651   \n",
       "split2_test_score                                       -0.0396589   \n",
       "split3_test_score                                       -0.0825685   \n",
       "split4_test_score                                       -0.0207454   \n",
       "mean_test_score                                         -0.0479242   \n",
       "std_test_score                                           0.0258648   \n",
       "rank_test_score                                                 46   \n",
       "split0_train_score                                      -0.0435839   \n",
       "split1_train_score                                      -0.0360456   \n",
       "split2_train_score                                      -0.0438874   \n",
       "split3_train_score                                      -0.0359387   \n",
       "split4_train_score                                      -0.0459806   \n",
       "mean_train_score                                        -0.0410872   \n",
       "std_train_score                                         0.00424133   \n",
       "\n",
       "                                                            16  \\\n",
       "mean_fit_time                                        0.0213999   \n",
       "std_fit_time                                        0.00434744   \n",
       "mean_score_time                                     0.00532994   \n",
       "std_score_time                                      0.00190058   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-08   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-08, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.0748417   \n",
       "split1_test_score                                    -0.022919   \n",
       "split2_test_score                                   -0.0399328   \n",
       "split3_test_score                                   -0.0832376   \n",
       "split4_test_score                                   -0.0214321   \n",
       "mean_test_score                                     -0.0484726   \n",
       "std_test_score                                        0.025927   \n",
       "rank_test_score                                             47   \n",
       "split0_train_score                                  -0.0458417   \n",
       "split1_train_score                                  -0.0379867   \n",
       "split2_train_score                                  -0.0452058   \n",
       "split3_train_score                                  -0.0378425   \n",
       "split4_train_score                                   -0.047834   \n",
       "mean_train_score                                    -0.0429422   \n",
       "std_train_score                                     0.00419582   \n",
       "\n",
       "                                                            32  \\\n",
       "mean_fit_time                                        0.0325716   \n",
       "std_fit_time                                        0.00870401   \n",
       "mean_score_time                                     0.00658307   \n",
       "std_score_time                                       0.0035396   \n",
       "param_C                                                     10   \n",
       "param_gamma                                              1e-08   \n",
       "param_kernel                                               rbf   \n",
       "params              {'C': 10, 'gamma': 1e-08, 'kernel': 'rbf'}   \n",
       "split0_test_score                                   -0.0748417   \n",
       "split1_test_score                                    -0.022919   \n",
       "split2_test_score                                   -0.0399328   \n",
       "split3_test_score                                   -0.0832376   \n",
       "split4_test_score                                   -0.0214321   \n",
       "mean_test_score                                     -0.0484726   \n",
       "std_test_score                                        0.025927   \n",
       "rank_test_score                                             47   \n",
       "split0_train_score                                  -0.0458417   \n",
       "split1_train_score                                  -0.0379867   \n",
       "split2_train_score                                  -0.0452058   \n",
       "split3_train_score                                  -0.0378425   \n",
       "split4_train_score                                   -0.047834   \n",
       "mean_train_score                                    -0.0429422   \n",
       "std_train_score                                     0.00419582   \n",
       "\n",
       "                                                           2   \\\n",
       "mean_fit_time                                       0.0124457   \n",
       "std_fit_time                                       0.00144901   \n",
       "mean_score_time                                    0.00249085   \n",
       "std_score_time                                    0.000580529   \n",
       "param_C                                                     1   \n",
       "param_gamma                                             1e-07   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 1, 'gamma': 1e-07, 'kernel': 'rbf'}   \n",
       "split0_test_score                                  -0.0748393   \n",
       "split1_test_score                                  -0.0229194   \n",
       "split2_test_score                                  -0.0399346   \n",
       "split3_test_score                                  -0.0832391   \n",
       "split4_test_score                                  -0.0214328   \n",
       "mean_test_score                                    -0.0484731   \n",
       "std_test_score                                      0.0259265   \n",
       "rank_test_score                                            49   \n",
       "split0_train_score                                 -0.0458394   \n",
       "split1_train_score                                 -0.0379872   \n",
       "split2_train_score                                 -0.0452074   \n",
       "split3_train_score                                 -0.0378434   \n",
       "split4_train_score                                 -0.0478355   \n",
       "mean_train_score                                   -0.0429426   \n",
       "std_train_score                                     0.0041957   \n",
       "\n",
       "                                                          15  \\\n",
       "mean_fit_time                                       0.078797   \n",
       "std_fit_time                                       0.0157642   \n",
       "mean_score_time                                   0.00181007   \n",
       "std_score_time                                   0.000227448   \n",
       "param_C                                                    1   \n",
       "param_gamma                                              0.1   \n",
       "param_kernel                                            poly   \n",
       "params              {'C': 1, 'gamma': 0.1, 'kernel': 'poly'}   \n",
       "split0_test_score                                   0.207179   \n",
       "split1_test_score                                   0.199718   \n",
       "split2_test_score                                  -0.665647   \n",
       "split3_test_score                                   0.437063   \n",
       "split4_test_score                                  -0.420811   \n",
       "mean_test_score                                   -0.0484995   \n",
       "std_test_score                                      0.420057   \n",
       "rank_test_score                                           50   \n",
       "split0_train_score                                  0.936629   \n",
       "split1_train_score                                  0.960848   \n",
       "split2_train_score                                  0.959221   \n",
       "split3_train_score                                   0.94876   \n",
       "split4_train_score                                  0.950617   \n",
       "mean_train_score                                    0.951215   \n",
       "std_train_score                                   0.00867213   \n",
       "\n",
       "                                                              77  \\\n",
       "mean_fit_time                                          0.0368963   \n",
       "std_fit_time                                          0.00390245   \n",
       "mean_score_time                                       0.00156126   \n",
       "std_score_time                                       6.08431e-05   \n",
       "param_C                                                     1000   \n",
       "param_gamma                                                 0.01   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 1000, 'gamma': 0.01, 'kernel': 'poly'}   \n",
       "split0_test_score                                       0.207236   \n",
       "split1_test_score                                       0.199733   \n",
       "split2_test_score                                      -0.665923   \n",
       "split3_test_score                                       0.436967   \n",
       "split4_test_score                                      -0.420997   \n",
       "mean_test_score                                       -0.0485968   \n",
       "std_test_score                                          0.420158   \n",
       "rank_test_score                                               51   \n",
       "split0_train_score                                       0.93663   \n",
       "split1_train_score                                      0.960849   \n",
       "split2_train_score                                      0.959221   \n",
       "split3_train_score                                      0.948765   \n",
       "split4_train_score                                      0.950617   \n",
       "mean_train_score                                        0.951217   \n",
       "std_train_score                                       0.00867172   \n",
       "\n",
       "                                                               57  \\\n",
       "mean_fit_time                                          0.00941353   \n",
       "std_fit_time                                          0.000989898   \n",
       "mean_score_time                                        0.00173917   \n",
       "std_score_time                                        0.000185636   \n",
       "param_C                                                       100   \n",
       "param_gamma                                                0.0001   \n",
       "param_kernel                                                 poly   \n",
       "params              {'C': 100, 'gamma': 0.0001, 'kernel': 'poly'}   \n",
       "split0_test_score                                      -0.0751064   \n",
       "split1_test_score                                      -0.0233246   \n",
       "split2_test_score                                       -0.040279   \n",
       "split3_test_score                                      -0.0835398   \n",
       "split4_test_score                                      -0.0216359   \n",
       "mean_test_score                                        -0.0487771   \n",
       "std_test_score                                          0.0259167   \n",
       "rank_test_score                                                52   \n",
       "split0_train_score                                     -0.0459673   \n",
       "split1_train_score                                     -0.0381701   \n",
       "split2_train_score                                     -0.0453396   \n",
       "split3_train_score                                     -0.0379382   \n",
       "split4_train_score                                     -0.0480769   \n",
       "mean_train_score                                       -0.0430984   \n",
       "std_train_score                                        0.00421793   \n",
       "\n",
       "                                                           0   \\\n",
       "mean_fit_time                                       0.0155993   \n",
       "std_fit_time                                        0.0049902   \n",
       "mean_score_time                                    0.00372028   \n",
       "std_score_time                                     0.00162344   \n",
       "param_C                                                     1   \n",
       "param_gamma                                             1e-08   \n",
       "param_kernel                                              rbf   \n",
       "params              {'C': 1, 'gamma': 1e-08, 'kernel': 'rbf'}   \n",
       "split0_test_score                                  -0.0751722   \n",
       "split1_test_score                                    -0.02337   \n",
       "split2_test_score                                  -0.0403064   \n",
       "split3_test_score                                  -0.0836068   \n",
       "split4_test_score                                  -0.0217046   \n",
       "mean_test_score                                     -0.048832   \n",
       "std_test_score                                      0.0259229   \n",
       "rank_test_score                                            53   \n",
       "split0_train_score                                 -0.0462016   \n",
       "split1_train_score                                 -0.0383714   \n",
       "split2_train_score                                 -0.0454716   \n",
       "split3_train_score                                 -0.0381357   \n",
       "split4_train_score                                 -0.0482713   \n",
       "mean_train_score                                   -0.0432903   \n",
       "std_train_score                                    0.00421448   \n",
       "\n",
       "                                                              41  \\\n",
       "mean_fit_time                                         0.00931616   \n",
       "std_fit_time                                         0.000979346   \n",
       "mean_score_time                                       0.00174618   \n",
       "std_score_time                                       0.000107926   \n",
       "param_C                                                       10   \n",
       "param_gamma                                               0.0001   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 10, 'gamma': 0.0001, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.0751987   \n",
       "split1_test_score                                     -0.0234106   \n",
       "split2_test_score                                     -0.0403411   \n",
       "split3_test_score                                     -0.0836371   \n",
       "split4_test_score                                      -0.021725   \n",
       "mean_test_score                                       -0.0488625   \n",
       "std_test_score                                         0.0259219   \n",
       "rank_test_score                                               54   \n",
       "split0_train_score                                     -0.046214   \n",
       "split1_train_score                                    -0.0383896   \n",
       "split2_train_score                                     -0.045485   \n",
       "split3_train_score                                    -0.0381452   \n",
       "split4_train_score                                    -0.0482955   \n",
       "mean_train_score                                      -0.0433059   \n",
       "std_train_score                                       0.00421672   \n",
       "\n",
       "                                                              25  \\\n",
       "mean_fit_time                                           0.024225   \n",
       "std_fit_time                                          0.00806805   \n",
       "mean_score_time                                        0.0034677   \n",
       "std_score_time                                        0.00136615   \n",
       "param_C                                                       10   \n",
       "param_gamma                                               0.0001   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 10, 'gamma': 0.0001, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.0751987   \n",
       "split1_test_score                                     -0.0234106   \n",
       "split2_test_score                                     -0.0403411   \n",
       "split3_test_score                                     -0.0836371   \n",
       "split4_test_score                                      -0.021725   \n",
       "mean_test_score                                       -0.0488625   \n",
       "std_test_score                                         0.0259219   \n",
       "rank_test_score                                               54   \n",
       "split0_train_score                                     -0.046214   \n",
       "split1_train_score                                    -0.0383896   \n",
       "split2_train_score                                     -0.045485   \n",
       "split3_train_score                                    -0.0381452   \n",
       "split4_train_score                                    -0.0482955   \n",
       "mean_train_score                                      -0.0433059   \n",
       "std_train_score                                       0.00421672   \n",
       "\n",
       "                                                               71  \\\n",
       "mean_fit_time                                          0.00981388   \n",
       "std_fit_time                                           0.00174013   \n",
       "mean_score_time                                         0.0016223   \n",
       "std_score_time                                        4.63506e-05   \n",
       "param_C                                                      1000   \n",
       "param_gamma                                                 1e-05   \n",
       "param_kernel                                                 poly   \n",
       "params              {'C': 1000, 'gamma': 1e-05, 'kernel': 'poly'}   \n",
       "split0_test_score                                      -0.0752079   \n",
       "split1_test_score                                      -0.0234192   \n",
       "split2_test_score                                      -0.0403473   \n",
       "split3_test_score                                      -0.0836468   \n",
       "split4_test_score                                      -0.0217339   \n",
       "mean_test_score                                         -0.048871   \n",
       "std_test_score                                          0.0259224   \n",
       "rank_test_score                                                56   \n",
       "split0_train_score                                     -0.0462388   \n",
       "split1_train_score                                     -0.0384117   \n",
       "split2_train_score                                     -0.0454995   \n",
       "split3_train_score                                      -0.038166   \n",
       "split4_train_score                                     -0.0483174   \n",
       "mean_train_score                                       -0.0433267   \n",
       "std_train_score                                        0.00421661   \n",
       "\n",
       "                                                             9   \\\n",
       "mean_fit_time                                        0.00949898   \n",
       "std_fit_time                                         0.00118055   \n",
       "mean_score_time                                      0.00168285   \n",
       "std_score_time                                      6.13237e-05   \n",
       "param_C                                                       1   \n",
       "param_gamma                                              0.0001   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 1, 'gamma': 0.0001, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.0752079   \n",
       "split1_test_score                                    -0.0234192   \n",
       "split2_test_score                                    -0.0403473   \n",
       "split3_test_score                                    -0.0836468   \n",
       "split4_test_score                                    -0.0217339   \n",
       "mean_test_score                                       -0.048871   \n",
       "std_test_score                                        0.0259224   \n",
       "rank_test_score                                              57   \n",
       "split0_train_score                                   -0.0462388   \n",
       "split1_train_score                                   -0.0384117   \n",
       "split2_train_score                                   -0.0454995   \n",
       "split3_train_score                                    -0.038166   \n",
       "split4_train_score                                   -0.0483174   \n",
       "mean_train_score                                     -0.0433267   \n",
       "std_train_score                                      0.00421661   \n",
       "\n",
       "                                                              55  \\\n",
       "mean_fit_time                                         0.00966134   \n",
       "std_fit_time                                           0.0013405   \n",
       "mean_score_time                                       0.00211902   \n",
       "std_score_time                                       0.000465749   \n",
       "param_C                                                      100   \n",
       "param_gamma                                                1e-05   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 100, 'gamma': 1e-05, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.0752089   \n",
       "split1_test_score                                       -0.02342   \n",
       "split2_test_score                                     -0.0403479   \n",
       "split3_test_score                                     -0.0836477   \n",
       "split4_test_score                                     -0.0217348   \n",
       "mean_test_score                                       -0.0488719   \n",
       "std_test_score                                         0.0259225   \n",
       "rank_test_score                                               58   \n",
       "split0_train_score                                    -0.0462413   \n",
       "split1_train_score                                    -0.0384139   \n",
       "split2_train_score                                     -0.045501   \n",
       "split3_train_score                                    -0.0381681   \n",
       "split4_train_score                                    -0.0483196   \n",
       "mean_train_score                                      -0.0433288   \n",
       "std_train_score                                        0.0042166   \n",
       "\n",
       "                                                             23  \\\n",
       "mean_fit_time                                         0.0208189   \n",
       "std_fit_time                                         0.00265354   \n",
       "mean_score_time                                       0.0037468   \n",
       "std_score_time                                       0.00163534   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-05   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-05, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.0752089   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836478   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                      -0.0488719   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              59   \n",
       "split0_train_score                                   -0.0462415   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455011   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483198   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                             39  \\\n",
       "mean_fit_time                                         0.0217481   \n",
       "std_fit_time                                         0.00712397   \n",
       "mean_score_time                                       0.0028317   \n",
       "std_score_time                                       0.00101608   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-05   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-05, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.0752089   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836478   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                      -0.0488719   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              59   \n",
       "split0_train_score                                   -0.0462415   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455011   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483198   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                            7   \\\n",
       "mean_fit_time                                        0.0111115   \n",
       "std_fit_time                                        0.00113538   \n",
       "mean_score_time                                     0.00291424   \n",
       "std_score_time                                      0.00166021   \n",
       "param_C                                                      1   \n",
       "param_gamma                                              1e-05   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 1, 'gamma': 1e-05, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.075209   \n",
       "split1_test_score                                   -0.0234201   \n",
       "split2_test_score                                   -0.0403479   \n",
       "split3_test_score                                   -0.0836479   \n",
       "split4_test_score                                   -0.0217349   \n",
       "mean_test_score                                      -0.048872   \n",
       "std_test_score                                       0.0259225   \n",
       "rank_test_score                                             61   \n",
       "split0_train_score                                  -0.0462416   \n",
       "split1_train_score                                  -0.0384141   \n",
       "split2_train_score                                  -0.0455012   \n",
       "split3_train_score                                  -0.0381683   \n",
       "split4_train_score                                  -0.0483199   \n",
       "mean_train_score                                     -0.043329   \n",
       "std_train_score                                      0.0042166   \n",
       "\n",
       "                                                               69  \\\n",
       "mean_fit_time                                           0.0146426   \n",
       "std_fit_time                                          0.000943859   \n",
       "mean_score_time                                        0.00329132   \n",
       "std_score_time                                         0.00198316   \n",
       "param_C                                                      1000   \n",
       "param_gamma                                                 1e-06   \n",
       "param_kernel                                                 poly   \n",
       "params              {'C': 1000, 'gamma': 1e-06, 'kernel': 'poly'}   \n",
       "split0_test_score                                       -0.075209   \n",
       "split1_test_score                                      -0.0234201   \n",
       "split2_test_score                                      -0.0403479   \n",
       "split3_test_score                                      -0.0836479   \n",
       "split4_test_score                                      -0.0217349   \n",
       "mean_test_score                                         -0.048872   \n",
       "std_test_score                                          0.0259225   \n",
       "rank_test_score                                                61   \n",
       "split0_train_score                                     -0.0462416   \n",
       "split1_train_score                                     -0.0384141   \n",
       "split2_train_score                                     -0.0455012   \n",
       "split3_train_score                                     -0.0381683   \n",
       "split4_train_score                                     -0.0483199   \n",
       "mean_train_score                                        -0.043329   \n",
       "std_train_score                                         0.0042166   \n",
       "\n",
       "                                                              53  \\\n",
       "mean_fit_time                                         0.00882339   \n",
       "std_fit_time                                         0.000239053   \n",
       "mean_score_time                                       0.00166922   \n",
       "std_score_time                                       4.80867e-05   \n",
       "param_C                                                      100   \n",
       "param_gamma                                                1e-06   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 100, 'gamma': 1e-06, 'kernel': 'poly'}   \n",
       "split0_test_score                                      -0.075209   \n",
       "split1_test_score                                     -0.0234201   \n",
       "split2_test_score                                     -0.0403479   \n",
       "split3_test_score                                     -0.0836479   \n",
       "split4_test_score                                     -0.0217349   \n",
       "mean_test_score                                        -0.048872   \n",
       "std_test_score                                         0.0259225   \n",
       "rank_test_score                                               63   \n",
       "split0_train_score                                    -0.0462416   \n",
       "split1_train_score                                    -0.0384141   \n",
       "split2_train_score                                    -0.0455012   \n",
       "split3_train_score                                    -0.0381683   \n",
       "split4_train_score                                    -0.0483199   \n",
       "mean_train_score                                       -0.043329   \n",
       "std_train_score                                        0.0042166   \n",
       "\n",
       "                                                             37  \\\n",
       "mean_fit_time                                         0.0220674   \n",
       "std_fit_time                                         0.00500108   \n",
       "mean_score_time                                      0.00321503   \n",
       "std_score_time                                       0.00100995   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-06   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-06, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.075209   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836479   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                       -0.048872   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              64   \n",
       "split0_train_score                                   -0.0462416   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455012   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483199   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                             21  \\\n",
       "mean_fit_time                                         0.0189825   \n",
       "std_fit_time                                         0.00303913   \n",
       "mean_score_time                                      0.00458407   \n",
       "std_score_time                                       0.00158298   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-06   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-06, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.075209   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836479   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                       -0.048872   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              64   \n",
       "split0_train_score                                   -0.0462416   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455012   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483199   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                            5   \\\n",
       "mean_fit_time                                        0.0134002   \n",
       "std_fit_time                                         0.0023538   \n",
       "mean_score_time                                     0.00284572   \n",
       "std_score_time                                     0.000573448   \n",
       "param_C                                                      1   \n",
       "param_gamma                                              1e-06   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 1, 'gamma': 1e-06, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.075209   \n",
       "split1_test_score                                   -0.0234201   \n",
       "split2_test_score                                   -0.0403479   \n",
       "split3_test_score                                   -0.0836479   \n",
       "split4_test_score                                   -0.0217349   \n",
       "mean_test_score                                      -0.048872   \n",
       "std_test_score                                       0.0259225   \n",
       "rank_test_score                                             66   \n",
       "split0_train_score                                  -0.0462416   \n",
       "split1_train_score                                  -0.0384141   \n",
       "split2_train_score                                  -0.0455012   \n",
       "split3_train_score                                  -0.0381683   \n",
       "split4_train_score                                  -0.0483199   \n",
       "mean_train_score                                     -0.043329   \n",
       "std_train_score                                      0.0042166   \n",
       "\n",
       "                                                               67  \\\n",
       "mean_fit_time                                           0.0169482   \n",
       "std_fit_time                                           0.00175601   \n",
       "mean_score_time                                        0.00246038   \n",
       "std_score_time                                        0.000937148   \n",
       "param_C                                                      1000   \n",
       "param_gamma                                                 1e-07   \n",
       "param_kernel                                                 poly   \n",
       "params              {'C': 1000, 'gamma': 1e-07, 'kernel': 'poly'}   \n",
       "split0_test_score                                       -0.075209   \n",
       "split1_test_score                                      -0.0234201   \n",
       "split2_test_score                                      -0.0403479   \n",
       "split3_test_score                                      -0.0836479   \n",
       "split4_test_score                                      -0.0217349   \n",
       "mean_test_score                                         -0.048872   \n",
       "std_test_score                                          0.0259225   \n",
       "rank_test_score                                                66   \n",
       "split0_train_score                                     -0.0462416   \n",
       "split1_train_score                                     -0.0384141   \n",
       "split2_train_score                                     -0.0455012   \n",
       "split3_train_score                                     -0.0381683   \n",
       "split4_train_score                                     -0.0483199   \n",
       "mean_train_score                                        -0.043329   \n",
       "std_train_score                                         0.0042166   \n",
       "\n",
       "                                                              51  \\\n",
       "mean_fit_time                                         0.00853281   \n",
       "std_fit_time                                         0.000307311   \n",
       "mean_score_time                                       0.00169711   \n",
       "std_score_time                                       2.93986e-05   \n",
       "param_C                                                      100   \n",
       "param_gamma                                                1e-07   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 100, 'gamma': 1e-07, 'kernel': 'poly'}   \n",
       "split0_test_score                                      -0.075209   \n",
       "split1_test_score                                     -0.0234201   \n",
       "split2_test_score                                     -0.0403479   \n",
       "split3_test_score                                     -0.0836479   \n",
       "split4_test_score                                     -0.0217349   \n",
       "mean_test_score                                        -0.048872   \n",
       "std_test_score                                         0.0259225   \n",
       "rank_test_score                                               68   \n",
       "split0_train_score                                    -0.0462416   \n",
       "split1_train_score                                    -0.0384141   \n",
       "split2_train_score                                    -0.0455012   \n",
       "split3_train_score                                    -0.0381683   \n",
       "split4_train_score                                    -0.0483199   \n",
       "mean_train_score                                       -0.043329   \n",
       "std_train_score                                        0.0042166   \n",
       "\n",
       "                                                             35  \\\n",
       "mean_fit_time                                          0.027524   \n",
       "std_fit_time                                         0.00475928   \n",
       "mean_score_time                                      0.00583396   \n",
       "std_score_time                                       0.00243674   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-07   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-07, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.075209   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836479   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                       -0.048872   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              69   \n",
       "split0_train_score                                   -0.0462416   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455012   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483199   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                             19  \\\n",
       "mean_fit_time                                         0.0129597   \n",
       "std_fit_time                                          0.0037289   \n",
       "mean_score_time                                      0.00396256   \n",
       "std_score_time                                        0.0026964   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-07   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-07, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.075209   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836479   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                       -0.048872   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              69   \n",
       "split0_train_score                                   -0.0462416   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455012   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483199   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                               65  \\\n",
       "mean_fit_time                                            0.015618   \n",
       "std_fit_time                                           0.00256799   \n",
       "mean_score_time                                        0.00421405   \n",
       "std_score_time                                         0.00289348   \n",
       "param_C                                                      1000   \n",
       "param_gamma                                                 1e-08   \n",
       "param_kernel                                                 poly   \n",
       "params              {'C': 1000, 'gamma': 1e-08, 'kernel': 'poly'}   \n",
       "split0_test_score                                       -0.075209   \n",
       "split1_test_score                                      -0.0234201   \n",
       "split2_test_score                                      -0.0403479   \n",
       "split3_test_score                                      -0.0836479   \n",
       "split4_test_score                                      -0.0217349   \n",
       "mean_test_score                                         -0.048872   \n",
       "std_test_score                                          0.0259225   \n",
       "rank_test_score                                                71   \n",
       "split0_train_score                                     -0.0462416   \n",
       "split1_train_score                                     -0.0384141   \n",
       "split2_train_score                                     -0.0455012   \n",
       "split3_train_score                                     -0.0381683   \n",
       "split4_train_score                                     -0.0483199   \n",
       "mean_train_score                                        -0.043329   \n",
       "std_train_score                                         0.0042166   \n",
       "\n",
       "                                                            3   \\\n",
       "mean_fit_time                                       0.00894423   \n",
       "std_fit_time                                        0.00100437   \n",
       "mean_score_time                                     0.00166492   \n",
       "std_score_time                                     9.85187e-05   \n",
       "param_C                                                      1   \n",
       "param_gamma                                              1e-07   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 1, 'gamma': 1e-07, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.075209   \n",
       "split1_test_score                                   -0.0234201   \n",
       "split2_test_score                                   -0.0403479   \n",
       "split3_test_score                                   -0.0836479   \n",
       "split4_test_score                                   -0.0217349   \n",
       "mean_test_score                                      -0.048872   \n",
       "std_test_score                                       0.0259225   \n",
       "rank_test_score                                             71   \n",
       "split0_train_score                                  -0.0462416   \n",
       "split1_train_score                                  -0.0384141   \n",
       "split2_train_score                                  -0.0455012   \n",
       "split3_train_score                                  -0.0381683   \n",
       "split4_train_score                                  -0.0483199   \n",
       "mean_train_score                                     -0.043329   \n",
       "std_train_score                                      0.0042166   \n",
       "\n",
       "                                                              49  \\\n",
       "mean_fit_time                                          0.0087657   \n",
       "std_fit_time                                         0.000227297   \n",
       "mean_score_time                                       0.00182614   \n",
       "std_score_time                                       0.000109903   \n",
       "param_C                                                      100   \n",
       "param_gamma                                                1e-08   \n",
       "param_kernel                                                poly   \n",
       "params              {'C': 100, 'gamma': 1e-08, 'kernel': 'poly'}   \n",
       "split0_test_score                                      -0.075209   \n",
       "split1_test_score                                     -0.0234201   \n",
       "split2_test_score                                     -0.0403479   \n",
       "split3_test_score                                     -0.0836479   \n",
       "split4_test_score                                     -0.0217349   \n",
       "mean_test_score                                        -0.048872   \n",
       "std_test_score                                         0.0259225   \n",
       "rank_test_score                                               73   \n",
       "split0_train_score                                    -0.0462416   \n",
       "split1_train_score                                    -0.0384141   \n",
       "split2_train_score                                    -0.0455012   \n",
       "split3_train_score                                    -0.0381683   \n",
       "split4_train_score                                    -0.0483199   \n",
       "mean_train_score                                       -0.043329   \n",
       "std_train_score                                        0.0042166   \n",
       "\n",
       "                                                            1   \\\n",
       "mean_fit_time                                        0.0109226   \n",
       "std_fit_time                                        0.00251205   \n",
       "mean_score_time                                     0.00254335   \n",
       "std_score_time                                     0.000997073   \n",
       "param_C                                                      1   \n",
       "param_gamma                                              1e-08   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 1, 'gamma': 1e-08, 'kernel': 'poly'}   \n",
       "split0_test_score                                    -0.075209   \n",
       "split1_test_score                                   -0.0234201   \n",
       "split2_test_score                                   -0.0403479   \n",
       "split3_test_score                                   -0.0836479   \n",
       "split4_test_score                                   -0.0217349   \n",
       "mean_test_score                                      -0.048872   \n",
       "std_test_score                                       0.0259225   \n",
       "rank_test_score                                             74   \n",
       "split0_train_score                                  -0.0462416   \n",
       "split1_train_score                                  -0.0384141   \n",
       "split2_train_score                                  -0.0455012   \n",
       "split3_train_score                                  -0.0381683   \n",
       "split4_train_score                                  -0.0483199   \n",
       "mean_train_score                                     -0.043329   \n",
       "std_train_score                                      0.0042166   \n",
       "\n",
       "                                                             17  \\\n",
       "mean_fit_time                                         0.0207184   \n",
       "std_fit_time                                         0.00228721   \n",
       "mean_score_time                                      0.00389619   \n",
       "std_score_time                                       0.00197418   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-08   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-08, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.075209   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836479   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                       -0.048872   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              74   \n",
       "split0_train_score                                   -0.0462416   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455012   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483199   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                             33  \\\n",
       "mean_fit_time                                         0.0249092   \n",
       "std_fit_time                                          0.0095417   \n",
       "mean_score_time                                      0.00384159   \n",
       "std_score_time                                       0.00240142   \n",
       "param_C                                                      10   \n",
       "param_gamma                                               1e-08   \n",
       "param_kernel                                               poly   \n",
       "params              {'C': 10, 'gamma': 1e-08, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -0.075209   \n",
       "split1_test_score                                    -0.0234201   \n",
       "split2_test_score                                    -0.0403479   \n",
       "split3_test_score                                    -0.0836479   \n",
       "split4_test_score                                    -0.0217349   \n",
       "mean_test_score                                       -0.048872   \n",
       "std_test_score                                        0.0259225   \n",
       "rank_test_score                                              74   \n",
       "split0_train_score                                   -0.0462416   \n",
       "split1_train_score                                   -0.0384141   \n",
       "split2_train_score                                   -0.0455012   \n",
       "split3_train_score                                   -0.0381683   \n",
       "split4_train_score                                   -0.0483199   \n",
       "mean_train_score                                      -0.043329   \n",
       "std_train_score                                       0.0042166   \n",
       "\n",
       "                                                           31  \\\n",
       "mean_fit_time                                        0.280743   \n",
       "std_fit_time                                         0.102452   \n",
       "mean_score_time                                    0.00431318   \n",
       "std_score_time                                     0.00308807   \n",
       "param_C                                                    10   \n",
       "param_gamma                                               0.1   \n",
       "param_kernel                                             poly   \n",
       "params              {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}   \n",
       "split0_test_score                                   -0.626207   \n",
       "split1_test_score                                   -0.497896   \n",
       "split2_test_score                                    -8.22031   \n",
       "split3_test_score                                   -0.412972   \n",
       "split4_test_score                                     -4.3544   \n",
       "mean_test_score                                      -2.82236   \n",
       "std_test_score                                        3.08274   \n",
       "rank_test_score                                            77   \n",
       "split0_train_score                                   0.971642   \n",
       "split1_train_score                                   0.981128   \n",
       "split2_train_score                                   0.983774   \n",
       "split3_train_score                                    0.97268   \n",
       "split4_train_score                                   0.980873   \n",
       "mean_train_score                                     0.978019   \n",
       "std_train_score                                    0.00490101   \n",
       "\n",
       "                                                           47  \\\n",
       "mean_fit_time                                        0.275873   \n",
       "std_fit_time                                        0.0801648   \n",
       "mean_score_time                                    0.00211086   \n",
       "std_score_time                                    0.000721526   \n",
       "param_C                                                    10   \n",
       "param_gamma                                               0.1   \n",
       "param_kernel                                             poly   \n",
       "params              {'C': 10, 'gamma': 0.1, 'kernel': 'poly'}   \n",
       "split0_test_score                                   -0.626207   \n",
       "split1_test_score                                   -0.497896   \n",
       "split2_test_score                                    -8.22031   \n",
       "split3_test_score                                   -0.412972   \n",
       "split4_test_score                                     -4.3544   \n",
       "mean_test_score                                      -2.82236   \n",
       "std_test_score                                        3.08274   \n",
       "rank_test_score                                            77   \n",
       "split0_train_score                                   0.971642   \n",
       "split1_train_score                                   0.981128   \n",
       "split2_train_score                                   0.983774   \n",
       "split3_train_score                                    0.97268   \n",
       "split4_train_score                                   0.980873   \n",
       "mean_train_score                                     0.978019   \n",
       "std_train_score                                    0.00490101   \n",
       "\n",
       "                                                            63  \\\n",
       "mean_fit_time                                         0.512994   \n",
       "std_fit_time                                          0.126405   \n",
       "mean_score_time                                     0.00210228   \n",
       "std_score_time                                     0.000491628   \n",
       "param_C                                                    100   \n",
       "param_gamma                                                0.1   \n",
       "param_kernel                                              poly   \n",
       "params              {'C': 100, 'gamma': 0.1, 'kernel': 'poly'}   \n",
       "split0_test_score                                     -5.51296   \n",
       "split1_test_score                                     -2.51479   \n",
       "split2_test_score                                     -6.71273   \n",
       "split3_test_score                                     -2.72908   \n",
       "split4_test_score                                     -3.62766   \n",
       "mean_test_score                                       -4.21945   \n",
       "std_test_score                                         1.63505   \n",
       "rank_test_score                                             79   \n",
       "split0_train_score                                    0.988171   \n",
       "split1_train_score                                    0.990888   \n",
       "split2_train_score                                    0.991283   \n",
       "split3_train_score                                      0.9903   \n",
       "split4_train_score                                    0.988829   \n",
       "mean_train_score                                      0.989894   \n",
       "std_train_score                                     0.00119878   \n",
       "\n",
       "                                                             79  \n",
       "mean_fit_time                                          0.620296  \n",
       "std_fit_time                                           0.184337  \n",
       "mean_score_time                                      0.00184264  \n",
       "std_score_time                                      0.000258118  \n",
       "param_C                                                    1000  \n",
       "param_gamma                                                 0.1  \n",
       "param_kernel                                               poly  \n",
       "params              {'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}  \n",
       "split0_test_score                                      -9.85748  \n",
       "split1_test_score                                      -3.24265  \n",
       "split2_test_score                                      -27.8004  \n",
       "split3_test_score                                      -4.68435  \n",
       "split4_test_score                                      -11.7236  \n",
       "mean_test_score                                        -11.4617  \n",
       "std_test_score                                          8.75307  \n",
       "rank_test_score                                              80  \n",
       "split0_train_score                                     0.990006  \n",
       "split1_train_score                                     0.991679  \n",
       "split2_train_score                                      0.99167  \n",
       "split3_train_score                                     0.991217  \n",
       "split4_train_score                                     0.989049  \n",
       "mean_train_score                                       0.990724  \n",
       "std_train_score                                      0.00103631  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search_cv.cv_results_ ).sort_values('mean_test_score', ascending = False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for Decision trees\n",
    "\n",
    "#from the lecture notes...\n",
    "def draw_tree(estimator, figsize =(15, 5), feature_names = [\"Hits\", \"Years\"]):\n",
    "    \"\"\"\n",
    "    Takes a decision  tree estimator and plots it's tree structure\n",
    "    :param estimator: A sklearn decision tree estimator. Should be fitted.\n",
    "    :param figsize: tuple. (int, int).\n",
    "    :param feature_names:\n",
    "    :return: It returns a plot. The image is not saved.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize= figsize)\n",
    "    _ = tree.plot_tree(estimator,\n",
    "                       feature_names= feature_names,\n",
    "                       filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpdForDecisionTree = dfCache.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>MLSNumber</th>\n",
       "      <th>DOM</th>\n",
       "      <th>CDOM</th>\n",
       "      <th>ListDate</th>\n",
       "      <th>AgreementOfSaleSignedLeaseDate</th>\n",
       "      <th>OffMarketDate</th>\n",
       "      <th>SettledDate</th>\n",
       "      <th>OriginalPrice</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>SoldPrice</th>\n",
       "      <th>StreetNumber</th>\n",
       "      <th>StreetDirection</th>\n",
       "      <th>StreetName</th>\n",
       "      <th>UnitNumber</th>\n",
       "      <th>City</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>County</th>\n",
       "      <th>Subdivision</th>\n",
       "      <th>ListAgentName</th>\n",
       "      <th>ListAgentCode</th>\n",
       "      <th>ListOfficeName</th>\n",
       "      <th>ListOfficeCode</th>\n",
       "      <th>SellingAgent</th>\n",
       "      <th>SellingAgentCode</th>\n",
       "      <th>SellingOfficeName</th>\n",
       "      <th>SellingOfficeCode</th>\n",
       "      <th>SellerConcessionsAmount</th>\n",
       "      <th>FinalFinancing</th>\n",
       "      <th>FinalShortSale</th>\n",
       "      <th>FinalThirdPartyApproval</th>\n",
       "      <th>FinalBankOwned</th>\n",
       "      <th>TaxAnnualTotal</th>\n",
       "      <th>TaxYear</th>\n",
       "      <th>AcresTotal</th>\n",
       "      <th>LandUseCode</th>\n",
       "      <th>Ownership</th>\n",
       "      <th>SeniorCommunity</th>\n",
       "      <th>CondoCoopAssoc</th>\n",
       "      <th>HOA</th>\n",
       "      <th>OneTimeAssociationFee</th>\n",
       "      <th>AssociationFee</th>\n",
       "      <th>AssociationFeeFrequency</th>\n",
       "      <th>Age</th>\n",
       "      <th>InteriorSqFt</th>\n",
       "      <th>PropertyCondition</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>BathsFull</th>\n",
       "      <th>BathsHalf</th>\n",
       "      <th>Design</th>\n",
       "      <th>Style</th>\n",
       "      <th>NumberofStories</th>\n",
       "      <th>FloorNumber</th>\n",
       "      <th>Basement</th>\n",
       "      <th>GarageSpaces</th>\n",
       "      <th>Fireplace</th>\n",
       "      <th>Laundry</th>\n",
       "      <th>OtherRooms</th>\n",
       "      <th>RoomCount</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Waterfront</th>\n",
       "      <th>NewConstruction</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>BuyerBrokerCompensation</th>\n",
       "      <th>SubAgentCompensation</th>\n",
       "      <th>TransactionBrokerCompensation</th>\n",
       "      <th>OriginatingMLS</th>\n",
       "      <th>AboveGradeSqFt</th>\n",
       "      <th>BelowGradeSqFt</th>\n",
       "      <th>HomeBuilt</th>\n",
       "      <th>BasementFootprintPct</th>\n",
       "      <th>BasementFinishedPct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MDBC2022914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1/14/22</td>\n",
       "      <td>5/10/21 0:00</td>\n",
       "      <td>1/14/22</td>\n",
       "      <td>6/11/21</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>275000</td>\n",
       "      <td>275000</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Stonewall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>21228</td>\n",
       "      <td>BALTIMOREMD</td>\n",
       "      <td>NONE AVAILABLE</td>\n",
       "      <td>Non Member</td>\n",
       "      <td>12345</td>\n",
       "      <td>Non Subscribing Office</td>\n",
       "      <td>NON1</td>\n",
       "      <td>Rachel B Sturm</td>\n",
       "      <td>103717.0</td>\n",
       "      <td>Keller Williams Realty Centre</td>\n",
       "      <td>KWR15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>3809.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>FeeSimple</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two</td>\n",
       "      <td>Rancher</td>\n",
       "      <td>Main</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRIGHT</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MDBC2021930</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1/7/22</td>\n",
       "      <td>3/25/21 0:00</td>\n",
       "      <td>1/7/22</td>\n",
       "      <td>4/23/21</td>\n",
       "      <td>410000.0</td>\n",
       "      <td>410000</td>\n",
       "      <td>410000</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Wells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>21219</td>\n",
       "      <td>BALTIMOREMD</td>\n",
       "      <td>EDGEMERE/SPARROWS POINT</td>\n",
       "      <td>Non Member</td>\n",
       "      <td>12345</td>\n",
       "      <td>Non Subscribing Office</td>\n",
       "      <td>NON1</td>\n",
       "      <td>Christa R Barkley</td>\n",
       "      <td>25193.0</td>\n",
       "      <td>RE/MAX First Choice</td>\n",
       "      <td>RXFC1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.0</td>\n",
       "      <td>FeeSimple</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two</td>\n",
       "      <td>Colonial</td>\n",
       "      <td>Main</td>\n",
       "      <td>None</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRIGHT</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MDBC2021520</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>1/4/22</td>\n",
       "      <td>1/4/22 0:00</td>\n",
       "      <td>1/4/22</td>\n",
       "      <td>2/26/21</td>\n",
       "      <td>737724.0</td>\n",
       "      <td>737724</td>\n",
       "      <td>737724</td>\n",
       "      <td>607.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Somerstown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Middle River</td>\n",
       "      <td>21220</td>\n",
       "      <td>BALTIMOREMD</td>\n",
       "      <td>GREENLEIGH AT CROSSROADS</td>\n",
       "      <td>Non Member</td>\n",
       "      <td>12345</td>\n",
       "      <td>Non Subscribing Office</td>\n",
       "      <td>NON1</td>\n",
       "      <td>Charlie Hatter</td>\n",
       "      <td>3090060.0</td>\n",
       "      <td>Monument Sotheby's International Realty</td>\n",
       "      <td>MONUM1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FeeSimple</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>105.0</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Three</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Lower1,Main,Upper1</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Hookup,UpprFlrLndry</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MOUNT VERNON</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRIGHT</td>\n",
       "      <td>3282.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MDBC2021428</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1/4/22</td>\n",
       "      <td>3/12/21 0:00</td>\n",
       "      <td>1/4/22</td>\n",
       "      <td>4/27/21</td>\n",
       "      <td>117800.0</td>\n",
       "      <td>117800</td>\n",
       "      <td>117800</td>\n",
       "      <td>1325.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Old Eastern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>21221</td>\n",
       "      <td>BALTIMOREMD</td>\n",
       "      <td>EDGEWOOD PARK</td>\n",
       "      <td>Non Member</td>\n",
       "      <td>12345</td>\n",
       "      <td>Non Subscribing Office</td>\n",
       "      <td>NON1</td>\n",
       "      <td>Adetoun Olunlade</td>\n",
       "      <td>3013617.0</td>\n",
       "      <td>Samson Properties</td>\n",
       "      <td>SAMP16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>BankOwnedREO</td>\n",
       "      <td>BankOwnedREO</td>\n",
       "      <td>BankOwnedREO</td>\n",
       "      <td>2593.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.0</td>\n",
       "      <td>FeeSimple</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Three</td>\n",
       "      <td>CapeCod</td>\n",
       "      <td>Lower1,Main,Upper1</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRIGHT</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MDBC2012350</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>9/23/21</td>\n",
       "      <td>4/1/21 0:00</td>\n",
       "      <td>9/23/21</td>\n",
       "      <td>5/26/21</td>\n",
       "      <td>1097700.0</td>\n",
       "      <td>1097700</td>\n",
       "      <td>1097700</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Crestline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Owings Mills</td>\n",
       "      <td>21117</td>\n",
       "      <td>BALTIMOREMD</td>\n",
       "      <td>GREENSPRING VALLEY</td>\n",
       "      <td>Non Member</td>\n",
       "      <td>12345</td>\n",
       "      <td>Non Subscribing Office</td>\n",
       "      <td>NON1</td>\n",
       "      <td>Stephen F Edelen</td>\n",
       "      <td>26426.0</td>\n",
       "      <td>Long &amp; Foster Real Estate, Inc.</td>\n",
       "      <td>LNG106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>StandardSale</td>\n",
       "      <td>8290.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>3.79</td>\n",
       "      <td>10.0</td>\n",
       "      <td>FeeSimple</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>3480.0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Two</td>\n",
       "      <td>Colonial</td>\n",
       "      <td>Main,Upper1</td>\n",
       "      <td>None</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MainFlrLndry</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRIGHT</td>\n",
       "      <td>3480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    MLSNumber  DOM  CDOM ListDate AgreementOfSaleSignedLeaseDate  \\\n",
       "0      0  MDBC2022914    0     0  1/14/22                   5/10/21 0:00   \n",
       "1      1  MDBC2021930    0     0   1/7/22                   3/25/21 0:00   \n",
       "2      2  MDBC2021520    0   206   1/4/22                    1/4/22 0:00   \n",
       "3      3  MDBC2021428    0    51   1/4/22                   3/12/21 0:00   \n",
       "4      4  MDBC2012350    0    68  9/23/21                    4/1/21 0:00   \n",
       "\n",
       "  OffMarketDate SettledDate  OriginalPrice  ListPrice  SoldPrice  \\\n",
       "0       1/14/22     6/11/21       275000.0     275000     275000   \n",
       "1        1/7/22     4/23/21       410000.0     410000     410000   \n",
       "2        1/4/22     2/26/21       737724.0     737724     737724   \n",
       "3        1/4/22     4/27/21       117800.0     117800     117800   \n",
       "4       9/23/21     5/26/21      1097700.0    1097700    1097700   \n",
       "\n",
       "   StreetNumber StreetDirection   StreetName  UnitNumber          City  \\\n",
       "0        2405.0            None    Stonewall         NaN     Baltimore   \n",
       "1        2810.0            None        Wells         NaN     Baltimore   \n",
       "2         607.0            None   Somerstown         NaN  Middle River   \n",
       "3        1325.0            None  Old Eastern         NaN     Baltimore   \n",
       "4           9.0            None    Crestline         NaN  Owings Mills   \n",
       "\n",
       "   ZipCode       County               Subdivision ListAgentName  \\\n",
       "0    21228  BALTIMOREMD            NONE AVAILABLE    Non Member   \n",
       "1    21219  BALTIMOREMD   EDGEMERE/SPARROWS POINT    Non Member   \n",
       "2    21220  BALTIMOREMD  GREENLEIGH AT CROSSROADS    Non Member   \n",
       "3    21221  BALTIMOREMD             EDGEWOOD PARK    Non Member   \n",
       "4    21117  BALTIMOREMD        GREENSPRING VALLEY    Non Member   \n",
       "\n",
       "   ListAgentCode          ListOfficeName ListOfficeCode       SellingAgent  \\\n",
       "0          12345  Non Subscribing Office           NON1     Rachel B Sturm   \n",
       "1          12345  Non Subscribing Office           NON1  Christa R Barkley   \n",
       "2          12345  Non Subscribing Office           NON1     Charlie Hatter   \n",
       "3          12345  Non Subscribing Office           NON1   Adetoun Olunlade   \n",
       "4          12345  Non Subscribing Office           NON1   Stephen F Edelen   \n",
       "\n",
       "   SellingAgentCode                        SellingOfficeName  \\\n",
       "0          103717.0            Keller Williams Realty Centre   \n",
       "1           25193.0                      RE/MAX First Choice   \n",
       "2         3090060.0  Monument Sotheby's International Realty   \n",
       "3         3013617.0                        Samson Properties   \n",
       "4           26426.0          Long & Foster Real Estate, Inc.   \n",
       "\n",
       "  SellingOfficeCode  SellerConcessionsAmount FinalFinancing FinalShortSale  \\\n",
       "0             KWR15                      NaN   Conventional   StandardSale   \n",
       "1             RXFC1                      NaN   Conventional   StandardSale   \n",
       "2            MONUM1                  10000.0   Conventional   StandardSale   \n",
       "3            SAMP16                      NaN          Other   BankOwnedREO   \n",
       "4            LNG106                      NaN           Cash   StandardSale   \n",
       "\n",
       "  FinalThirdPartyApproval FinalBankOwned  TaxAnnualTotal  TaxYear  AcresTotal  \\\n",
       "0            StandardSale   StandardSale          3809.0   2020.0        0.49   \n",
       "1            StandardSale   StandardSale          2066.0   2020.0        0.17   \n",
       "2            StandardSale   StandardSale             0.0   2021.0        0.17   \n",
       "3            BankOwnedREO   BankOwnedREO          2593.0   2020.0        0.17   \n",
       "4            StandardSale   StandardSale          8290.0   2021.0        3.79   \n",
       "\n",
       "   LandUseCode  Ownership SeniorCommunity CondoCoopAssoc  HOA  \\\n",
       "0         10.0  FeeSimple              No             No   No   \n",
       "1         10.0  FeeSimple              No             No   No   \n",
       "2          NaN  FeeSimple              No             No  Yes   \n",
       "3         10.0  FeeSimple              No             No   No   \n",
       "4         10.0  FeeSimple              No             No   No   \n",
       "\n",
       "  OneTimeAssociationFee  AssociationFee AssociationFeeFrequency     Age  \\\n",
       "0                  None             NaN                    None  1961.0   \n",
       "1                  None             NaN                    None  1942.0   \n",
       "2               Monthly           105.0                 Monthly  2022.0   \n",
       "3                  None             NaN                    None  1939.0   \n",
       "4                  None             NaN                    None  1967.0   \n",
       "\n",
       "   InteriorSqFt PropertyCondition  Bedrooms  BathsFull  BathsHalf Design  \\\n",
       "0        2532.0              None       3.0        2.0        NaN    Two   \n",
       "1        1089.0         Excellent       4.0        2.0        NaN    Two   \n",
       "2        4254.0         Excellent       5.0        4.0        1.0  Three   \n",
       "3        1453.0              None       4.0        2.0        1.0  Three   \n",
       "4        3480.0              None       5.0        4.0        1.0    Two   \n",
       "\n",
       "         Style     NumberofStories FloorNumber Basement  GarageSpaces  \\\n",
       "0      Rancher                Main        None      Yes           NaN   \n",
       "1     Colonial                Main        None       No           NaN   \n",
       "2  Traditional  Lower1,Main,Upper1        None      Yes           2.0   \n",
       "3      CapeCod  Lower1,Main,Upper1        None      Yes           1.0   \n",
       "4     Colonial         Main,Upper1        None      Yes           3.0   \n",
       "\n",
       "  Fireplace              Laundry OtherRooms  RoomCount CentralAir Waterfront  \\\n",
       "0      None                 None       None        NaN        Yes         No   \n",
       "1      None                 None       None        NaN        Yes         No   \n",
       "2       Yes  Hookup,UpprFlrLndry       None        NaN        Yes         No   \n",
       "3      None                 None       None        NaN        Yes         No   \n",
       "4       Yes         MainFlrLndry       None        NaN        Yes         No   \n",
       "\n",
       "  NewConstruction     ModelName  BuyerBrokerCompensation  \\\n",
       "0              No          None                     2.50   \n",
       "1             Yes          None                     2.25   \n",
       "2             Yes  MOUNT VERNON                     2.00   \n",
       "3              No          None                     1.50   \n",
       "4              No          None                     2.50   \n",
       "\n",
       "   SubAgentCompensation  TransactionBrokerCompensation OriginatingMLS  \\\n",
       "0                  0.00                            NaN         BRIGHT   \n",
       "1                  2.25                            NaN         BRIGHT   \n",
       "2                  2.00                            NaN         BRIGHT   \n",
       "3                  0.00                            NaN         BRIGHT   \n",
       "4                  0.00                            NaN         BRIGHT   \n",
       "\n",
       "   AboveGradeSqFt  BelowGradeSqFt  HomeBuilt  BasementFootprintPct  \\\n",
       "0          1316.0          1216.0       None                   NaN   \n",
       "1          1089.0             0.0  Excellent                   NaN   \n",
       "2          3282.0           972.0  Excellent                   NaN   \n",
       "3          1453.0             0.0       None                   NaN   \n",
       "4          3480.0             0.0       None                   NaN   \n",
       "\n",
       "   BasementFinishedPct  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpdForDecisionTree.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X = dfpdForDecisionTree.drop(['index', 'MLSNumber', 'DOM', 'CDOM', 'AgreementOfSaleSignedLeaseDate', 'OriginalPrice', 'ListPrice', 'UnitNumber', 'ListAgentName', 'ListAgentCode', 'ListOfficeName', 'ListOfficeCode','SellingAgent', 'SellingAgentCode', 'SellingOfficeName', 'SellingOfficeCode', 'SellerConcessionsAmount', 'LandUseCode', 'Ownership', 'BasementFootprintPct', 'BasementFinishedPct', 'OriginatingMLS'], axis=1)\n",
    "X = dfpdForLinearRegression.drop(['SoldPrice'], axis=1)\n",
    "y = dfpdForLinearRegression['SoldPrice']\n",
    "#y = dfpdForDecisionTree['SoldPrice']\n",
    "#print(X.head(2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_leaf_nodes= 10)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "draw_tree(regressor,feature_names= X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0 ..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0, total=   3.5s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0 ..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0 ..............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0, total=   3.5s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0 ..............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0 ..............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0, total=   3.7s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5, total=   3.6s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5 ............\n",
      "[CV]  ccp_alpha=0, max_depth=15, min_impurity_decrease=0.5, total=   3.4s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0 .............\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0, total=   3.3s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0 .............\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0 .............\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0 .............\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0 .............\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1, total=   3.3s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.1, total=   3.4s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5, total=   3.3s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5, total=   3.4s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5, total=   3.2s\n",
      "[CV] ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5 ...........\n",
      "[CV]  ccp_alpha=0, max_depth=100, min_impurity_decrease=0.5, total=   4.3s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0 ............\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0, total=   4.6s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0 ............\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0, total=   4.7s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0 ............\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0, total=   4.2s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0 ............\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0, total=   4.0s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0 ............\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0, total=   3.9s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1, total=   3.6s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1, total=   3.8s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1, total=   3.4s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1, total=   4.9s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.1, total=   4.3s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5, total=   3.3s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5, total=   5.2s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5, total=   4.8s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5, total=   3.2s\n",
      "[CV] ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5 ..........\n",
      "[CV]  ccp_alpha=0.1, max_depth=15, min_impurity_decrease=0.5, total=   3.3s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0 ...........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0, total=   4.4s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0 ...........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0 ...........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0, total=   3.2s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0 ...........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0, total=   4.8s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0 ...........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0, total=   3.5s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1, total=   4.4s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1, total=   3.4s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1, total=   5.5s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1, total=   3.5s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.1, total=   4.7s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5, total=   6.4s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5, total=   6.8s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5, total=   3.8s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5 .........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5, total=   4.4s\n",
      "[CV] ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5 .........\n",
      "[CV]  ccp_alpha=0.1, max_depth=100, min_impurity_decrease=0.5, total=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.37432127, 3.14748573, 3.25712247, 3.16290932, 3.21338582,\n",
       "        3.4446312 , 4.19137979, 3.93824401, 3.89957976, 3.78647423,\n",
       "        4.20914063, 4.8855134 ]),\n",
       " 'std_fit_time': array([0.17724843, 0.02603893, 0.14519967, 0.03361578, 0.0802968 ,\n",
       "        0.42955183, 0.3389276 , 0.52202993, 0.82802583, 0.63343007,\n",
       "        0.77128674, 1.35411177]),\n",
       " 'mean_score_time': array([0.06055155, 0.05194845, 0.05257783, 0.05217395, 0.05306749,\n",
       "        0.05335355, 0.06955819, 0.05949669, 0.0629024 , 0.05311255,\n",
       "        0.08197436, 0.07404566]),\n",
       " 'std_score_time': array([0.01329005, 0.00069695, 0.00145225, 0.00038427, 0.00041501,\n",
       "        0.00054646, 0.02088521, 0.00758964, 0.01724709, 0.00081396,\n",
       "        0.04307133, 0.01849769]),\n",
       " 'param_ccp_alpha': masked_array(data=[0, 0, 0, 0, 0, 0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[15, 15, 15, 100, 100, 100, 15, 15, 15, 100, 100, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_impurity_decrease': masked_array(data=[0, 0.1, 0.5, 0, 0.1, 0.5, 0, 0.1, 0.5, 0, 0.1, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'ccp_alpha': 0, 'max_depth': 15, 'min_impurity_decrease': 0},\n",
       "  {'ccp_alpha': 0, 'max_depth': 15, 'min_impurity_decrease': 0.1},\n",
       "  {'ccp_alpha': 0, 'max_depth': 15, 'min_impurity_decrease': 0.5},\n",
       "  {'ccp_alpha': 0, 'max_depth': 100, 'min_impurity_decrease': 0},\n",
       "  {'ccp_alpha': 0, 'max_depth': 100, 'min_impurity_decrease': 0.1},\n",
       "  {'ccp_alpha': 0, 'max_depth': 100, 'min_impurity_decrease': 0.5},\n",
       "  {'ccp_alpha': 0.1, 'max_depth': 15, 'min_impurity_decrease': 0},\n",
       "  {'ccp_alpha': 0.1, 'max_depth': 15, 'min_impurity_decrease': 0.1},\n",
       "  {'ccp_alpha': 0.1, 'max_depth': 15, 'min_impurity_decrease': 0.5},\n",
       "  {'ccp_alpha': 0.1, 'max_depth': 100, 'min_impurity_decrease': 0},\n",
       "  {'ccp_alpha': 0.1, 'max_depth': 100, 'min_impurity_decrease': 0.1},\n",
       "  {'ccp_alpha': 0.1, 'max_depth': 100, 'min_impurity_decrease': 0.5}],\n",
       " 'split0_test_score': array([0.71530383, 0.70893883, 0.658905  , 0.67212808, 0.66549789,\n",
       "        0.70349712, 0.65650119, 0.68873328, 0.67854883, 0.66836568,\n",
       "        0.68928387, 0.68593417]),\n",
       " 'split1_test_score': array([0.59596307, 0.64748634, 0.60176224, 0.56294892, 0.59330971,\n",
       "        0.60347185, 0.63381672, 0.59006734, 0.60110387, 0.64299699,\n",
       "        0.59285774, 0.59794641]),\n",
       " 'split2_test_score': array([0.63677074, 0.59363569, 0.60413837, 0.60062905, 0.61874928,\n",
       "        0.58276445, 0.6422445 , 0.62825864, 0.62927873, 0.59349368,\n",
       "        0.62371154, 0.59307419]),\n",
       " 'split3_test_score': array([0.65134559, 0.50910845, 0.66496615, 0.67391768, 0.57737675,\n",
       "        0.64457751, 0.58229241, 0.65756866, 0.50625817, 0.58975386,\n",
       "        0.50830764, 0.67788679]),\n",
       " 'split4_test_score': array([0.65626277, 0.67578159, 0.65006734, 0.66210934, 0.67002626,\n",
       "        0.67412678, 0.64457172, 0.66464126, 0.67254309, 0.65099444,\n",
       "        0.65429293, 0.66619512]),\n",
       " 'mean_test_score': array([0.6511292 , 0.62699018, 0.63596782, 0.63434662, 0.62499198,\n",
       "        0.64168754, 0.63188531, 0.64585384, 0.61754654, 0.62912093,\n",
       "        0.61369074, 0.64420734]),\n",
       " 'std_test_score': array([0.03845582, 0.07003556, 0.02738229, 0.04471819, 0.03735977,\n",
       "        0.04430226, 0.02583558, 0.03391334, 0.06251241, 0.03171818,\n",
       "        0.06164882, 0.04028292]),\n",
       " 'rank_test_score': array([ 1,  9,  5,  6, 10,  4,  7,  2, 11,  8, 12,  3], dtype=int32),\n",
       " 'split0_train_score': array([0.84108016, 0.84108016, 0.84108016, 0.84108016, 0.84108016,\n",
       "        0.84108016, 0.84108016, 0.84108016, 0.84108016, 0.84108016,\n",
       "        0.84108016, 0.84108016]),\n",
       " 'split1_train_score': array([0.84404683, 0.84404683, 0.84404683, 0.84416672, 0.84416672,\n",
       "        0.84416672, 0.84404683, 0.84404683, 0.84404683, 0.84416672,\n",
       "        0.84416672, 0.84416672]),\n",
       " 'split2_train_score': array([0.84589633, 0.84589633, 0.84589633, 0.84603469, 0.84603469,\n",
       "        0.84603469, 0.84589633, 0.84589633, 0.84589633, 0.84603469,\n",
       "        0.84603469, 0.84603469]),\n",
       " 'split3_train_score': array([0.84445207, 0.84445207, 0.84445207, 0.84454123, 0.84454123,\n",
       "        0.84454123, 0.84445207, 0.84445207, 0.84445207, 0.84454123,\n",
       "        0.84454123, 0.84454123]),\n",
       " 'split4_train_score': array([0.84069173, 0.84069173, 0.84069173, 0.84183582, 0.84183582,\n",
       "        0.84183582, 0.84069173, 0.84069173, 0.84069173, 0.84183582,\n",
       "        0.84183582, 0.84183582]),\n",
       " 'mean_train_score': array([0.84323342, 0.84323342, 0.84323342, 0.84353172, 0.84353172,\n",
       "        0.84353172, 0.84323342, 0.84323342, 0.84323342, 0.84353172,\n",
       "        0.84353172, 0.84353172]),\n",
       " 'std_train_score': array([0.00201666, 0.00201666, 0.00201666, 0.00182063, 0.00182063,\n",
       "        0.00182063, 0.00201666, 0.00201666, 0.00201666, 0.00182063,\n",
       "        0.00182063, 0.00182063])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid Search CV\n",
    "#now lets try to find the best parameters for decision trees\n",
    "#this takes a pretty long time to run, so gave faster option below\n",
    "dt = DecisionTreeRegressor(max_leaf_nodes= 300)\n",
    "#grid = {'n_estimators': [50,300,500,700], 'max_leaf_nodes': [5, 15, 25, 100], 'ccp_alpha': [0, 0.01, 0.1, 1, 10], 'criterion' : ['gini', 'entropy'], \"min_impurity_split\":[0,0.1,0.3,0.5], \"n_jobs\": [4]}\n",
    "grid = {'max_depth': [15, 100], 'ccp_alpha': [0, 0.1], \"min_impurity_decrease\":[0,0.1,0.5]}\n",
    "grid_search_cv = GridSearchCV(estimator = dt, param_grid= grid, cv =5, return_train_score= True, verbose= 2)\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "grid_search_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>7</th>\n",
       "      <th>11</th>\n",
       "      <th>5</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>6</th>\n",
       "      <th>9</th>\n",
       "      <th>1</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>3.37432</td>\n",
       "      <td>3.93824</td>\n",
       "      <td>4.88551</td>\n",
       "      <td>3.44463</td>\n",
       "      <td>3.25712</td>\n",
       "      <td>3.16291</td>\n",
       "      <td>4.19138</td>\n",
       "      <td>3.78647</td>\n",
       "      <td>3.14749</td>\n",
       "      <td>3.21339</td>\n",
       "      <td>3.89958</td>\n",
       "      <td>4.20914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.177248</td>\n",
       "      <td>0.52203</td>\n",
       "      <td>1.35411</td>\n",
       "      <td>0.429552</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0336158</td>\n",
       "      <td>0.338928</td>\n",
       "      <td>0.63343</td>\n",
       "      <td>0.0260389</td>\n",
       "      <td>0.0802968</td>\n",
       "      <td>0.828026</td>\n",
       "      <td>0.771287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.0605515</td>\n",
       "      <td>0.0594967</td>\n",
       "      <td>0.0740457</td>\n",
       "      <td>0.0533535</td>\n",
       "      <td>0.0525778</td>\n",
       "      <td>0.0521739</td>\n",
       "      <td>0.0695582</td>\n",
       "      <td>0.0531126</td>\n",
       "      <td>0.0519485</td>\n",
       "      <td>0.0530675</td>\n",
       "      <td>0.0629024</td>\n",
       "      <td>0.0819744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0132901</td>\n",
       "      <td>0.00758964</td>\n",
       "      <td>0.0184977</td>\n",
       "      <td>0.000546459</td>\n",
       "      <td>0.00145225</td>\n",
       "      <td>0.000384273</td>\n",
       "      <td>0.0208852</td>\n",
       "      <td>0.000813958</td>\n",
       "      <td>0.000696951</td>\n",
       "      <td>0.000415007</td>\n",
       "      <td>0.0172471</td>\n",
       "      <td>0.0430713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_ccp_alpha</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_depth</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_min_impurity_decrease</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'ccp_alpha': 0, 'max_depth': 15, 'min_impurit...</td>\n",
       "      <td>{'ccp_alpha': 0.1, 'max_depth': 15, 'min_impur...</td>\n",
       "      <td>{'ccp_alpha': 0.1, 'max_depth': 100, 'min_impu...</td>\n",
       "      <td>{'ccp_alpha': 0, 'max_depth': 100, 'min_impuri...</td>\n",
       "      <td>{'ccp_alpha': 0, 'max_depth': 15, 'min_impurit...</td>\n",
       "      <td>{'ccp_alpha': 0, 'max_depth': 100, 'min_impuri...</td>\n",
       "      <td>{'ccp_alpha': 0.1, 'max_depth': 15, 'min_impur...</td>\n",
       "      <td>{'ccp_alpha': 0.1, 'max_depth': 100, 'min_impu...</td>\n",
       "      <td>{'ccp_alpha': 0, 'max_depth': 15, 'min_impurit...</td>\n",
       "      <td>{'ccp_alpha': 0, 'max_depth': 100, 'min_impuri...</td>\n",
       "      <td>{'ccp_alpha': 0.1, 'max_depth': 15, 'min_impur...</td>\n",
       "      <td>{'ccp_alpha': 0.1, 'max_depth': 100, 'min_impu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.715304</td>\n",
       "      <td>0.688733</td>\n",
       "      <td>0.685934</td>\n",
       "      <td>0.703497</td>\n",
       "      <td>0.658905</td>\n",
       "      <td>0.672128</td>\n",
       "      <td>0.656501</td>\n",
       "      <td>0.668366</td>\n",
       "      <td>0.708939</td>\n",
       "      <td>0.665498</td>\n",
       "      <td>0.678549</td>\n",
       "      <td>0.689284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.595963</td>\n",
       "      <td>0.590067</td>\n",
       "      <td>0.597946</td>\n",
       "      <td>0.603472</td>\n",
       "      <td>0.601762</td>\n",
       "      <td>0.562949</td>\n",
       "      <td>0.633817</td>\n",
       "      <td>0.642997</td>\n",
       "      <td>0.647486</td>\n",
       "      <td>0.59331</td>\n",
       "      <td>0.601104</td>\n",
       "      <td>0.592858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.636771</td>\n",
       "      <td>0.628259</td>\n",
       "      <td>0.593074</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.604138</td>\n",
       "      <td>0.600629</td>\n",
       "      <td>0.642244</td>\n",
       "      <td>0.593494</td>\n",
       "      <td>0.593636</td>\n",
       "      <td>0.618749</td>\n",
       "      <td>0.629279</td>\n",
       "      <td>0.623712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.651346</td>\n",
       "      <td>0.657569</td>\n",
       "      <td>0.677887</td>\n",
       "      <td>0.644578</td>\n",
       "      <td>0.664966</td>\n",
       "      <td>0.673918</td>\n",
       "      <td>0.582292</td>\n",
       "      <td>0.589754</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.577377</td>\n",
       "      <td>0.506258</td>\n",
       "      <td>0.508308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.656263</td>\n",
       "      <td>0.664641</td>\n",
       "      <td>0.666195</td>\n",
       "      <td>0.674127</td>\n",
       "      <td>0.650067</td>\n",
       "      <td>0.662109</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.650994</td>\n",
       "      <td>0.675782</td>\n",
       "      <td>0.670026</td>\n",
       "      <td>0.672543</td>\n",
       "      <td>0.654293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.651129</td>\n",
       "      <td>0.645854</td>\n",
       "      <td>0.644207</td>\n",
       "      <td>0.641688</td>\n",
       "      <td>0.635968</td>\n",
       "      <td>0.634347</td>\n",
       "      <td>0.631885</td>\n",
       "      <td>0.629121</td>\n",
       "      <td>0.62699</td>\n",
       "      <td>0.624992</td>\n",
       "      <td>0.617547</td>\n",
       "      <td>0.613691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.0384558</td>\n",
       "      <td>0.0339133</td>\n",
       "      <td>0.0402829</td>\n",
       "      <td>0.0443023</td>\n",
       "      <td>0.0273823</td>\n",
       "      <td>0.0447182</td>\n",
       "      <td>0.0258356</td>\n",
       "      <td>0.0317182</td>\n",
       "      <td>0.0700356</td>\n",
       "      <td>0.0373598</td>\n",
       "      <td>0.0625124</td>\n",
       "      <td>0.0616488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "      <td>0.84108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.844047</td>\n",
       "      <td>0.844047</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.844047</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.844047</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.844047</td>\n",
       "      <td>0.844167</td>\n",
       "      <td>0.844047</td>\n",
       "      <td>0.844167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.845896</td>\n",
       "      <td>0.845896</td>\n",
       "      <td>0.846035</td>\n",
       "      <td>0.846035</td>\n",
       "      <td>0.845896</td>\n",
       "      <td>0.846035</td>\n",
       "      <td>0.845896</td>\n",
       "      <td>0.846035</td>\n",
       "      <td>0.845896</td>\n",
       "      <td>0.846035</td>\n",
       "      <td>0.845896</td>\n",
       "      <td>0.846035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.844541</td>\n",
       "      <td>0.844541</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.844541</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.844541</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.844541</td>\n",
       "      <td>0.844452</td>\n",
       "      <td>0.844541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.841836</td>\n",
       "      <td>0.841836</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.841836</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.841836</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.841836</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.841836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.843233</td>\n",
       "      <td>0.843233</td>\n",
       "      <td>0.843532</td>\n",
       "      <td>0.843532</td>\n",
       "      <td>0.843233</td>\n",
       "      <td>0.843532</td>\n",
       "      <td>0.843233</td>\n",
       "      <td>0.843532</td>\n",
       "      <td>0.843233</td>\n",
       "      <td>0.843532</td>\n",
       "      <td>0.843233</td>\n",
       "      <td>0.843532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.00201666</td>\n",
       "      <td>0.00201666</td>\n",
       "      <td>0.00182063</td>\n",
       "      <td>0.00182063</td>\n",
       "      <td>0.00201666</td>\n",
       "      <td>0.00182063</td>\n",
       "      <td>0.00201666</td>\n",
       "      <td>0.00182063</td>\n",
       "      <td>0.00201666</td>\n",
       "      <td>0.00182063</td>\n",
       "      <td>0.00201666</td>\n",
       "      <td>0.00182063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            0   \\\n",
       "mean_fit_time                                                          3.37432   \n",
       "std_fit_time                                                          0.177248   \n",
       "mean_score_time                                                      0.0605515   \n",
       "std_score_time                                                       0.0132901   \n",
       "param_ccp_alpha                                                              0   \n",
       "param_max_depth                                                             15   \n",
       "param_min_impurity_decrease                                                  0   \n",
       "params                       {'ccp_alpha': 0, 'max_depth': 15, 'min_impurit...   \n",
       "split0_test_score                                                     0.715304   \n",
       "split1_test_score                                                     0.595963   \n",
       "split2_test_score                                                     0.636771   \n",
       "split3_test_score                                                     0.651346   \n",
       "split4_test_score                                                     0.656263   \n",
       "mean_test_score                                                       0.651129   \n",
       "std_test_score                                                       0.0384558   \n",
       "rank_test_score                                                              1   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844047   \n",
       "split2_train_score                                                    0.845896   \n",
       "split3_train_score                                                    0.844452   \n",
       "split4_train_score                                                    0.840692   \n",
       "mean_train_score                                                      0.843233   \n",
       "std_train_score                                                     0.00201666   \n",
       "\n",
       "                                                                            7   \\\n",
       "mean_fit_time                                                          3.93824   \n",
       "std_fit_time                                                           0.52203   \n",
       "mean_score_time                                                      0.0594967   \n",
       "std_score_time                                                      0.00758964   \n",
       "param_ccp_alpha                                                            0.1   \n",
       "param_max_depth                                                             15   \n",
       "param_min_impurity_decrease                                                0.1   \n",
       "params                       {'ccp_alpha': 0.1, 'max_depth': 15, 'min_impur...   \n",
       "split0_test_score                                                     0.688733   \n",
       "split1_test_score                                                     0.590067   \n",
       "split2_test_score                                                     0.628259   \n",
       "split3_test_score                                                     0.657569   \n",
       "split4_test_score                                                     0.664641   \n",
       "mean_test_score                                                       0.645854   \n",
       "std_test_score                                                       0.0339133   \n",
       "rank_test_score                                                              2   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844047   \n",
       "split2_train_score                                                    0.845896   \n",
       "split3_train_score                                                    0.844452   \n",
       "split4_train_score                                                    0.840692   \n",
       "mean_train_score                                                      0.843233   \n",
       "std_train_score                                                     0.00201666   \n",
       "\n",
       "                                                                            11  \\\n",
       "mean_fit_time                                                          4.88551   \n",
       "std_fit_time                                                           1.35411   \n",
       "mean_score_time                                                      0.0740457   \n",
       "std_score_time                                                       0.0184977   \n",
       "param_ccp_alpha                                                            0.1   \n",
       "param_max_depth                                                            100   \n",
       "param_min_impurity_decrease                                                0.5   \n",
       "params                       {'ccp_alpha': 0.1, 'max_depth': 100, 'min_impu...   \n",
       "split0_test_score                                                     0.685934   \n",
       "split1_test_score                                                     0.597946   \n",
       "split2_test_score                                                     0.593074   \n",
       "split3_test_score                                                     0.677887   \n",
       "split4_test_score                                                     0.666195   \n",
       "mean_test_score                                                       0.644207   \n",
       "std_test_score                                                       0.0402829   \n",
       "rank_test_score                                                              3   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844167   \n",
       "split2_train_score                                                    0.846035   \n",
       "split3_train_score                                                    0.844541   \n",
       "split4_train_score                                                    0.841836   \n",
       "mean_train_score                                                      0.843532   \n",
       "std_train_score                                                     0.00182063   \n",
       "\n",
       "                                                                            5   \\\n",
       "mean_fit_time                                                          3.44463   \n",
       "std_fit_time                                                          0.429552   \n",
       "mean_score_time                                                      0.0533535   \n",
       "std_score_time                                                     0.000546459   \n",
       "param_ccp_alpha                                                              0   \n",
       "param_max_depth                                                            100   \n",
       "param_min_impurity_decrease                                                0.5   \n",
       "params                       {'ccp_alpha': 0, 'max_depth': 100, 'min_impuri...   \n",
       "split0_test_score                                                     0.703497   \n",
       "split1_test_score                                                     0.603472   \n",
       "split2_test_score                                                     0.582764   \n",
       "split3_test_score                                                     0.644578   \n",
       "split4_test_score                                                     0.674127   \n",
       "mean_test_score                                                       0.641688   \n",
       "std_test_score                                                       0.0443023   \n",
       "rank_test_score                                                              4   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844167   \n",
       "split2_train_score                                                    0.846035   \n",
       "split3_train_score                                                    0.844541   \n",
       "split4_train_score                                                    0.841836   \n",
       "mean_train_score                                                      0.843532   \n",
       "std_train_score                                                     0.00182063   \n",
       "\n",
       "                                                                            2   \\\n",
       "mean_fit_time                                                          3.25712   \n",
       "std_fit_time                                                            0.1452   \n",
       "mean_score_time                                                      0.0525778   \n",
       "std_score_time                                                      0.00145225   \n",
       "param_ccp_alpha                                                              0   \n",
       "param_max_depth                                                             15   \n",
       "param_min_impurity_decrease                                                0.5   \n",
       "params                       {'ccp_alpha': 0, 'max_depth': 15, 'min_impurit...   \n",
       "split0_test_score                                                     0.658905   \n",
       "split1_test_score                                                     0.601762   \n",
       "split2_test_score                                                     0.604138   \n",
       "split3_test_score                                                     0.664966   \n",
       "split4_test_score                                                     0.650067   \n",
       "mean_test_score                                                       0.635968   \n",
       "std_test_score                                                       0.0273823   \n",
       "rank_test_score                                                              5   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844047   \n",
       "split2_train_score                                                    0.845896   \n",
       "split3_train_score                                                    0.844452   \n",
       "split4_train_score                                                    0.840692   \n",
       "mean_train_score                                                      0.843233   \n",
       "std_train_score                                                     0.00201666   \n",
       "\n",
       "                                                                            3   \\\n",
       "mean_fit_time                                                          3.16291   \n",
       "std_fit_time                                                         0.0336158   \n",
       "mean_score_time                                                      0.0521739   \n",
       "std_score_time                                                     0.000384273   \n",
       "param_ccp_alpha                                                              0   \n",
       "param_max_depth                                                            100   \n",
       "param_min_impurity_decrease                                                  0   \n",
       "params                       {'ccp_alpha': 0, 'max_depth': 100, 'min_impuri...   \n",
       "split0_test_score                                                     0.672128   \n",
       "split1_test_score                                                     0.562949   \n",
       "split2_test_score                                                     0.600629   \n",
       "split3_test_score                                                     0.673918   \n",
       "split4_test_score                                                     0.662109   \n",
       "mean_test_score                                                       0.634347   \n",
       "std_test_score                                                       0.0447182   \n",
       "rank_test_score                                                              6   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844167   \n",
       "split2_train_score                                                    0.846035   \n",
       "split3_train_score                                                    0.844541   \n",
       "split4_train_score                                                    0.841836   \n",
       "mean_train_score                                                      0.843532   \n",
       "std_train_score                                                     0.00182063   \n",
       "\n",
       "                                                                            6   \\\n",
       "mean_fit_time                                                          4.19138   \n",
       "std_fit_time                                                          0.338928   \n",
       "mean_score_time                                                      0.0695582   \n",
       "std_score_time                                                       0.0208852   \n",
       "param_ccp_alpha                                                            0.1   \n",
       "param_max_depth                                                             15   \n",
       "param_min_impurity_decrease                                                  0   \n",
       "params                       {'ccp_alpha': 0.1, 'max_depth': 15, 'min_impur...   \n",
       "split0_test_score                                                     0.656501   \n",
       "split1_test_score                                                     0.633817   \n",
       "split2_test_score                                                     0.642244   \n",
       "split3_test_score                                                     0.582292   \n",
       "split4_test_score                                                     0.644572   \n",
       "mean_test_score                                                       0.631885   \n",
       "std_test_score                                                       0.0258356   \n",
       "rank_test_score                                                              7   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844047   \n",
       "split2_train_score                                                    0.845896   \n",
       "split3_train_score                                                    0.844452   \n",
       "split4_train_score                                                    0.840692   \n",
       "mean_train_score                                                      0.843233   \n",
       "std_train_score                                                     0.00201666   \n",
       "\n",
       "                                                                            9   \\\n",
       "mean_fit_time                                                          3.78647   \n",
       "std_fit_time                                                           0.63343   \n",
       "mean_score_time                                                      0.0531126   \n",
       "std_score_time                                                     0.000813958   \n",
       "param_ccp_alpha                                                            0.1   \n",
       "param_max_depth                                                            100   \n",
       "param_min_impurity_decrease                                                  0   \n",
       "params                       {'ccp_alpha': 0.1, 'max_depth': 100, 'min_impu...   \n",
       "split0_test_score                                                     0.668366   \n",
       "split1_test_score                                                     0.642997   \n",
       "split2_test_score                                                     0.593494   \n",
       "split3_test_score                                                     0.589754   \n",
       "split4_test_score                                                     0.650994   \n",
       "mean_test_score                                                       0.629121   \n",
       "std_test_score                                                       0.0317182   \n",
       "rank_test_score                                                              8   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844167   \n",
       "split2_train_score                                                    0.846035   \n",
       "split3_train_score                                                    0.844541   \n",
       "split4_train_score                                                    0.841836   \n",
       "mean_train_score                                                      0.843532   \n",
       "std_train_score                                                     0.00182063   \n",
       "\n",
       "                                                                            1   \\\n",
       "mean_fit_time                                                          3.14749   \n",
       "std_fit_time                                                         0.0260389   \n",
       "mean_score_time                                                      0.0519485   \n",
       "std_score_time                                                     0.000696951   \n",
       "param_ccp_alpha                                                              0   \n",
       "param_max_depth                                                             15   \n",
       "param_min_impurity_decrease                                                0.1   \n",
       "params                       {'ccp_alpha': 0, 'max_depth': 15, 'min_impurit...   \n",
       "split0_test_score                                                     0.708939   \n",
       "split1_test_score                                                     0.647486   \n",
       "split2_test_score                                                     0.593636   \n",
       "split3_test_score                                                     0.509108   \n",
       "split4_test_score                                                     0.675782   \n",
       "mean_test_score                                                        0.62699   \n",
       "std_test_score                                                       0.0700356   \n",
       "rank_test_score                                                              9   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844047   \n",
       "split2_train_score                                                    0.845896   \n",
       "split3_train_score                                                    0.844452   \n",
       "split4_train_score                                                    0.840692   \n",
       "mean_train_score                                                      0.843233   \n",
       "std_train_score                                                     0.00201666   \n",
       "\n",
       "                                                                            4   \\\n",
       "mean_fit_time                                                          3.21339   \n",
       "std_fit_time                                                         0.0802968   \n",
       "mean_score_time                                                      0.0530675   \n",
       "std_score_time                                                     0.000415007   \n",
       "param_ccp_alpha                                                              0   \n",
       "param_max_depth                                                            100   \n",
       "param_min_impurity_decrease                                                0.1   \n",
       "params                       {'ccp_alpha': 0, 'max_depth': 100, 'min_impuri...   \n",
       "split0_test_score                                                     0.665498   \n",
       "split1_test_score                                                      0.59331   \n",
       "split2_test_score                                                     0.618749   \n",
       "split3_test_score                                                     0.577377   \n",
       "split4_test_score                                                     0.670026   \n",
       "mean_test_score                                                       0.624992   \n",
       "std_test_score                                                       0.0373598   \n",
       "rank_test_score                                                             10   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844167   \n",
       "split2_train_score                                                    0.846035   \n",
       "split3_train_score                                                    0.844541   \n",
       "split4_train_score                                                    0.841836   \n",
       "mean_train_score                                                      0.843532   \n",
       "std_train_score                                                     0.00182063   \n",
       "\n",
       "                                                                            8   \\\n",
       "mean_fit_time                                                          3.89958   \n",
       "std_fit_time                                                          0.828026   \n",
       "mean_score_time                                                      0.0629024   \n",
       "std_score_time                                                       0.0172471   \n",
       "param_ccp_alpha                                                            0.1   \n",
       "param_max_depth                                                             15   \n",
       "param_min_impurity_decrease                                                0.5   \n",
       "params                       {'ccp_alpha': 0.1, 'max_depth': 15, 'min_impur...   \n",
       "split0_test_score                                                     0.678549   \n",
       "split1_test_score                                                     0.601104   \n",
       "split2_test_score                                                     0.629279   \n",
       "split3_test_score                                                     0.506258   \n",
       "split4_test_score                                                     0.672543   \n",
       "mean_test_score                                                       0.617547   \n",
       "std_test_score                                                       0.0625124   \n",
       "rank_test_score                                                             11   \n",
       "split0_train_score                                                     0.84108   \n",
       "split1_train_score                                                    0.844047   \n",
       "split2_train_score                                                    0.845896   \n",
       "split3_train_score                                                    0.844452   \n",
       "split4_train_score                                                    0.840692   \n",
       "mean_train_score                                                      0.843233   \n",
       "std_train_score                                                     0.00201666   \n",
       "\n",
       "                                                                            10  \n",
       "mean_fit_time                                                          4.20914  \n",
       "std_fit_time                                                          0.771287  \n",
       "mean_score_time                                                      0.0819744  \n",
       "std_score_time                                                       0.0430713  \n",
       "param_ccp_alpha                                                            0.1  \n",
       "param_max_depth                                                            100  \n",
       "param_min_impurity_decrease                                                0.1  \n",
       "params                       {'ccp_alpha': 0.1, 'max_depth': 100, 'min_impu...  \n",
       "split0_test_score                                                     0.689284  \n",
       "split1_test_score                                                     0.592858  \n",
       "split2_test_score                                                     0.623712  \n",
       "split3_test_score                                                     0.508308  \n",
       "split4_test_score                                                     0.654293  \n",
       "mean_test_score                                                       0.613691  \n",
       "std_test_score                                                       0.0616488  \n",
       "rank_test_score                                                             12  \n",
       "split0_train_score                                                     0.84108  \n",
       "split1_train_score                                                    0.844167  \n",
       "split2_train_score                                                    0.846035  \n",
       "split3_train_score                                                    0.844541  \n",
       "split4_train_score                                                    0.841836  \n",
       "mean_train_score                                                      0.843532  \n",
       "std_train_score                                                     0.00182063  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search_cv.cv_results_ ).sort_values('mean_test_score', ascending = False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6206040820079577"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9760031281415894"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dfpdForLinearRegressionNoStyle.sample(frac=0.1, replace=False, random_state=1)\n",
    "X = sample.drop(['SoldPrice'], axis=1)\n",
    "y = sample['SoldPrice']\n",
    "#y = dfpdForDecisionTree['SoldPrice']\n",
    "#print(X.head(2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n",
    "\n",
    "# define model\n",
    "model = XGBRegressor()\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_test)\n",
    "r2_score(y_test, y_pred)\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "#scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "\n",
    "#print(scores.mean())\n",
    "#print(scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../Models/finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SettledDate</th>\n",
       "      <th>ZipCode</th>\n",
       "      <th>AcresTotal</th>\n",
       "      <th>Age</th>\n",
       "      <th>InteriorSqFt</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>BathsFull</th>\n",
       "      <th>BathsHalf</th>\n",
       "      <th>GarageSpaces</th>\n",
       "      <th>ANNEARUNDELMD</th>\n",
       "      <th>BALTIMOREMD</th>\n",
       "      <th>HARFORDMD</th>\n",
       "      <th>HOWARDMD</th>\n",
       "      <th>NoBasement</th>\n",
       "      <th>HasBasement</th>\n",
       "      <th>NoFireplace</th>\n",
       "      <th>HasFireplace</th>\n",
       "      <th>NoCentralAir</th>\n",
       "      <th>HasCentralAir</th>\n",
       "      <th>NotWaterfront</th>\n",
       "      <th>IsWaterfront</th>\n",
       "      <th>NotNewConstruction</th>\n",
       "      <th>IsNewConstruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63484</th>\n",
       "      <td>736653</td>\n",
       "      <td>21009</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>3172.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13321</th>\n",
       "      <td>737502</td>\n",
       "      <td>21032</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>909.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69463</th>\n",
       "      <td>738195</td>\n",
       "      <td>21154</td>\n",
       "      <td>9.44</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27823</th>\n",
       "      <td>737090</td>\n",
       "      <td>21409</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2996.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>737347</td>\n",
       "      <td>21114</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2308.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SettledDate  ZipCode  AcresTotal     Age  InteriorSqFt  Bedrooms  \\\n",
       "63484       736653    21009        0.38  1993.0        3172.0       4.0   \n",
       "13321       737502    21032        0.31  1987.0         909.0       2.0   \n",
       "69463       738195    21154        9.44  1981.0        2540.0       4.0   \n",
       "27823       737090    21409        0.25  2018.0        2996.0       4.0   \n",
       "6494        737347    21114        0.19  1994.0        2308.0       4.0   \n",
       "\n",
       "       BathsFull  BathsHalf  GarageSpaces  ANNEARUNDELMD  BALTIMOREMD  \\\n",
       "63484        3.0        1.0           2.0              0            0   \n",
       "13321        1.0        0.0           0.0              1            0   \n",
       "69463        3.0        0.0           2.0              0            0   \n",
       "27823        2.0        1.0           2.0              1            0   \n",
       "6494         2.0        1.0           2.0              1            0   \n",
       "\n",
       "       HARFORDMD  HOWARDMD  NoBasement  HasBasement  NoFireplace  \\\n",
       "63484          1         0           0            1            1   \n",
       "13321          0         0           1            0            1   \n",
       "69463          1         0           0            1            0   \n",
       "27823          0         0           0            1            0   \n",
       "6494           0         0           0            1            0   \n",
       "\n",
       "       HasFireplace  NoCentralAir  HasCentralAir  NotWaterfront  IsWaterfront  \\\n",
       "63484             0             0              1              1             0   \n",
       "13321             0             0              1              0             1   \n",
       "69463             1             0              1              1             0   \n",
       "27823             1             0              1              1             0   \n",
       "6494              1             0              1              1             0   \n",
       "\n",
       "       NotNewConstruction  IsNewConstruction  \n",
       "63484                   1                  0  \n",
       "13321                   1                  0  \n",
       "69463                   1                  0  \n",
       "27823                   0                  1  \n",
       "6494                    1                  0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([803012.8], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X_test)\n",
    "data = asarray([[738124, 21144, 1.61, 1995.0, 3806.0, 5.0, 2.0, 1.0, 2.0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]])\n",
    "data2 = asarray([[737928, 21784, 6.00, 1996.0, 4428.0, 4.0, 2.0, 1.0, 4.0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0]])\n",
    "pred = loaded_model.predict(data2)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1525,)\n",
      "(1525,)\n",
      "[361896.78 522698.5  591435.75 ... 449284.78 676837.   670505.75]\n",
      "1525\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4uElEQVR4nO3df5zVZZ338debYdDBXwNIpYMGldEtqRCT0tK2pgVoqZPpSllZ68Zmdvdz2WDtTkz3FqP71lw3i7Zuf5ViaIiRSxhamyvq0ICIyYJJykCJDUMoIw7wuf/4Xge+c+b8nJkz53vO+Twfj/OY77m+P851DXo+c13fz/e6ZGY455xzSTOk3BVwzjnnMvEA5ZxzLpE8QDnnnEskD1DOOecSyQOUc865RPIA5ZxzLpE8QLmaJ+liSb/Isf9hSX8/AJ9zuqQt/b1O2jXnSbpjIK/pXFJ4gHIVRdJmSV2SXpb0R0m3SDq8P9c0sx+Z2bSBquNAC218LbS5Q9IKSW/rw3U2S3pfEcefE37HI2Nl50lql3RUeC9Jn5P0pKTd4fiHJc2MnfOwpFdD/XdK+rWkk4qtfzHC7+yaUn6GKz0PUK4SnWNmhwMTgUnA3PJWZ1B8M7R5DPAicEupP9DM7gdWAtcDSGoEbgYuM7Od4bAbgS8CXwFGAU3A14AZaZf7XKj/KOBh4PbS1t5VAw9QrmKZ2R+B5USBCgBJUyT9l6ROSWslnR7b90lJv5e0S9Jzki6Olf8mdtz7JT0T/tq/CVBsX48hNUljJZmkoeH9pyT9LnzG7yX9Q7b6S/pq6I3skrRB0pkFtHk38GPg7Vmuea6k9aH9D0v6H6H8duB44P7Qk/mnfJ8VfB44S9J0okD1KzNbGq75VuCzwEwzW2FmXWa2z8x+Y2afzFL/vcBdwImxOh8i6QZJW8PrBkmHxPZ/WtKm0HtcKunYUC5J10t6MfxbPSnp7ZJmARcD/xTaen+BbXUJ4wHKVSxJY4CzgE3hfROwDLgGGAn8I3CPpNGSDiP6a/8sMzsC+CtgTYZrHg3cQ9QLOBp4FphaRLVeBD4IHAl8Crhe0jsyfM544HPAO0N9pgObC2jz4URfvm0Z9r0VuJOoRzMa+DlRQBpmZh8Hnif0Ps3sm+GcJyV9NNvnmdlLwBeAH4V2fT62+wzgBTNrzVfvWB2HhfqvihVfAUwh+kPjFOBUot8/ks4ArgX+FjgG+ANRgAOYBrwHeCvQCFwE/NnMFob6fjO09ZxC6+eSxQOUq0RLJO0CXiAKCFeG8o8BPzezn5vZfjNbAbQCZ4f9+4G3S2ows21mtj7Dtc8GnjazxWbWDdwA/LHQipnZMjN71iK/An4B/HWGQ/cBhwAnSqo3s81m9myOS/+jpE6iYHw48MkMx1wELAu9mW7gW0ADUTDOVt+TzezHeZq1CjgK+IWZbY+VH03a70bSltB7e1XSG2O7bgz1f5koMF8V23cx8A0zezFc/yrg47F9PzSz35rZHqLh3HdJGgt0A0cAbwNkZr8zs2152uIqiAcoV4laQq/jdKIvp6ND+RuBC8MXZGf4Qnw3cIyZvUL0Bf4ZYJukZVkSDY4lCnwAWDSb8gsZjstI0lmSVoXhqE6igHd0+nFmtomopzMPeFHSXamhqyy+ZWaNZvYGMzs3SzA7lqiHkfqM/aHuTYXWP4uFwG3A2ZLiwe7PRL2aA8xsDFF7DyE2NAp83swagUOJemKLJZ2cqd5h+9hM+8zs5fC5TWa2ErgJ+DfgT5IWSjqyH+10CeMBylWs0EO5hainANGX8e3hizz1OszM5ofjl5vZ+4m+VJ8Bvp/hstuA41JvJCn+HngFGB57/4bYsYcQDQ9+C3h9+EL+OT2/qOP1/7GZvZsosBpwXYFNz2ZruFZ63dtTH1nsBSVdGq7xWeCfge+HYTqIEijGSGou9HqhZ/ufRD3BVOZkj3oT3SvbmmlfGKodRWiTmd1oZpOBCURDfbNTH1VonVxyeYByle4G4P2SJgJ3AOdImi6pTtKhip49GiPp9SGB4DBgD9FQ074M11sGTJB0fkh8+DyxIER03+o9ko5XlGodzyAcRtRz2A7slXQWB7+Ee5A0XtIZIai9CnRlqU8x7gY+IOlMSfVEmXV7gP8K+/8EvKnQi4Ue3QLg02F47btEvZcrAMxsA/A94K6QWNIgqY4cQ4rhuu8iSpJIDbHeCXwt3Cs8Gvg60b8lRAkhn5I0Mfyu/jfwmJltlvROSaeFtr5C9HtM/Q6LaqtLKDPzl78q5kWUSPC+tLKbgXvC9mnAr4AOokCxjOgv8mNC+U6gkyjV+cRwzieB38SuNwP473DsTeG8v4/t/7dwjU3Ap4n+Wh8a9l1O9OXYSZRKfRdwTdh3OrAlbJ8MPA7sCnX9GXBsljbfkrpGhn3zgDti7z8EPB3q/itgQmzfeUSJEp3AP4ay9cDFWa69BPhOWtn4cO0J4b2Igvg6oiC7LXzu3wJDwjEPEwWPl8NrE/Cl2DUPJUpg2RZeNwKHxvZ/hihZJfV7GhPKzwSeDNd8iSgx4vCw7wSiPyY6gSXl/u/WX317KfxjOuecc4niQ3zOOecSyQOUc865RPIA5ZxzLpE8QDnnnEukoeWuQCU5+uijbezYseWuhnPOVZzVq1e/ZGajiznHA1QRxo4dS2trwdOOOeecCyT9If9RPfkQn3POuUTyAOWccy6RPEA555xLJA9QzjnnEskDlHPOuUTyLD5XsCVt7SxYvoGtnV0c29jA7OnjaZnU36WGnHMuMw9QriBL2tqZe+86urqj1QzaO7uYe+86AA9SzrmS8CE+V5AFyzccCE4pXd37WLB8Q5lq5Jyrdh6gXEG2dnYVVe6cc/3lAcoV5NjGhqLKnXOuvzxAuYLMnj6ehvq6HmUN9XXMnj6+TDVyzlU7T5JwBUklQngWn3NusJQtQEk6DrgNeAOwH1hoZt+WNBJYBIwFNgN/a2Y7wjlzgUuBfcDnzWx5KJ8M3AI0AD8HvmBmJumQ8BmTgT8DF5nZ5nDOJcDXQnWuMbNbS9zkitcyqckDknNu0JRziG8v8BUz+x/AFOBySScCc4BfmtkJwC/De8K+mcAEYAbwHUmpMaebgVnACeE1I5RfCuwws7cA1wPXhWuNBK4ETgNOBa6UNKK0zXXOOVeMsgUoM9tmZr8N27uA3wFNwHlAqjdzK9ASts8D7jKzPWb2HLAJOFXSMcCRZvaomRlRjyl+Tupai4EzJQmYDqwws47QO1vBwaDmnHMuARKRJCFpLDAJeAx4vZltgyiIAa8LhzUBL8RO2xLKmsJ2enmPc8xsL7ATGJXjWpnqNktSq6TW7du397GFzjnnilX2ACXpcOAe4Itm9pdch2YosxzlfT2nZ6HZQjNrNrPm0aOLWgzSOedcP5Q1QEmqJwpOPzKze0Pxn8KwHeHni6F8C3Bc7PQxwNZQPiZDeY9zJA0FjgI6clzLOedcQpQtQIV7QT8Afmdm/ze2aylwSdi+BLgvVj5T0iGSxhElQzwehgF3SZoSrvmJtHNS17oAWBnuUy0HpkkaEZIjpoUy55xzCVHO56CmAh8H1klaE8r+GZgP3C3pUuB54EIAM1sv6W7gaaIMwMvNLDU53GUcTDN/ILwgCoC3S9pE1HOaGa7VIelq4Ilw3DfMrKNE7XTOOdcHijoUrhDNzc3W2tpa7mo451zFkbTazJqLOafsSRLOOedcJh6gnHPOJZIHKOecc4nkAco551wieYByzjmXSB6gnHPOJZIHKOecc4nkCxbWqCVt7b74oHMu0TxA1aAlbe3MvXcdXd3RRBztnV3MvXcdgAcp51xi+BBfDVqwfMOB4JTS1b2PBcs3lKlGzjnXmweoGrS1s6uocuecKwcPUDXo2MaGosqdc64cPEDVoNnTx9NQX9ejrKG+jtnTx5epRs4515snSVSZQrLzUu89i885l2QeoKrIkrZ2Zi9eS/e+aAmV9s4uZi9eC/TOzmuZ1OQByTmXaOVe8v2Hkl6U9FSsbJ6kdklrwuvs2L65kjZJ2iBpeqx8sqR1Yd+NYWVdwuq7i0L5Y5LGxs65RNLG8EqtulvRrrp//YHglNK9z7jq/vUZj1/S1s7U+SsZN2cZU+evZElb+2BU0znnClLuHtQtwE3AbWnl15vZt+IFkk4kWhF3AnAs8KCkt4ZVdW8GZgGrgJ8DM4hW1b0U2GFmb5E0E7gOuEjSSOBKoBkwYLWkpWa2ozTNHBw7dncXXJ7vWSh/kNc5V25l7UGZ2a+JlmIvxHnAXWa2x8yeAzYBp0o6BjjSzB61aHng24CW2Dm3hu3FwJmhdzUdWGFmHSEorSAKajUj17NQqeDV3tmFcTB4eQ/LOTeYkprF9zlJT4YhwBGhrAl4IXbMllDWFLbTy3ucY2Z7gZ3AqBzX6kXSLEmtklq3b9/ev1aVWGNDfcHluZ6F8gd5nXNJkMQAdTPwZmAisA34P6FcGY61HOV9PadnodlCM2s2s+bRo0fnqHb5zTt3AvVDejatfoiYd+6EXsfmehbKH+R1ziVB4gKUmf3JzPaZ2X7g+8CpYdcW4LjYoWOAraF8TIbyHudIGgocRTSkmO1aiZcrsaFlUhMLLjyFpsYGBDQ1NrDgwlMy3jvK9SxUtuDVODxzD80550ohcQEq3FNK+RCQyvBbCswMmXnjgBOAx81sG7BL0pRwf+kTwH2xc1IZehcAK8N9quXANEkjwhDitFCWaPnuDRWT2NAyqYlrzz+pRzC79vyTaJnUxOzp46mv693JfPnVvX4fyjk3aMqaxSfpTuB04GhJW4gy606XNJFoyG0z8A8AZrZe0t3A08Be4PKQwQdwGVFGYANR9t4DofwHwO2SNhH1nGaGa3VIuhp4Ihz3DTMrNFmjbPLdGyp2hvJsz0K1TGpi3tL1dHb1zP7r3m8sWL5hwLP5PGPQOZeJog6FK0Rzc7O1traW7fPHzVmW8UaZiO4dtWe4R9TU2MAjc84Y0M96bv4Hir5eNunp7hANNaZ6c8656iBptZk1F3NO4ob4XHaDmdgwWBPKesagcy4bD1AVpC+JDX0NKIM1oaxnDDrnsin3TBKuCPkmec00VNbXgDJYE8pmG5r0pT+ccx6gKkimZAKAqfNX0t7Z1ePhrhHD67nynAn9CiiDMaHs7OnjBzSwOueqhweoCpFp7rzZi9eCRdl10PNJ41e795ehlsXzpT+cc9l4gKoQmZIJ0mcuj0slGuT7ok9Circv/eGcy8QDVIXoS9JApns7cflmNHfOuXLyLL4K0dekgYlX/SLr7A+e4p18vmaXq2UeoCpEprTv+jr1mhw2XWdXd9alMjzFO9l82RNX6zxAVYhMc+ctuOCUA5PD5pKtVzRYD+O6vvEerqt1PtVREco91VEuqVTzXJrSkiByTTMEnllXboM13ZRzg8GnOqphmYYA06UPEWWb0RzwoaUE8B6uq3WexVdhsqWFp3o3V92/nh27u7Oen55+ninFe+r8lVmHlrwXNXj8IWZX63yIrwjlHuLLNCQnogd048N3qSCWa8ivKUwwm2n4ri9DS0l4nqoa+e/VVYu+DPF5D6qCZLppngokqWG41j908NAz29na2YUEmf7+EAefkcr07FOx8+P581Sl4w8xu1rm96AqSL70767uffxo1fMH7h1l6xynF6dnhhU7k7lnmznnSqGsAUrSDyW9KOmpWNlISSskbQw/R8T2zZW0SdIGSdNj5ZMlrQv7bgxLvxOWh18Uyh+TNDZ2ziXhMzZKSi0Ln2iF3Bzv64BtPPjlWg4+37mFlDvnXCHKPcR3C3ATcFusbA7wSzObL2lOeP9VSScSLdk+ATgWeFDSW8Oy7zcDs4BVwM+BGUTLvl8K7DCzt0iaCVwHXCRpJNHy8s1E3+mrJS01sx0lb3ER0u8/vPdto7lndXuv3spAOKqhvsf7+NBSqh5fWrQm430QXzLDOVcKZe1BmdmvgY604vOAW8P2rUBLrPwuM9tjZs8Bm4BTJR0DHGlmj1qU8XFb2jmpay0Gzgy9q+nACjPrCEFpBVFQS4xMswjcs7qdD09uOvBgbu45JIrzymt7M6aRFzKbQaYhwdR9Lp+exznXV+XuQWXyejPbBmBm2yS9LpQ3EfWQUraEsu6wnV6eOueFcK29knYCo+LlGc7pQdIsot4Zxx9/fN9bVaRs93UeemY7j8w5A+jZw+pvLmb3PuOLi9bwxUVrqJPYZ3bgZ7pMqeqpOqfWpUpP3ogf55xzhaikJIlMHQbLUd7Xc3oWmi00s2Yzax49enRBFR0Iue7rpCYQ/dKiNQBcPOX4Ae1NpYJSpuCUrX4tk5p4ZM4ZNDU25E3CcM65QiQxQP0pDNsRfr4YyrcAx8WOGwNsDeVjMpT3OEfSUOAooiHFbNdKjGz3bxqH1/cacvvRqueL6kENRDDLVj9PmHDODZQkBqilQCqr7hLgvlj5zJCZNw44AXg8DAfukjQl3F/6RNo5qWtdAKwM96mWA9MkjQhZgtNCWWJkS/U2I+uzUIUYMbyexuH1+Q/MIVfKebbAZeD3o5xzRSl3mvmdwKPAeElbJF0KzAfeL2kj8P7wHjNbD9wNPA38B3B5yOADuAz4d6LEiWeJMvgAfgCMkrQJ+DJRRiBm1gFcDTwRXt8IZYmQurfU1b2Puihj/kCq986u7NMYFWLH7m46c0yFlE++lPNccwL6nH7OuWL4VEdFGIypjvLNMP6Vu9fmvDdUiKYsaeG5pOpQSKJDvqmWmhobDiR6OOdqg89mXuGWtLXzlbvXZsze++d7n+RLi9b0Ozg1NtQXNPN5XL5eU7pUwkS2e11+P8o5V4gkppnXpFTPKVsA2t29f0A+Z965E3qlhWfT33WH/AFe51x/eIBKiEzPPQ2khvohXHv+yUCUrJCanSKX/gaSfMtF+EzdzrlcPEAlRCmHvSR4tXs/V92/npdf3Uv3/qiXlv5QbY9zoN/rDsV7aulByGdAd87l4wEqIbINh9VJHHHoUDr7kb2XGjXMtJBh6qnleJAS0cO/2QJFrp5Ppn2ZEiJyzYDuAco5Bx6gEiPTcJiAj5x2HM1vHNlr30BKLXhYyFBbrp4PUHCvKFuPsb2zi3FzlvmQn3POA1RStExqovUPHT1mhTDgjlXPc8eq5xlWN5CTGfVUTNp3vrWfCukVLWlrZ0iWef6AHpPSgg/5OVerPEAlyEPPbM86K8Rr+0rzvFoqaeFrS9Zx52MvHJgk9iOnHcc1LSf1Oj5b1l+ue2jxffmyFeN8yM+52uYBqozS79cU+/BsXzU21LOzq/vAMFrrHzq4Y9XzB/bvMzvwPh6klrS1Z02qSGX85UsrLzZb0Z+Zcq52+YO6ZbKkrZ3ZP1nbY9LXwVA/JHoW6rn5HzgwrBcPTnE/fqxn+YLlG7L28N77ttEFLRVfbMDpS6p7arb3cXOW+fx/zlUwD1BlMm/p+gPp3oOpez/M/slalrS1Hxhuyya9ermCyz2royCQb6n4bAFnxPD6vMGtEIUssOicqww+F18RBnIuvrFzlg3IdfqqKceQXPpxqSHIV/bszZnuXkiyRb65Bvv74O7U+Ssztsnn/3OuvPoyF5/fg6pRhQ4ppo4r5PhChu9yPbwb399Xvh6Vc9XDA1SZjBhen/HB2UpW6P2ilklNJcvM8/n/nKsefg+qTK48Z0K5q5BVY0PfFjQcO6r8QaCQRA3nXGVIbICStFnSOklrJLWGspGSVkjaGH6OiB0/V9ImSRskTY+VTw7X2STpxrDqLmFl3kWh/DFJYwezfS2TmhjRz5VtS+GQoUP6PK3SI8928LUlvZMuBjOrrmVSU95EDedcZUhskoSkzUCzmb0UK/sm0GFm8yXNAUaY2VclnQjcCZwKHAs8CLzVzPZJehz4ArAK+Dlwo5k9IOmzwMlm9hlJM4EPmdlFueo00AsWLmlrZ/bitXSX6CHccqiTePbasw+8z5UUUY1Bw2dody6zWkiSOA84PWzfCjwMfDWU32Vme4DnwhLvp4Ygd6SZPQog6TaghWhJ+POAeeFai4GbJMlKHLHTv8BI6B8IfZU+Q0QtTQrrM7Q7N7AKGuKTNFXSYWH7Y5L+r6Q3lrZqGPALSaslzQplrzezbQDh5+tCeRPwQuzcLaGsKWynl/c4x8z2AjuBUemVkDRLUquk1u3bt/erQZme0RmgdQgHhYiW7silLu2AWsqqyzdPoXOuOIXeg7oZ2C3pFOCfgD8At5WsVpGpZvYO4CzgcknvyXFspq9Ny1Ge65yeBWYLzazZzJpHjx6dr845lXpRwlJqamzg+osmMjRPhPrIacf1eJ8te64as+pqKRg7NxgKDVB7w9DXecC3zezbwBGlqxaY2dbw80Xgp0T3l/4k6RiA8PPFcPgWIP7NOAbYGsrHZCjvcY6kocBRQEcp2pJSqV9U9XVi9vTxLFi+IefsF1PfPLLXBLO1lFVXS8HYucFQaIDaJWku8DFgmaQ6oGQpaJIOk3REahuYBjwFLAUuCYddAtwXtpcCM0Nm3jjgBODxMAy4S9KUkL33ibRzUte6AFhZ6vtPlfpFddiwobRMasobYH/7/M5eGXq1lFVXS8HYucFQaJLERcBHgUvN7I+SjgcWlK5avB74acgIHwr82Mz+Q9ITwN2SLgWeBy4EMLP1ku4Gngb2ApebWWos7TLgFqCBKDnigVD+A+D2kFDRAcwsYXuAzIsSVoJU2nm+GdezJT8U82BuJWfB5ZslwzlXnMSmmSfRQKSZp76AB2v28oGQSh3PlDKeyeb5H+jT59RaSrpztaQvaeY5h/gk7ZL0lwyvXZL+0r/q1qaWSU0VN2lpKnU8PlyXTXoWXzH6kgXnS2s4V71yDvGZWUkTIWpVroX/kig+40VquC7bbOyFrJSbTbFZcH157qiShxCdqzVFPagr6XXAoan3ZpZ5pTuXU66F/5Koc3c34+Ys46iGeqTofZ2UMRjl6l3lU8hEr/EAMyRDHXI9BFyuB2k9KDrXN4U+qHuupI3Ac8CvgM0cTDZwRaq0dHMLr86ubnbs7sbI3FPqb8Zaviy49Aeds/XWsv1+y/EgrS+g6FzfFZpmfjUwBfhvMxsHnAk8UrJaVbmj+jhbeBLVSQOWPp4vJb3QB52zpfOX40Fan13Cub4rdIiv28z+LGmIpCFm9pCk60pasyq1pK2dV17bW+5qDJj9ZjzXx6y9THKlpBcSSFIPFWdSjrWifHYJ5/qu0B5Up6TDgV8DP5L0baLnjVyRFizfUFWzlxtkzJ4rRXZdQYEkx6+2HA/S+uwSzvVdoQHqPKAL+BLwH8CzwDmlqlQ1q6TnnwrV3tnF7J+sPRCESnXfJVOASde937IOn5VjVgufXcK5vitoiM/MXom9vbVEdakJ2bLfKl33fmPe0vW0TGoq2RIb6TM1ZPst5ho+K+Vy89k+D3x2Cef6oqAAJWkXBwdPhhHNw/eKmR1ZqopVq2oMTimpKZFKed8lHmCmzl856PeU+iJe51TK+ZcWrfFg5VwehfagejywK6mFaHZxV6SmPPPZVYNsyQhHNdQzdf7KnD2JYp4ZyjS3YZKHz3xBQ+eKU+g9qB7MbAlQWfP1JMCStnZe2VP9uSWZ7rvUDxGvvLY3532pYu9dVdpM6Z5y7lxxCh3iOz/2dgjQTOXM1JMIhU60Wskaw/Ndme677H5tLzt2d/c4vqt7H1+5e+2Bc/Ldu8rWu0pqQErnKefOFafQ56DiGXt7iWaSOG/Aa1OFKnH28r6oHyLmnTvhwPv0wDEux9x9s38SBalcX+DVMDxWjuewnKtkBQ3xmdmnYq9Pm9m/hJVuXQ7xIatqd/ihuf/WyfUlnMoAzPXMUDUMj3nKuXPFybfcxr9KujHba7AqWSqSZkjaIGmTpDkDff1Cp+apBjt2d/d4FipdvmeYOru6c36BZ+tdtYfeVSVI3TOLzw5/yNA+3QZ2ribk+7+jFVhNNIP5O4CN4TURqOhv3rBs/b8BZwEnAh+RdOJAfkat3VtI9YQyaZnUxIcn5x6Ky5X0kKsHVmmTr77avf/AdmdXd8XV39WWcq65lm89qFsBJH0SeK+ZdYf33wV+UfLaldapwCYz+z2ApLuI7qs9PVAfkG+J9GqUehYqk4ee2Z51X6pXkS3pIVNKecpAPAQ8EApJkS/VQ8zOlUK57/0WtOS7pA3Au8ysI7wfAawys4odPJd0ATDDzP4+vP84cJqZfS7bOc1HHGGtkycX/BkvvbyH329/hf1V/HBuJq8/8lDGHX1Yr/JVv/9z1nPe8rrDOfrwQ3Je96WX97DpxZez7p/yplGFV3KAZfq3HiLxptGH9WhXrt9BOevvXCa/fb6T1/b2/qNw2NA63nF8Y1HX0q9+VfSS74Vm8c0H2iQ9FN7/DTCvmA9KoExrk/eKJJJmAbMATj4k9xdoutQX0/MdXRn/kavVn/7yKl3d+3i1ez+v7d3HsKF1HD+ygWFD6zL+HobWDckbnCD6fWb7XQ4bmnuOvlJ7vqOr1x8i+814vqOrR9uy/Q7KXX/nMsn2vTVY32eFziTx/yQ9AJwWiuaY2R9LV61BsQU4LvZ+DLA1/SAzWwgsBGhubjYefrioDzk6vLJNy1MrGurr+PDkJu5Z3d5r5odrzz+J5gKHC57P8DxZ6hrvKOMQ2YfnLMv4YKCgx3IkSa2/c5l8Jcv3VlNjA4/MKXKuBmXqE+SWL4vvbeHnO4BjgRfC69hQVsmeAE6QNE7SMGAmsLRUH1ZrCRPpurr38dAz2/s980NSZ48odFmNpNbfuUzK/WhEzntQkhaa2azY0F6cmVlFT3ck6WzgBqAO+KGZ/Uuu45ubm621tbVPn1XrPSjo3ZuoJplmCkn1jDz4uEpWzPyYuUgq+h5UQUkSLtKfAFULUx3l06dhgQoyUP8jO1eN+hKgCp2L70LgP8xsl6SvET0TdbWZtfWhnjUpPj9dNfekhgje9aaR/Pb5nRUzy/hAqaR5AZ2rBIVm8f0vM/uJpHcD04FvAd/lYNKEyyP+13U1LlrY2FDPvHMn9Fr3qD20Nz4tkX+JO+cKUWiASv0p/AHgZjO7T9K80lSp+qQP71VTcGrKMpSVel/pE7w658qn0ADVLul7wPuA6yQdQh/XkqpF1TYn3yFDh3Ddh0/OG2TmLV3vsyY45/qs0AD1t8AM4Ftm1inpGGB26apVXaotxXzP3mguuVxJAUva2rNOe1Rtvw/nXGkU+qDubkkvAu8mmix2b/jpClCNc/Jddf96Xu3en3X4LtcyGL7+kXOuEAUN00m6EvgqMDcU1QN3lKpS1SbfUhOVaMfu7pzrM+XqJVV7Np9zbmAUeh/pQ8C5wCsAZrYVOKJUlao2qdkDUkuiV7PU6rdDskxrMmJ4vd9/cs4VpNAA9ZpFT/QagKTeU1W7nFomNfVlKqqK0zi8nrn3rsuYqdhQX8eV50zIcJZzzvWWN0BJEvCzkMXXKOnTwIPA90tduWqzY3f2tZIqUaY5uszImLFYJ/m0P865ouQNUKHn1AIsBu4BxgNfN7N/LW3VXJINrx+ScdLTnVky9/abeXByzhWl0DTzR4FOM/PU8n5obKjPueJsJeneFw3hpc+tl20qJ8/cc84Vq9B7UO8FHpX0rKQnU69SVqwazTt3AvVDquNGVPd+y5hKXu7p+Z1z1aPQHtRZJa1FjUgNcX3l7rVVMd1RKmMv/WHda88/yWf1ds71my+3UYT+LLcRNy7L6quVZnj9EAz5GkjOubz6styGz6dXBtVyP6Zr7/6cD+s651x/JC5ASZonqV3SmvA6O7ZvrqRNkjZImh4rnyxpXdh3Y0iNR9IhkhaF8sckjY2dc4mkjeF1yWC2cfb08dTXVf69qGydb59rzzk3EBIXoILrzWxieP0cQNKJwExgAtHEtd+RlLobfzMwCzghvGaE8kuBHWb2FuB64LpwrZHAlUTrWZ0KXClpxKC0jDBX3QWnMGJ4Zc8skS3fo1p6iM658kpqgMrkPOAuM9tjZs8Bm4BTw8zqR5rZo+GZrduInttKnXNr2F4MnBl6V9OBFWbWYWY7gBUcDGqDomVSE1eeM6Gi5+irU+aHdT1jzzk3EJIaoD4XUtl/GOvZNAEvxI7ZEsqawnZ6eY9zzGwvsBMYleNavUiaJalVUuv27dv716o0lb5OVPd+2LP3YP0bG+o9QcI5N2AKTTMfUJIeBN6QYdcVRMN1VxPN+3c18H+AvwMyDShZjnL6eE7PQrOFwEKIsvgyHdNX1bAEx/7Yb+SVPXvLV5GYXOtUOecqR1kClJm9r5DjJH0f+Fl4uwU4LrZ7DLA1lI/JUB4/Z4ukocBRQEcoPz3tnIeLacNAqJMS/zxUMbNfdO835i1dX9ZgsKSt3ZeZd65KJG6IL9xTSvkQ8FTYXgrMDJl544iSIR43s23ALklTwv2lTwD3xc5JZehdAKwM96mWA9MkjQhDiNNC2aBZ0tae+OAE0NnVnbG7mev4cso0bOqp785VprL0oPL4pqSJRENum4F/ADCz9ZLuBp4mWtH3cjNLfRNdBtwCNAAPhBfAD4DbJW0i6jnNDNfqkHQ18EQ47htm1lHaZh2U+iu/Egj4qzeP5JFnB+3X0y/ZUtw99d25ypO4AGVmH8+x71+Af8lQ3gq8PUP5q8CFWa71Q+CHfa9p31VScoQBm//cxcemHM+dj72Qt9dX7tT5YxsbfLJa56pE4ob4akGl/TW/tbOLa1pO4tlrz2bz/A9ww0UTMwai+jqVfUFCn6zWuerhAaoMKu2v+fT6tkxqou3r07jhook91oNacMEpZU9EaJnUlHGdqnLXyzlXPJ8stggDNVlseqZZkvnkr865geCTxVaIlklNfHhysr/wvffhnCu3xCVJ1IqHnhnYWSkG0ojh9bR9fVq5q+Gcq3HegyqTJCdKvPzqXpa0tZe7Gs65GucBqkySnCiRbTl355wbTB6gyiTpac/5enhL2tqZOn8l4+YsY+r8ld7jcs4NOA9QZdIyqYnh9cn99efq4S1pa2f24rW0d3ZhRPPdzV681oOUc25AJfcbssotaWune18yU/zrhyhnD++q+9f3qnv3PuOq+9eXumrOuRriAapMFizfQPf+gQ9Qw+pU1OSumaRmJc/WI9qxO/OEsNnKnXOuLzxAlUmpsviGDxvKxVOOLzhIZTuus6ubufeu82E751zZ+HNQZZJtUtP+2tnVTfMbR/KztdvyLn3RlKcOXd37mLd0fa/F/7KtEdXYUN6JYp1z1cV7UGUye/r4fg/FZXJUQz1z711XUHB6ZM4ZNOVJd+/s6u6RDDH33nV88JRjqB/Ss/b1Q8S8c8s7Uaxzrrp4gCqTlklNXDzl+AG9pgCJvHP81Q8Ru1/by7g5y9j92t5ewSaXru59PPTMdhZceErPiWIvLP9Esc656lKWACXpQknrJe2X1Jy2b66kTZI2SJoeK58saV3Yd2NYPZewwu6iUP6YpLGxcy6RtDG8LomVjwvHbgznDhuEZvdyTctJ3HDRRIqIDzkZuRMVRBiGU3Rc6vj9RX7O1s4uWiY18cicM3hu/gd4ZM4ZHpyccwOuXD2op4DzgV/HCyWdSLTq7QRgBvAdSanFfW4GZhEt9X5C2A9wKbDDzN4CXA9cF641ErgSOA04FbgyLO9OOOZ6MzsB2BGuURYtk5oYqAnlcy0WWCdx/UUT2fXq3l4p4vuKzCZM8iwYzrnqUZYAZWa/M7NMc+mcB9xlZnvM7DlgE3CqpGOAI83sUYvWB7kNaImdc2vYXgycGXpX04EVZtZhZjuAFcCMsO+McCzh3NS1ymIgvvAb6utyBrp9Zsy9d13eFXEL+Zykz4LhnKsOSbsH1QS8EHu/JZQ1he308h7nmNleYCcwKse1RgGd4dj0a/UiaZakVkmt27eXZgbyTKvAFiO1LMbOHIkRdVKf159KjUD68hvOucFUsjRzSQ8Cb8iw6wozuy/baRnKLEd5X87Jda3eO8wWAgshWrAw23H9kfrC/8rda4vu4aSy8SB6+Ddb2nhfe04CLp5yPNe0nNSn851zrq9K1oMys/eZ2dszvLIFJ4h6M8fF3o8BtobyMRnKe5wjaShwFNCR41ovAY3h2PRrlU3LpCb2FxlE0ofbZk8fnzEjb4iyP5Cbj5Hstaucc9UraUN8S4GZITNvHFEyxONmtg3YJWlKuIf0CeC+2DmpDL0LgJXhPtVyYJqkESE5YhqwPOx7KBxLODdX0Bw0+e5FTX3zyB6p3enDbS2Tmjj80N6d4v2Wo4tYgCSvXeWcq15lmUlC0oeAfwVGA8skrTGz6Wa2XtLdwNPAXuByM0vdOLkMuAVoAB4IL4AfALdL2kTUc5oJYGYdkq4GngjHfcPMOsL2V4G7JF0DtIVrlN3s6eOZe++6XveKhgg+elphw2ydBc6HN0TRfan4fIAicyBLatbekrb2XrNc+P0x56qHbKBynGtAc3Oztba2lvQz+vulO3X+yoKmUBJw/UUTe3zWe982mntWt/cIkA31dYlMjFjS1t4rmCe1rs45kLTazJrzH3mQz8WXMC2Tmvr1Bfvet43mjlXP5z3u2MaGjJ/V/MaRFdErWbB8Q6+eZlf3PhYs35DI+jrniucBKiEGYrhqSVs796zOP/u4iObVmzp/Za/P6W+AHCzZ7ov5/TLnqkfSkiRqUmq4Kn1S1mKXusjUq8gkNajb3tnFlxat4WtL1hVf6TLLdl8sqffLnHPF8wCVALmGq4rRl96DAT9a9XzFrfuU6eFmn+XCueriASoBBmq4qq+9B4Oig2G5tUxq4trzT8qZdu+cq2x+DyoBsi1eWGzAyZamXohKvHdTKffLnHN94z2oBBio4ar0XkVjQz0jhtcjomeecvF7N865pPEeVAKkegEDkd6drVcxbs6yrOf4vRvnXBJ5gEqIUg9XZRtGhCgh4yt3r6X1Dx0+KaxzLjF8iK9G5FvSY58Zd6x6viJTzp1z1ckDVI1Ivz+VzZ2PvZBjr3PODR4PUDWkZVITj8w5g+fmfyDrMfvMmDp/ZcU9F+Wcqz4eoGpUrqy+vs5k4ZxzA8kDVI36yGnH5dzfl5ksnHNuIHkWX41KZevd+dgLWZeDr8SHd51z1cN7UDXsmpaTePbas2nyiVedcwlUlgAl6UJJ6yXtl9QcKx8rqUvSmvD6bmzfZEnrJG2SdGNY+p2wPPyiUP6YpLGxcy6RtDG8LomVjwvHbgznDhukpieST7zqnEuicvWgngLOB36dYd+zZjYxvD4TK78ZmAWcEF4zQvmlwA4zewtwPXAdgKSRwJXAacCpwJWSRoRzrgOuN7MTgB3hGjXLJ151ziVRWe5BmdnvAJRnfrgUSccAR5rZo+H9bUAL8ABwHjAvHLoYuCn0rqYDK8ysI5yzApgh6S7gDOCj4Zxbw/k397NZFc0nXnXOJU0S70GNk9Qm6VeS/jqUNQFbYsdsCWWpfS8AmNleYCcwKl6eds4ooDMcm36tXiTNktQqqXX79u39a5lzzrmClawHJelB4A0Zdl1hZvdlOW0bcLyZ/VnSZGCJpAmQcfKDVOpZtn3FlmdkZguBhQDNzc1Zj3POOTewShagzOx9fThnD7AnbK+W9CzwVqJezpjYoWOArWF7C3AcsEXSUOAooCOUn552zsPAS0CjpKGhFxW/lnPOuYRI1BCfpNGS6sL2m4iSIX5vZtuAXZKmhPtLnwBSvbClQCpD7wJgpZkZsByYJmlESI6YBiwP+x4KxxLOzdajc845VyblSjP/kKQtwLuAZZKWh13vAZ6UtJYo4eEzqSQH4DLg34FNwLNECRIAPwBGSdoEfBmYAxDOuxp4Iry+EbvWV4Evh3NGhWs455xLEFmWWQRcb83Nzdba2joon7WkrX1AFjB0zrkkkLTazJrzH3mQT3WUQEva2pl77zq6uvcBBydvBTxIOedqRqLuQbnIguUbDgSnFJ+81TlXazxAJVC2SVp98lbnXC3xAJVA2SZp9clbnXO1xANUAvnkrc4550kSiZRKhPAsPudcLfMAlVA+eatzrtb5EJ9zzrlE8gDlnHMukTxAOeecSyQPUM455xLJA5RzzrlE8gDlnHMukTzNvEb5bOnOuaTzAFWDfLZ051wlKNeChQskPSPpSUk/ldQY2zdX0iZJGyRNj5VPlrQu7LsxrKyLpEMkLQrlj0kaGzvnEkkbw+uSWPm4cOzGcO6wwWl5Mvhs6c65SlCue1ArgLeb2cnAfwNzASSdCMwEJgAzgO+kloAHbgZmES0Df0LYD3ApsMPM3gJcD1wXrjUSuBI4DTgVuDIs/U445nozOwHYEa5RM3y2dOdcJShLgDKzX5jZ3vB2FTAmbJ8H3GVme8zsOaLl3U+VdAxwpJk9atESwLcBLbFzbg3bi4EzQ+9qOrDCzDrMbAdRUJwR9p0RjiWcm7pWTfDZ0p1zlSAJWXx/BzwQtpuAF2L7toSyprCdXt7jnBD0dgKjclxrFNAZC5Dxa/UiaZakVkmt27dvL7pxSeSzpTvnKkHJkiQkPQi8IcOuK8zsvnDMFcBe4Eep0zIcbznK+3JOrmv13mG2EFgI0NzcnPW4SuKzpTvnKkHJApSZvS/X/pC08EHgzDBsB1Fv5rjYYWOAraF8TIby+DlbJA0FjgI6Qvnpaec8DLwENEoaGnpR8WvVDJ8t3TmXdOXK4psBfBU418x2x3YtBWaGzLxxRMkQj5vZNmCXpCnhHtIngPti56Qy9C4AVoaAtxyYJmlESI6YBiwP+x4KxxLOTV3LOedcQpTrOaibgEOAFSFbfJWZfcbM1ku6G3iaaOjvcjNL5UNfBtwCNBDds0rdt/oBcLukTUQ9p5kAZtYh6WrgiXDcN8ysI2x/FbhL0jVAW7iGc865BNHB0TWXT3Nzs7W2tpa7Gs45V3EkrTaz5mLOSUIWn3POOdeLByjnnHOJ5EN8RZC0HfhDEaccTZQ1WGtqsd212GbwdteS/rb5jWY2upgTPECVkKTWYsdcq0EttrsW2wze7nLXYzCVo80+xOeccy6RPEA555xLJA9QpbWw3BUok1psdy22GbzdtWTQ2+z3oJxzziWS96Ccc84lkgco55xzieQBqkQkzQjL1m+SNKfc9clG0g8lvSjpqVjZSEkrJG0MP0fE9s0NbdogaXqsfLKkdWHfjWFSX8LEv4tC+WOSxsbOuSR8xsYwu32qfFw4dmM4d9gAt/k4SQ9J+p2k9ZK+UO3tlnSopMclrQ1tvqra25zW/jpJbZJ+VivtlrQ51HeNpNaKbLeZ+WuAX0Ad8CzwJmAYsBY4sdz1ylLX9wDvAJ6KlX0TmBO25wDXhe0TQ1sOAcaFNtaFfY8D7yJab+sB4KxQ/lngu2F7JrAobI8Efh9+jgjbI8K+u4GZYfu7wGUD3OZjgHeE7SOA/w5tq9p2h/odHrbrgceAKdXc5rT2fxn4MfCzWvhvPFx3M3B0WllFtbvsX5DV+Ar/mMtj7+cCc8tdrxz1HUvPALUBOCZsHwNsyNQOoiVN3hWOeSZW/hHge/FjwvZQoifRFT8m7PteKFM4Zmim32WJ2n8f8P5aaTcwHPgtcFottJlozbdfAmdwMEDVQrs30ztAVVS7fYivNLItN18pXm/RGlyEn68L5dna1RS208t7nGPRApE7gVE5rjUK6AzHpl9rwIVhiUlEPYqqbncY5loDvAisMLOqb3NwA/BPwP5YWS2024BfSFotaVYoq6h2l2s9qGpX1LLyFSRbu3K1t9hzBu13J+lw4B7gi2b2lzC0nvHQLHWqqHZbtLbaREmNwE8lvT3H4VXRZkkfBF40s9WSTi/klCx1qqh2B1PNbKuk1xGtvfdMjmMT2W7vQZVGtqXrK8WfJB0DEH6+GMqztWtL2E4v73GOpKHAUUQLS2a71ktAYzg2/VoDRlI9UXD6kZndG4qrvt0AZtYJPAzMoPrbPBU4V9Jm4C7gDEl3UP3txsy2hp8vAj8FTqXS2j3Q457+OjAe+3uim42pJIkJ5a5XjvqOpec9qAX0vJH6zbA9gZ43Un/PwRupTxDddE/dSD07lF9Ozxupd4ftkcBzRDdRR4TtkWHfT+h5I/WzA9xeAbcBN6SVV227gdFAY9huAP4T+GA1tznD7+B0Dt6Dqup2A4cBR8S2/4voD5KKanfZvxyr9QWcTZQd9ixwRbnrk6OedwLbgG6iv3wuJRor/iWwMfwcGTv+itCmDYRsnlDeDDwV9t3EwVlKDg3/UW4iygZ6U+ycvwvlm4BPxcrfFI7dFM49ZIDb/G6ioYUngTXhdXY1txs4GWgLbX4K+Hoor9o2Z/gdnM7BAFXV7Q7XXxte6wnfQZXWbp/qyDnnXCL5PSjnnHOJ5AHKOedcInmAcs45l0geoJxzziWSByjnnHOJ5AHKuUEm6fTYrNrnKsds95IaJX22D58xT9I/9qeeA3kd5/rCA5RzA0RSXbHnmNlSM5uf45BGolmjnas5HqCcy0PSWEnPSLpV0pOSFksaHvZtlvR1Sb8BLpQ0TdKjkn4r6Sdhvr/U+mDPhOPOj137k5JuCtuvl/RTRWs2rZX0V8B84M1hTZ8F4bjZkp4Idbkqdq0rwlo+DwLjM7TjqFDfIeH9cEkvSKqX9OlwzbWS7km1L+38hyU1h+2jw/RBqUloF8Tq9A+h/BhJvw51f0rSXw/Ev4erHR6gnCvMeGChmZ0M/IWevZpXzezdwIPA14D3mdk7gFbgy5IOBb4PnAP8NfCGLJ9xI/ArMzuFaI2u9UTT0TxrZhPNbLakacAJRPOqTQQmS3qPpMlE081MIgqA70y/uJntJJpZ4G9C0TlEyx10A/ea2TvDZ/+OaEaRQl0K7DSzd4bP/bSkccBHw/UnAqcQzdjhXMF8NnPnCvOCmT0Stu8APg98K7xfFH5OIVr47ZEwM/ow4FHgbcBzZrYRIExWmlr+IO4M4BNwYObxnYqteBpMC6+28P5wooB1BPBTM9sdPmNplnYsAi4CHiIKaN8J5W+XdA3RkOLhRGv9FGoacLKkC8L7o0KdngB+GCbmXWJma4q4pnMeoJwrUPqcYPH3r4SfIlpn6SPxAyVNzHB+Xwm41sy+l/YZXyzwM5YC10oaCUwGVobyW4AWM1sr6ZNE89al28vBUZdD0+r0P82sV1CT9B7gA8DtkhaY2W0F1NE5wIf4nCvU8ZLeFbY/AvwmwzGrgKmS3gIH7vG8FXgGGCfpzbHzM/klcFk4t07SkcAuot5RynLg72L3tprCej+/Bj4kqUHSEUTDd72Y2ctEk3V+m2ji1H1h1xHAttDbuThL/TYTBTWAC2Lly4HLwrlIequkwyS9kWgtpu8DPyAatnSuYB6gnCvM74BLJD1JtJzAzekHmNl24JPAneG4VcDbzOxVoiG9ZSFJ4g9ZPuMLwHslrQNWEy3R8meiIcOnQg/kF8CPgUfDcYuJllX4LdHw3Rqida7+M0dbFgEf4+DQJMD/IlpVeAVRQM3kW0SB6L+Ao2Pl/w48DfxW0lNES3wPJeqFrZHUBnyYKCg6VzCfzdy5PBQtC/8zM8u1Aq1zboB5D8o551wieQ/KOedcInkPyjnnXCJ5gHLOOZdIHqCcc84lkgco55xzieQByjnnXCL9fw3oreEZhbIMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actualPrices = y_test\n",
    "predictedPrices = y_pred\n",
    "#actualPrices2 = np.reshape(actualPrices, (610,))\n",
    "print(actualPrices.shape)\n",
    "#predictedPrices = np.reshape(sc_y.inverse_transform(regressor.predict(X)), (610,))\n",
    "print(predictedPrices.shape)\n",
    "#print(actualPrices2)\n",
    "print(predictedPrices)\n",
    "residuals = []\n",
    "for item1, item2 in zip(actualPrices.tolist(), predictedPrices.tolist()):\n",
    "    residuals.append(item1 - item2)\n",
    "#rediduals = actualPrices - predictedPrices\n",
    "#print(residuals.shape)\n",
    "print(len(residuals))\n",
    "#print(residuals)\n",
    "plt.scatter(predictedPrices, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('residuals')\n",
    "plt.title('Residuals Plot: XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../Images/xgBoostResiduals.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72789415, 0.79646951, 0.78322253, 0.66847488, 0.81708377,\n",
       "       0.64730498, 0.83206918, 0.74545147, 0.74528407, 0.65285402,\n",
       "       0.82182906, 0.82573398, 0.71005051, 0.67008886, 0.74246874,\n",
       "       0.74166955, 0.79234545, 0.79830372, 0.68066537, 0.81018177,\n",
       "       0.69741091, 0.82411455, 0.75275527, 0.74636393, 0.70789594,\n",
       "       0.76774402, 0.80859486, 0.81228216, 0.76080524, 0.70005627])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=3 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=3, total=   1.5s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=3 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=3, total=   1.7s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=3, total=   1.3s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=3, total=   1.6s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=3, total=   2.9s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=6, total=   3.2s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=6, total=   3.5s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=6, total=   3.2s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=6, total=   3.4s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=6, total=   2.1s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=9, total=   3.0s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=9, total=   4.8s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=9, total=   5.4s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=9, total=   6.4s\n",
      "[CV] booster=gbtree, eta=0.1, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.1, max_depth=9, total=   6.7s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=3, total=   3.0s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=3, total=   2.8s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=3, total=   2.5s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=3, total=   1.7s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=3, total=   2.1s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=6, total=   3.3s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=6, total=   2.6s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=6, total=   1.8s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=6, total=   1.7s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=6, total=   1.7s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=9, total=   2.1s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=9, total=   2.4s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=9, total=   2.6s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=9, total=   2.5s\n",
      "[CV] booster=gbtree, eta=0.3, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.3, max_depth=9, total=   2.3s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=3, total=   1.1s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=3, total=   1.0s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=3, total=   1.1s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=3, total=   1.2s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=3 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=3, total=   1.1s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=6, total=   1.6s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=6, total=   1.7s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=6, total=   1.7s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=6, total=   1.8s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=6 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=6, total=   1.8s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=9, total=   2.2s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=9, total=   4.3s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=9, total=   5.8s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=9, total=   2.5s\n",
      "[CV] booster=gbtree, eta=0.5, max_depth=9 ............................\n",
      "[CV] ............. booster=gbtree, eta=0.5, max_depth=9, total=   2.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=3 ..........................\n",
      "[21:57:06] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=3 ..........................\n",
      "[21:57:06] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=3 ..........................\n",
      "[21:57:07] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=3, total=   0.4s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=3 ..........................\n",
      "[21:57:07] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=3 ..........................\n",
      "[21:57:08] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=6 ..........................\n",
      "[21:57:08] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=6 ..........................\n",
      "[21:57:09] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=6 ..........................\n",
      "[21:57:09] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=6 ..........................\n",
      "[21:57:10] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=6 ..........................\n",
      "[21:57:11] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=9 ..........................\n",
      "[21:57:11] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=9 ..........................\n",
      "[21:57:12] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=9 ..........................\n",
      "[21:57:12] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=9 ..........................\n",
      "[21:57:13] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=9, total=   0.6s\n",
      "[CV] booster=gblinear, eta=0.1, max_depth=9 ..........................\n",
      "[21:57:14] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.1, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=3 ..........................\n",
      "[21:57:14] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=3 ..........................\n",
      "[21:57:15] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=3 ..........................\n",
      "[21:57:15] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=3 ..........................\n",
      "[21:57:16] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=3 ..........................\n",
      "[21:57:16] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=6 ..........................\n",
      "[21:57:17] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=6, total=   0.6s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=6 ..........................\n",
      "[21:57:18] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=6 ..........................\n",
      "[21:57:18] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=6 ..........................\n",
      "[21:57:19] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=6 ..........................\n",
      "[21:57:19] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=9 ..........................\n",
      "[21:57:20] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=9 ..........................\n",
      "[21:57:21] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=9 ..........................\n",
      "[21:57:21] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=9 ..........................\n",
      "[21:57:22] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.3, max_depth=9 ..........................\n",
      "[21:57:22] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.3, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=3 ..........................\n",
      "[21:57:23] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=3 ..........................\n",
      "[21:57:23] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=3 ..........................\n",
      "[21:57:24] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=3, total=   0.6s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=3 ..........................\n",
      "[21:57:25] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=3 ..........................\n",
      "[21:57:25] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=3, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=6 ..........................\n",
      "[21:57:26] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=6 ..........................\n",
      "[21:57:26] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=6 ..........................\n",
      "[21:57:27] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=6 ..........................\n",
      "[21:57:28] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=6, total=   0.6s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=6 ..........................\n",
      "[21:57:28] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=6, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=9 ..........................\n",
      "[21:57:29] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=9 ..........................\n",
      "[21:57:29] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=9 ..........................\n",
      "[21:57:30] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=9, total=   0.6s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=9 ..........................\n",
      "[21:57:31] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=9, total=   0.5s\n",
      "[CV] booster=gblinear, eta=0.5, max_depth=9 ..........................\n",
      "[21:57:31] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[CV] ........... booster=gblinear, eta=0.5, max_depth=9, total=   0.7s\n",
      "[CV] booster=dart, eta=0.1, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=3, total=   4.9s\n",
      "[CV] booster=dart, eta=0.1, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=3, total=   4.3s\n",
      "[CV] booster=dart, eta=0.1, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=3, total=   4.8s\n",
      "[CV] booster=dart, eta=0.1, max_depth=3 ..............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... booster=dart, eta=0.1, max_depth=3, total=   6.3s\n",
      "[CV] booster=dart, eta=0.1, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=3, total=   6.9s\n",
      "[CV] booster=dart, eta=0.1, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=6, total=   9.4s\n",
      "[CV] booster=dart, eta=0.1, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=6, total=   5.6s\n",
      "[CV] booster=dart, eta=0.1, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=6, total=   5.5s\n",
      "[CV] booster=dart, eta=0.1, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=6, total=   5.6s\n",
      "[CV] booster=dart, eta=0.1, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=6, total=   5.6s\n",
      "[CV] booster=dart, eta=0.1, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=9, total=   7.3s\n",
      "[CV] booster=dart, eta=0.1, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=9, total=   6.2s\n",
      "[CV] booster=dart, eta=0.1, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=9, total=   6.3s\n",
      "[CV] booster=dart, eta=0.1, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=9, total=   6.0s\n",
      "[CV] booster=dart, eta=0.1, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.1, max_depth=9, total=   6.4s\n",
      "[CV] booster=dart, eta=0.3, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=3, total=   4.8s\n",
      "[CV] booster=dart, eta=0.3, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=3, total=   4.9s\n",
      "[CV] booster=dart, eta=0.3, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=3, total=   4.7s\n",
      "[CV] booster=dart, eta=0.3, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=3, total=   4.9s\n",
      "[CV] booster=dart, eta=0.3, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=3, total=   4.8s\n",
      "[CV] booster=dart, eta=0.3, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=6, total=   5.6s\n",
      "[CV] booster=dart, eta=0.3, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=6, total=   5.6s\n",
      "[CV] booster=dart, eta=0.3, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=6, total=   7.1s\n",
      "[CV] booster=dart, eta=0.3, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=6, total=   8.3s\n",
      "[CV] booster=dart, eta=0.3, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=6, total=   9.5s\n",
      "[CV] booster=dart, eta=0.3, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=9, total=   6.3s\n",
      "[CV] booster=dart, eta=0.3, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=9, total=   5.9s\n",
      "[CV] booster=dart, eta=0.3, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=9, total=   6.2s\n",
      "[CV] booster=dart, eta=0.3, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=9, total=   5.9s\n",
      "[CV] booster=dart, eta=0.3, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.3, max_depth=9, total=   5.9s\n",
      "[CV] booster=dart, eta=0.5, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=3, total=   4.6s\n",
      "[CV] booster=dart, eta=0.5, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=3, total=   4.7s\n",
      "[CV] booster=dart, eta=0.5, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=3, total=   4.7s\n",
      "[CV] booster=dart, eta=0.5, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=3, total=   4.7s\n",
      "[CV] booster=dart, eta=0.5, max_depth=3 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=3, total=   4.7s\n",
      "[CV] booster=dart, eta=0.5, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=6, total=   5.2s\n",
      "[CV] booster=dart, eta=0.5, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=6, total=   5.2s\n",
      "[CV] booster=dart, eta=0.5, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=6, total=   5.1s\n",
      "[CV] booster=dart, eta=0.5, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=6, total=   5.2s\n",
      "[CV] booster=dart, eta=0.5, max_depth=6 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=6, total=   5.4s\n",
      "[CV] booster=dart, eta=0.5, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=9, total=   6.1s\n",
      "[CV] booster=dart, eta=0.5, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=9, total=   5.9s\n",
      "[CV] booster=dart, eta=0.5, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=9, total=   6.2s\n",
      "[CV] booster=dart, eta=0.5, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=9, total=   5.9s\n",
      "[CV] booster=dart, eta=0.5, max_depth=9 ..............................\n",
      "[CV] ............... booster=dart, eta=0.5, max_depth=9, total=   5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.77927341, 3.05423899, 5.21527419, 2.37384133, 2.2018405 ,\n",
       "        2.33740382, 1.082091  , 1.69360714, 3.42852726, 0.43018994,\n",
       "        0.48235688, 0.50257535, 0.485712  , 0.49552336, 0.49381409,\n",
       "        0.50098324, 0.5009923 , 0.52896886, 5.04808798, 6.01168261,\n",
       "        6.12181668, 4.50238037, 6.7622592 , 5.75355191, 4.38592463,\n",
       "        4.93230348, 5.71629071]),\n",
       " 'std_fit_time': array([0.57896674, 0.52373237, 1.30252747, 0.47341682, 0.62709308,\n",
       "        0.17527939, 0.04924335, 0.09343235, 1.38746154, 0.04073115,\n",
       "        0.01986129, 0.00773961, 0.01560711, 0.00834877, 0.00759194,\n",
       "        0.00678189, 0.00539736, 0.05161715, 0.84851709, 1.53461208,\n",
       "        0.45277657, 0.08735878, 1.47166189, 0.19213958, 0.04913504,\n",
       "        0.07949223, 0.14810061]),\n",
       " 'mean_score_time': array([0.0372282 , 0.04096465, 0.04461837, 0.04605217, 0.03176513,\n",
       "        0.0325922 , 0.03271575, 0.03081164, 0.03277693, 0.03729005,\n",
       "        0.04076753, 0.04141335, 0.03635058, 0.03742127, 0.03969183,\n",
       "        0.04031944, 0.04221516, 0.03892355, 0.38920202, 0.3324163 ,\n",
       "        0.32650142, 0.31474247, 0.46813617, 0.28804054, 0.29627585,\n",
       "        0.28548236, 0.28469663]),\n",
       " 'std_score_time': array([0.01077618, 0.00668034, 0.0073479 , 0.00523978, 0.0054471 ,\n",
       "        0.00522442, 0.0040801 , 0.00431828, 0.00720121, 0.00309194,\n",
       "        0.0058371 , 0.00247902, 0.00562707, 0.00548295, 0.00688606,\n",
       "        0.00477391, 0.00589135, 0.0060827 , 0.18476407, 0.0129538 ,\n",
       "        0.00232989, 0.01225535, 0.12183004, 0.01024303, 0.01099607,\n",
       "        0.01865002, 0.01810493]),\n",
       " 'param_booster': masked_array(data=['gbtree', 'gbtree', 'gbtree', 'gbtree', 'gbtree',\n",
       "                    'gbtree', 'gbtree', 'gbtree', 'gbtree', 'gblinear',\n",
       "                    'gblinear', 'gblinear', 'gblinear', 'gblinear',\n",
       "                    'gblinear', 'gblinear', 'gblinear', 'gblinear', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_eta': masked_array(data=[0.1, 0.1, 0.1, 0.3, 0.3, 0.3, 0.5, 0.5, 0.5, 0.1, 0.1,\n",
       "                    0.1, 0.3, 0.3, 0.3, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1, 0.3,\n",
       "                    0.3, 0.3, 0.5, 0.5, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 6, 9, 3, 6, 9, 3, 6, 9, 3, 6, 9, 3, 6, 9, 3, 6, 9,\n",
       "                    3, 6, 9, 3, 6, 9, 3, 6, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'booster': 'gbtree', 'eta': 0.1, 'max_depth': 3},\n",
       "  {'booster': 'gbtree', 'eta': 0.1, 'max_depth': 6},\n",
       "  {'booster': 'gbtree', 'eta': 0.1, 'max_depth': 9},\n",
       "  {'booster': 'gbtree', 'eta': 0.3, 'max_depth': 3},\n",
       "  {'booster': 'gbtree', 'eta': 0.3, 'max_depth': 6},\n",
       "  {'booster': 'gbtree', 'eta': 0.3, 'max_depth': 9},\n",
       "  {'booster': 'gbtree', 'eta': 0.5, 'max_depth': 3},\n",
       "  {'booster': 'gbtree', 'eta': 0.5, 'max_depth': 6},\n",
       "  {'booster': 'gbtree', 'eta': 0.5, 'max_depth': 9},\n",
       "  {'booster': 'gblinear', 'eta': 0.1, 'max_depth': 3},\n",
       "  {'booster': 'gblinear', 'eta': 0.1, 'max_depth': 6},\n",
       "  {'booster': 'gblinear', 'eta': 0.1, 'max_depth': 9},\n",
       "  {'booster': 'gblinear', 'eta': 0.3, 'max_depth': 3},\n",
       "  {'booster': 'gblinear', 'eta': 0.3, 'max_depth': 6},\n",
       "  {'booster': 'gblinear', 'eta': 0.3, 'max_depth': 9},\n",
       "  {'booster': 'gblinear', 'eta': 0.5, 'max_depth': 3},\n",
       "  {'booster': 'gblinear', 'eta': 0.5, 'max_depth': 6},\n",
       "  {'booster': 'gblinear', 'eta': 0.5, 'max_depth': 9},\n",
       "  {'booster': 'dart', 'eta': 0.1, 'max_depth': 3},\n",
       "  {'booster': 'dart', 'eta': 0.1, 'max_depth': 6},\n",
       "  {'booster': 'dart', 'eta': 0.1, 'max_depth': 9},\n",
       "  {'booster': 'dart', 'eta': 0.3, 'max_depth': 3},\n",
       "  {'booster': 'dart', 'eta': 0.3, 'max_depth': 6},\n",
       "  {'booster': 'dart', 'eta': 0.3, 'max_depth': 9},\n",
       "  {'booster': 'dart', 'eta': 0.5, 'max_depth': 3},\n",
       "  {'booster': 'dart', 'eta': 0.5, 'max_depth': 6},\n",
       "  {'booster': 'dart', 'eta': 0.5, 'max_depth': 9}],\n",
       " 'split0_test_score': array([0.71861746, 0.59588129, 0.58438411, 0.74001378, 0.63673633,\n",
       "        0.52973231, 0.80117171, 0.60934962, 0.57347477, 0.62793547,\n",
       "        0.62804966, 0.62804996, 0.66273044, 0.66276206, 0.66276083,\n",
       "        0.66829293, 0.66829101, 0.66829826, 0.71861746, 0.59588129,\n",
       "        0.58438411, 0.74001378, 0.63673633, 0.52973231, 0.80117171,\n",
       "        0.60934962, 0.57347477]),\n",
       " 'split1_test_score': array([0.70561627, 0.62585502, 0.60140134, 0.65736397, 0.58483555,\n",
       "        0.5878546 , 0.63600459, 0.50844235, 0.56055305, 0.61943091,\n",
       "        0.61936366, 0.61936576, 0.61936011, 0.6193514 , 0.61929465,\n",
       "        0.61908084, 0.61909321, 0.61906104, 0.70561627, 0.62585502,\n",
       "        0.60140134, 0.65736397, 0.58483555, 0.58785459, 0.63600459,\n",
       "        0.50844235, 0.56055305]),\n",
       " 'split2_test_score': array([0.70744894, 0.58430836, 0.52085667, 0.70655444, 0.60648579,\n",
       "        0.55614267, 0.67183268, 0.55856159, 0.48277246, 0.54303908,\n",
       "        0.54295306, 0.54296345, 0.5289683 , 0.52899831, 0.5290867 ,\n",
       "        0.51984866, 0.51989954, 0.5198539 , 0.70744894, 0.58430836,\n",
       "        0.52085667, 0.70655444, 0.60648579, 0.55614267, 0.67183268,\n",
       "        0.55856159, 0.48277246]),\n",
       " 'split3_test_score': array([0.68616639, 0.59248898, 0.52099879, 0.63980708, 0.5747786 ,\n",
       "        0.54417822, 0.66453529, 0.59551561, 0.56310984, 0.76032398,\n",
       "        0.76029072, 0.76030385, 0.78223898, 0.78221357, 0.78220196,\n",
       "        0.78397525, 0.78396998, 0.78405681, 0.68616639, 0.59248898,\n",
       "        0.52099879, 0.63980708, 0.5747786 , 0.54417822, 0.66453529,\n",
       "        0.59551561, 0.56310984]),\n",
       " 'split4_test_score': array([0.54834814, 0.33272481, 0.37386724, 0.55756549, 0.42031645,\n",
       "        0.43811561, 0.55829677, 0.48028032, 0.48718474, 0.5752085 ,\n",
       "        0.57520033, 0.57520044, 0.57787795, 0.57768837, 0.57772052,\n",
       "        0.57829747, 0.57849169, 0.57492927, 0.54834814, 0.33272481,\n",
       "        0.37386724, 0.55756549, 0.42031645, 0.43811561, 0.55829677,\n",
       "        0.48028032, 0.48718474]),\n",
       " 'mean_test_score': array([0.67323944, 0.54625169, 0.52030163, 0.66026095, 0.56463054,\n",
       "        0.53120468, 0.66636821, 0.5504299 , 0.53341897, 0.62518759,\n",
       "        0.62517149, 0.62517669, 0.63423516, 0.63420274, 0.63421293,\n",
       "        0.63389903, 0.63394909, 0.63323986, 0.67323944, 0.54625169,\n",
       "        0.52030163, 0.66026095, 0.56463054, 0.53120468, 0.66636821,\n",
       "        0.5504299 , 0.53341897]),\n",
       " 'std_test_score': array([0.06331296, 0.10768446, 0.08015928, 0.06241268, 0.0752182 ,\n",
       "        0.05033323, 0.07849406, 0.04950616, 0.03981192, 0.07422779,\n",
       "        0.07423774, 0.07424017, 0.08625036, 0.08626152, 0.08623365,\n",
       "        0.08947001, 0.08943059, 0.08992484, 0.06331296, 0.10768446,\n",
       "        0.08015928, 0.06241268, 0.0752182 , 0.05033323, 0.07849406,\n",
       "        0.04950616, 0.03981192]),\n",
       " 'rank_test_score': array([ 1, 20, 26,  5, 16, 24,  3, 18, 22, 13, 15, 14,  7,  9,  8, 11, 10,\n",
       "        12,  1, 20, 26,  5, 16, 25,  3, 18, 22], dtype=int32),\n",
       " 'split0_train_score': array([0.92350073, 0.98576519, 0.99826109, 0.98013422, 0.99975645,\n",
       "        0.99999999, 0.99255306, 0.99999927, 1.        , 0.81062945,\n",
       "        0.81062731, 0.81062945, 0.81556825, 0.81556884, 0.81556868,\n",
       "        0.81590709, 0.81590725, 0.81590692, 0.92350073, 0.98576519,\n",
       "        0.99826109, 0.98013422, 0.99975645, 0.99999999, 0.99255306,\n",
       "        0.99999927, 1.        ]),\n",
       " 'split1_train_score': array([0.94565158, 0.99265913, 0.99888217, 0.98412328, 0.99991843,\n",
       "        0.99999998, 0.99407569, 0.99999971, 1.        , 0.80901082,\n",
       "        0.80902412, 0.80901925, 0.81616925, 0.81616958, 0.81616914,\n",
       "        0.81655943, 0.81655912, 0.81655974, 0.94565158, 0.99265913,\n",
       "        0.99888217, 0.98412328, 0.99991843, 0.99999998, 0.99407569,\n",
       "        0.99999971, 1.        ]),\n",
       " 'split2_train_score': array([0.94475128, 0.99295706, 0.99941155, 0.9842032 , 0.99993413,\n",
       "        0.99999998, 0.99362369, 0.99999895, 1.        , 0.81744662,\n",
       "        0.8174435 , 0.81744601, 0.82625199, 0.82625209, 0.82625071,\n",
       "        0.82689516, 0.82689555, 0.82689489, 0.94475128, 0.99295706,\n",
       "        0.99941155, 0.9842032 , 0.99993413, 0.99999998, 0.99362369,\n",
       "        0.99999895, 1.        ]),\n",
       " 'split3_train_score': array([0.94747873, 0.99166842, 0.9993758 , 0.98291544, 0.99984621,\n",
       "        0.99999998, 0.9933366 , 0.99999896, 1.        , 0.79095974,\n",
       "        0.79096177, 0.79096172, 0.79703695, 0.79703696, 0.79703798,\n",
       "        0.79745565, 0.79745609, 0.79745548, 0.94747873, 0.99166842,\n",
       "        0.9993758 , 0.98291544, 0.99984621, 0.99999998, 0.9933366 ,\n",
       "        0.99999896, 1.        ]),\n",
       " 'split4_train_score': array([0.93964212, 0.99240384, 0.99920169, 0.98302101, 0.99989976,\n",
       "        0.99999996, 0.99419992, 0.9999994 , 1.        , 0.74383937,\n",
       "        0.74383597, 0.74383209, 0.75500537, 0.75500707, 0.75500395,\n",
       "        0.75621246, 0.75621236, 0.75621415, 0.93964212, 0.99240384,\n",
       "        0.99920169, 0.98302101, 0.99989976, 0.99999996, 0.99419992,\n",
       "        0.9999994 , 1.        ]),\n",
       " 'mean_train_score': array([0.94020489, 0.99109073, 0.99902646, 0.98287943, 0.999871  ,\n",
       "        0.99999998, 0.99355779, 0.99999926, 1.        , 0.7943772 ,\n",
       "        0.79437853, 0.7943777 , 0.80200636, 0.80200691, 0.80200609,\n",
       "        0.80260596, 0.80260607, 0.80260624, 0.94020489, 0.99109073,\n",
       "        0.99902646, 0.98287943, 0.999871  , 0.99999998, 0.99355779,\n",
       "        0.99999926, 1.        ]),\n",
       " 'std_train_score': array([8.74746233e-03, 2.69675577e-03, 4.26065469e-04, 1.47356967e-03,\n",
       "        6.44945486e-05, 7.76821874e-09, 5.90474756e-04, 2.86814164e-07,\n",
       "        3.44860285e-13, 2.67440752e-02, 2.67459646e-02, 2.67475940e-02,\n",
       "        2.53225240e-02, 2.53220111e-02, 2.53228006e-02, 2.50617931e-02,\n",
       "        2.50618709e-02, 2.50611364e-02, 8.74746233e-03, 2.69675577e-03,\n",
       "        4.26065469e-04, 1.47356967e-03, 6.44945486e-05, 7.76830336e-09,\n",
       "        5.90474756e-04, 2.86814164e-07, 3.44860285e-13])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid Search CV\n",
    "#now lets try to find the best parameters for decision trees\n",
    "#this takes a pretty long time to run, so gave faster option below\n",
    "sample = dfpdForLinearRegression.sample(frac=0.01, replace=False, random_state=1)\n",
    "X = sample.drop(['SoldPrice'], axis=1)\n",
    "y = sample['SoldPrice']\n",
    "#y = dfpdForDecisionTree['SoldPrice']\n",
    "#print(X.head(2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)\n",
    "\n",
    "model = XGBRegressor()\n",
    "#grid = {'n_estimators': [50,300,500,700], 'max_leaf_nodes': [5, 15, 25, 100], 'ccp_alpha': [0, 0.01, 0.1, 1, 10], 'criterion' : ['gini', 'entropy'], \"min_impurity_split\":[0,0.1,0.3,0.5], \"n_jobs\": [4]}\n",
    "grid = {'booster': ['gbtree', 'gblinear', 'dart'], 'eta': [0.1, 0.3, 0.5], \"max_depth\":[3, 6, 9]}\n",
    "grid_search_cv = GridSearchCV(estimator = model, param_grid= grid, cv =5, return_train_score= True, verbose= 2)\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "grid_search_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>18</th>\n",
       "      <th>24</th>\n",
       "      <th>6</th>\n",
       "      <th>3</th>\n",
       "      <th>21</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>13</th>\n",
       "      <th>16</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>10</th>\n",
       "      <th>4</th>\n",
       "      <th>22</th>\n",
       "      <th>25</th>\n",
       "      <th>7</th>\n",
       "      <th>1</th>\n",
       "      <th>19</th>\n",
       "      <th>26</th>\n",
       "      <th>8</th>\n",
       "      <th>5</th>\n",
       "      <th>23</th>\n",
       "      <th>20</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>1.77927</td>\n",
       "      <td>5.04809</td>\n",
       "      <td>4.38592</td>\n",
       "      <td>1.08209</td>\n",
       "      <td>2.37384</td>\n",
       "      <td>4.50238</td>\n",
       "      <td>0.485712</td>\n",
       "      <td>0.493814</td>\n",
       "      <td>0.495523</td>\n",
       "      <td>0.500992</td>\n",
       "      <td>0.500983</td>\n",
       "      <td>0.528969</td>\n",
       "      <td>0.43019</td>\n",
       "      <td>0.502575</td>\n",
       "      <td>0.482357</td>\n",
       "      <td>2.20184</td>\n",
       "      <td>6.76226</td>\n",
       "      <td>4.9323</td>\n",
       "      <td>1.69361</td>\n",
       "      <td>3.05424</td>\n",
       "      <td>6.01168</td>\n",
       "      <td>5.71629</td>\n",
       "      <td>3.42853</td>\n",
       "      <td>2.3374</td>\n",
       "      <td>5.75355</td>\n",
       "      <td>6.12182</td>\n",
       "      <td>5.21527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.578967</td>\n",
       "      <td>0.848517</td>\n",
       "      <td>0.049135</td>\n",
       "      <td>0.0492433</td>\n",
       "      <td>0.473417</td>\n",
       "      <td>0.0873588</td>\n",
       "      <td>0.0156071</td>\n",
       "      <td>0.00759194</td>\n",
       "      <td>0.00834877</td>\n",
       "      <td>0.00539736</td>\n",
       "      <td>0.00678189</td>\n",
       "      <td>0.0516172</td>\n",
       "      <td>0.0407312</td>\n",
       "      <td>0.00773961</td>\n",
       "      <td>0.0198613</td>\n",
       "      <td>0.627093</td>\n",
       "      <td>1.47166</td>\n",
       "      <td>0.0794922</td>\n",
       "      <td>0.0934324</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>1.53461</td>\n",
       "      <td>0.148101</td>\n",
       "      <td>1.38746</td>\n",
       "      <td>0.175279</td>\n",
       "      <td>0.19214</td>\n",
       "      <td>0.452777</td>\n",
       "      <td>1.30253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.0372282</td>\n",
       "      <td>0.389202</td>\n",
       "      <td>0.296276</td>\n",
       "      <td>0.0327157</td>\n",
       "      <td>0.0460522</td>\n",
       "      <td>0.314742</td>\n",
       "      <td>0.0363506</td>\n",
       "      <td>0.0396918</td>\n",
       "      <td>0.0374213</td>\n",
       "      <td>0.0422152</td>\n",
       "      <td>0.0403194</td>\n",
       "      <td>0.0389235</td>\n",
       "      <td>0.03729</td>\n",
       "      <td>0.0414134</td>\n",
       "      <td>0.0407675</td>\n",
       "      <td>0.0317651</td>\n",
       "      <td>0.468136</td>\n",
       "      <td>0.285482</td>\n",
       "      <td>0.0308116</td>\n",
       "      <td>0.0409647</td>\n",
       "      <td>0.332416</td>\n",
       "      <td>0.284697</td>\n",
       "      <td>0.0327769</td>\n",
       "      <td>0.0325922</td>\n",
       "      <td>0.288041</td>\n",
       "      <td>0.326501</td>\n",
       "      <td>0.0446184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.0107762</td>\n",
       "      <td>0.184764</td>\n",
       "      <td>0.0109961</td>\n",
       "      <td>0.0040801</td>\n",
       "      <td>0.00523978</td>\n",
       "      <td>0.0122553</td>\n",
       "      <td>0.00562707</td>\n",
       "      <td>0.00688606</td>\n",
       "      <td>0.00548295</td>\n",
       "      <td>0.00589135</td>\n",
       "      <td>0.00477391</td>\n",
       "      <td>0.0060827</td>\n",
       "      <td>0.00309194</td>\n",
       "      <td>0.00247902</td>\n",
       "      <td>0.0058371</td>\n",
       "      <td>0.0054471</td>\n",
       "      <td>0.12183</td>\n",
       "      <td>0.01865</td>\n",
       "      <td>0.00431828</td>\n",
       "      <td>0.00668034</td>\n",
       "      <td>0.0129538</td>\n",
       "      <td>0.0181049</td>\n",
       "      <td>0.00720121</td>\n",
       "      <td>0.00522442</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.00232989</td>\n",
       "      <td>0.0073479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_booster</th>\n",
       "      <td>gbtree</td>\n",
       "      <td>dart</td>\n",
       "      <td>dart</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>dart</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>dart</td>\n",
       "      <td>dart</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>dart</td>\n",
       "      <td>dart</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>dart</td>\n",
       "      <td>dart</td>\n",
       "      <td>gbtree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_eta</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_max_depth</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.1, 'max_depth': 3}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.1, 'max_depth': 3}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.5, 'max_depth': 3}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.5, 'max_depth': 3}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.3, 'max_depth': 3}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.3, 'max_depth': 3}</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.3, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.3, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.3, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.5, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.5, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.5, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.1, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.1, 'max_depth...</td>\n",
       "      <td>{'booster': 'gblinear', 'eta': 0.1, 'max_depth...</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.3, 'max_depth': 6}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.3, 'max_depth': 6}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.5, 'max_depth': 6}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.5, 'max_depth': 6}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.1, 'max_depth': 6}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.1, 'max_depth': 6}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.5, 'max_depth': 9}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.5, 'max_depth': 9}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.3, 'max_depth': 9}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.3, 'max_depth': 9}</td>\n",
       "      <td>{'booster': 'dart', 'eta': 0.1, 'max_depth': 9}</td>\n",
       "      <td>{'booster': 'gbtree', 'eta': 0.1, 'max_depth': 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.718617</td>\n",
       "      <td>0.718617</td>\n",
       "      <td>0.801172</td>\n",
       "      <td>0.801172</td>\n",
       "      <td>0.740014</td>\n",
       "      <td>0.740014</td>\n",
       "      <td>0.66273</td>\n",
       "      <td>0.662761</td>\n",
       "      <td>0.662762</td>\n",
       "      <td>0.668291</td>\n",
       "      <td>0.668293</td>\n",
       "      <td>0.668298</td>\n",
       "      <td>0.627935</td>\n",
       "      <td>0.62805</td>\n",
       "      <td>0.62805</td>\n",
       "      <td>0.636736</td>\n",
       "      <td>0.636736</td>\n",
       "      <td>0.60935</td>\n",
       "      <td>0.60935</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.573475</td>\n",
       "      <td>0.573475</td>\n",
       "      <td>0.529732</td>\n",
       "      <td>0.529732</td>\n",
       "      <td>0.584384</td>\n",
       "      <td>0.584384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.705616</td>\n",
       "      <td>0.705616</td>\n",
       "      <td>0.636005</td>\n",
       "      <td>0.636005</td>\n",
       "      <td>0.657364</td>\n",
       "      <td>0.657364</td>\n",
       "      <td>0.61936</td>\n",
       "      <td>0.619295</td>\n",
       "      <td>0.619351</td>\n",
       "      <td>0.619093</td>\n",
       "      <td>0.619081</td>\n",
       "      <td>0.619061</td>\n",
       "      <td>0.619431</td>\n",
       "      <td>0.619366</td>\n",
       "      <td>0.619364</td>\n",
       "      <td>0.584836</td>\n",
       "      <td>0.584836</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.625855</td>\n",
       "      <td>0.625855</td>\n",
       "      <td>0.560553</td>\n",
       "      <td>0.560553</td>\n",
       "      <td>0.587855</td>\n",
       "      <td>0.587855</td>\n",
       "      <td>0.601401</td>\n",
       "      <td>0.601401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.707449</td>\n",
       "      <td>0.707449</td>\n",
       "      <td>0.671833</td>\n",
       "      <td>0.671833</td>\n",
       "      <td>0.706554</td>\n",
       "      <td>0.706554</td>\n",
       "      <td>0.528968</td>\n",
       "      <td>0.529087</td>\n",
       "      <td>0.528998</td>\n",
       "      <td>0.5199</td>\n",
       "      <td>0.519849</td>\n",
       "      <td>0.519854</td>\n",
       "      <td>0.543039</td>\n",
       "      <td>0.542963</td>\n",
       "      <td>0.542953</td>\n",
       "      <td>0.606486</td>\n",
       "      <td>0.606486</td>\n",
       "      <td>0.558562</td>\n",
       "      <td>0.558562</td>\n",
       "      <td>0.584308</td>\n",
       "      <td>0.584308</td>\n",
       "      <td>0.482772</td>\n",
       "      <td>0.482772</td>\n",
       "      <td>0.556143</td>\n",
       "      <td>0.556143</td>\n",
       "      <td>0.520857</td>\n",
       "      <td>0.520857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.686166</td>\n",
       "      <td>0.686166</td>\n",
       "      <td>0.664535</td>\n",
       "      <td>0.664535</td>\n",
       "      <td>0.639807</td>\n",
       "      <td>0.639807</td>\n",
       "      <td>0.782239</td>\n",
       "      <td>0.782202</td>\n",
       "      <td>0.782214</td>\n",
       "      <td>0.78397</td>\n",
       "      <td>0.783975</td>\n",
       "      <td>0.784057</td>\n",
       "      <td>0.760324</td>\n",
       "      <td>0.760304</td>\n",
       "      <td>0.760291</td>\n",
       "      <td>0.574779</td>\n",
       "      <td>0.574779</td>\n",
       "      <td>0.595516</td>\n",
       "      <td>0.595516</td>\n",
       "      <td>0.592489</td>\n",
       "      <td>0.592489</td>\n",
       "      <td>0.56311</td>\n",
       "      <td>0.56311</td>\n",
       "      <td>0.544178</td>\n",
       "      <td>0.544178</td>\n",
       "      <td>0.520999</td>\n",
       "      <td>0.520999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.548348</td>\n",
       "      <td>0.548348</td>\n",
       "      <td>0.558297</td>\n",
       "      <td>0.558297</td>\n",
       "      <td>0.557565</td>\n",
       "      <td>0.557565</td>\n",
       "      <td>0.577878</td>\n",
       "      <td>0.577721</td>\n",
       "      <td>0.577688</td>\n",
       "      <td>0.578492</td>\n",
       "      <td>0.578297</td>\n",
       "      <td>0.574929</td>\n",
       "      <td>0.575208</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>0.420316</td>\n",
       "      <td>0.420316</td>\n",
       "      <td>0.48028</td>\n",
       "      <td>0.48028</td>\n",
       "      <td>0.332725</td>\n",
       "      <td>0.332725</td>\n",
       "      <td>0.487185</td>\n",
       "      <td>0.487185</td>\n",
       "      <td>0.438116</td>\n",
       "      <td>0.438116</td>\n",
       "      <td>0.373867</td>\n",
       "      <td>0.373867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.673239</td>\n",
       "      <td>0.673239</td>\n",
       "      <td>0.666368</td>\n",
       "      <td>0.666368</td>\n",
       "      <td>0.660261</td>\n",
       "      <td>0.660261</td>\n",
       "      <td>0.634235</td>\n",
       "      <td>0.634213</td>\n",
       "      <td>0.634203</td>\n",
       "      <td>0.633949</td>\n",
       "      <td>0.633899</td>\n",
       "      <td>0.63324</td>\n",
       "      <td>0.625188</td>\n",
       "      <td>0.625177</td>\n",
       "      <td>0.625171</td>\n",
       "      <td>0.564631</td>\n",
       "      <td>0.564631</td>\n",
       "      <td>0.55043</td>\n",
       "      <td>0.55043</td>\n",
       "      <td>0.546252</td>\n",
       "      <td>0.546252</td>\n",
       "      <td>0.533419</td>\n",
       "      <td>0.533419</td>\n",
       "      <td>0.531205</td>\n",
       "      <td>0.531205</td>\n",
       "      <td>0.520302</td>\n",
       "      <td>0.520302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.063313</td>\n",
       "      <td>0.063313</td>\n",
       "      <td>0.0784941</td>\n",
       "      <td>0.0784941</td>\n",
       "      <td>0.0624127</td>\n",
       "      <td>0.0624127</td>\n",
       "      <td>0.0862504</td>\n",
       "      <td>0.0862336</td>\n",
       "      <td>0.0862615</td>\n",
       "      <td>0.0894306</td>\n",
       "      <td>0.08947</td>\n",
       "      <td>0.0899248</td>\n",
       "      <td>0.0742278</td>\n",
       "      <td>0.0742402</td>\n",
       "      <td>0.0742377</td>\n",
       "      <td>0.0752182</td>\n",
       "      <td>0.0752182</td>\n",
       "      <td>0.0495062</td>\n",
       "      <td>0.0495062</td>\n",
       "      <td>0.107684</td>\n",
       "      <td>0.107684</td>\n",
       "      <td>0.0398119</td>\n",
       "      <td>0.0398119</td>\n",
       "      <td>0.0503332</td>\n",
       "      <td>0.0503332</td>\n",
       "      <td>0.0801593</td>\n",
       "      <td>0.0801593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.923501</td>\n",
       "      <td>0.923501</td>\n",
       "      <td>0.992553</td>\n",
       "      <td>0.992553</td>\n",
       "      <td>0.980134</td>\n",
       "      <td>0.980134</td>\n",
       "      <td>0.815568</td>\n",
       "      <td>0.815569</td>\n",
       "      <td>0.815569</td>\n",
       "      <td>0.815907</td>\n",
       "      <td>0.815907</td>\n",
       "      <td>0.815907</td>\n",
       "      <td>0.810629</td>\n",
       "      <td>0.810629</td>\n",
       "      <td>0.810627</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.985765</td>\n",
       "      <td>0.985765</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998261</td>\n",
       "      <td>0.998261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.994076</td>\n",
       "      <td>0.994076</td>\n",
       "      <td>0.984123</td>\n",
       "      <td>0.984123</td>\n",
       "      <td>0.816169</td>\n",
       "      <td>0.816169</td>\n",
       "      <td>0.81617</td>\n",
       "      <td>0.816559</td>\n",
       "      <td>0.816559</td>\n",
       "      <td>0.81656</td>\n",
       "      <td>0.809011</td>\n",
       "      <td>0.809019</td>\n",
       "      <td>0.809024</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992659</td>\n",
       "      <td>0.992659</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998882</td>\n",
       "      <td>0.998882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.944751</td>\n",
       "      <td>0.944751</td>\n",
       "      <td>0.993624</td>\n",
       "      <td>0.993624</td>\n",
       "      <td>0.984203</td>\n",
       "      <td>0.984203</td>\n",
       "      <td>0.826252</td>\n",
       "      <td>0.826251</td>\n",
       "      <td>0.826252</td>\n",
       "      <td>0.826896</td>\n",
       "      <td>0.826895</td>\n",
       "      <td>0.826895</td>\n",
       "      <td>0.817447</td>\n",
       "      <td>0.817446</td>\n",
       "      <td>0.817443</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.992957</td>\n",
       "      <td>0.992957</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999412</td>\n",
       "      <td>0.999412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.947479</td>\n",
       "      <td>0.947479</td>\n",
       "      <td>0.993337</td>\n",
       "      <td>0.993337</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>0.797037</td>\n",
       "      <td>0.797038</td>\n",
       "      <td>0.797037</td>\n",
       "      <td>0.797456</td>\n",
       "      <td>0.797456</td>\n",
       "      <td>0.797455</td>\n",
       "      <td>0.79096</td>\n",
       "      <td>0.790962</td>\n",
       "      <td>0.790962</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.991668</td>\n",
       "      <td>0.991668</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999376</td>\n",
       "      <td>0.999376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.939642</td>\n",
       "      <td>0.939642</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.983021</td>\n",
       "      <td>0.983021</td>\n",
       "      <td>0.755005</td>\n",
       "      <td>0.755004</td>\n",
       "      <td>0.755007</td>\n",
       "      <td>0.756212</td>\n",
       "      <td>0.756212</td>\n",
       "      <td>0.756214</td>\n",
       "      <td>0.743839</td>\n",
       "      <td>0.743832</td>\n",
       "      <td>0.743836</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.992404</td>\n",
       "      <td>0.992404</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999202</td>\n",
       "      <td>0.999202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.940205</td>\n",
       "      <td>0.940205</td>\n",
       "      <td>0.993558</td>\n",
       "      <td>0.993558</td>\n",
       "      <td>0.982879</td>\n",
       "      <td>0.982879</td>\n",
       "      <td>0.802006</td>\n",
       "      <td>0.802006</td>\n",
       "      <td>0.802007</td>\n",
       "      <td>0.802606</td>\n",
       "      <td>0.802606</td>\n",
       "      <td>0.802606</td>\n",
       "      <td>0.794377</td>\n",
       "      <td>0.794378</td>\n",
       "      <td>0.794379</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.991091</td>\n",
       "      <td>0.991091</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>0.999026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.00874746</td>\n",
       "      <td>0.00874746</td>\n",
       "      <td>0.000590475</td>\n",
       "      <td>0.000590475</td>\n",
       "      <td>0.00147357</td>\n",
       "      <td>0.00147357</td>\n",
       "      <td>0.0253225</td>\n",
       "      <td>0.0253228</td>\n",
       "      <td>0.025322</td>\n",
       "      <td>0.0250619</td>\n",
       "      <td>0.0250618</td>\n",
       "      <td>0.0250611</td>\n",
       "      <td>0.0267441</td>\n",
       "      <td>0.0267476</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>6.44945e-05</td>\n",
       "      <td>6.44945e-05</td>\n",
       "      <td>2.86814e-07</td>\n",
       "      <td>2.86814e-07</td>\n",
       "      <td>0.00269676</td>\n",
       "      <td>0.00269676</td>\n",
       "      <td>3.4486e-13</td>\n",
       "      <td>3.4486e-13</td>\n",
       "      <td>7.76822e-09</td>\n",
       "      <td>7.7683e-09</td>\n",
       "      <td>0.000426065</td>\n",
       "      <td>0.000426065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   0   \\\n",
       "mean_fit_time                                                 1.77927   \n",
       "std_fit_time                                                 0.578967   \n",
       "mean_score_time                                             0.0372282   \n",
       "std_score_time                                              0.0107762   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.1   \n",
       "param_max_depth                                                     3   \n",
       "params              {'booster': 'gbtree', 'eta': 0.1, 'max_depth': 3}   \n",
       "split0_test_score                                            0.718617   \n",
       "split1_test_score                                            0.705616   \n",
       "split2_test_score                                            0.707449   \n",
       "split3_test_score                                            0.686166   \n",
       "split4_test_score                                            0.548348   \n",
       "mean_test_score                                              0.673239   \n",
       "std_test_score                                               0.063313   \n",
       "rank_test_score                                                     1   \n",
       "split0_train_score                                           0.923501   \n",
       "split1_train_score                                           0.945652   \n",
       "split2_train_score                                           0.944751   \n",
       "split3_train_score                                           0.947479   \n",
       "split4_train_score                                           0.939642   \n",
       "mean_train_score                                             0.940205   \n",
       "std_train_score                                            0.00874746   \n",
       "\n",
       "                                                                 18  \\\n",
       "mean_fit_time                                               5.04809   \n",
       "std_fit_time                                               0.848517   \n",
       "mean_score_time                                            0.389202   \n",
       "std_score_time                                             0.184764   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.1   \n",
       "param_max_depth                                                   3   \n",
       "params              {'booster': 'dart', 'eta': 0.1, 'max_depth': 3}   \n",
       "split0_test_score                                          0.718617   \n",
       "split1_test_score                                          0.705616   \n",
       "split2_test_score                                          0.707449   \n",
       "split3_test_score                                          0.686166   \n",
       "split4_test_score                                          0.548348   \n",
       "mean_test_score                                            0.673239   \n",
       "std_test_score                                             0.063313   \n",
       "rank_test_score                                                   1   \n",
       "split0_train_score                                         0.923501   \n",
       "split1_train_score                                         0.945652   \n",
       "split2_train_score                                         0.944751   \n",
       "split3_train_score                                         0.947479   \n",
       "split4_train_score                                         0.939642   \n",
       "mean_train_score                                           0.940205   \n",
       "std_train_score                                          0.00874746   \n",
       "\n",
       "                                                                 24  \\\n",
       "mean_fit_time                                               4.38592   \n",
       "std_fit_time                                               0.049135   \n",
       "mean_score_time                                            0.296276   \n",
       "std_score_time                                            0.0109961   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.5   \n",
       "param_max_depth                                                   3   \n",
       "params              {'booster': 'dart', 'eta': 0.5, 'max_depth': 3}   \n",
       "split0_test_score                                          0.801172   \n",
       "split1_test_score                                          0.636005   \n",
       "split2_test_score                                          0.671833   \n",
       "split3_test_score                                          0.664535   \n",
       "split4_test_score                                          0.558297   \n",
       "mean_test_score                                            0.666368   \n",
       "std_test_score                                            0.0784941   \n",
       "rank_test_score                                                   3   \n",
       "split0_train_score                                         0.992553   \n",
       "split1_train_score                                         0.994076   \n",
       "split2_train_score                                         0.993624   \n",
       "split3_train_score                                         0.993337   \n",
       "split4_train_score                                           0.9942   \n",
       "mean_train_score                                           0.993558   \n",
       "std_train_score                                         0.000590475   \n",
       "\n",
       "                                                                   6   \\\n",
       "mean_fit_time                                                 1.08209   \n",
       "std_fit_time                                                0.0492433   \n",
       "mean_score_time                                             0.0327157   \n",
       "std_score_time                                              0.0040801   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.5   \n",
       "param_max_depth                                                     3   \n",
       "params              {'booster': 'gbtree', 'eta': 0.5, 'max_depth': 3}   \n",
       "split0_test_score                                            0.801172   \n",
       "split1_test_score                                            0.636005   \n",
       "split2_test_score                                            0.671833   \n",
       "split3_test_score                                            0.664535   \n",
       "split4_test_score                                            0.558297   \n",
       "mean_test_score                                              0.666368   \n",
       "std_test_score                                              0.0784941   \n",
       "rank_test_score                                                     3   \n",
       "split0_train_score                                           0.992553   \n",
       "split1_train_score                                           0.994076   \n",
       "split2_train_score                                           0.993624   \n",
       "split3_train_score                                           0.993337   \n",
       "split4_train_score                                             0.9942   \n",
       "mean_train_score                                             0.993558   \n",
       "std_train_score                                           0.000590475   \n",
       "\n",
       "                                                                   3   \\\n",
       "mean_fit_time                                                 2.37384   \n",
       "std_fit_time                                                 0.473417   \n",
       "mean_score_time                                             0.0460522   \n",
       "std_score_time                                             0.00523978   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.3   \n",
       "param_max_depth                                                     3   \n",
       "params              {'booster': 'gbtree', 'eta': 0.3, 'max_depth': 3}   \n",
       "split0_test_score                                            0.740014   \n",
       "split1_test_score                                            0.657364   \n",
       "split2_test_score                                            0.706554   \n",
       "split3_test_score                                            0.639807   \n",
       "split4_test_score                                            0.557565   \n",
       "mean_test_score                                              0.660261   \n",
       "std_test_score                                              0.0624127   \n",
       "rank_test_score                                                     5   \n",
       "split0_train_score                                           0.980134   \n",
       "split1_train_score                                           0.984123   \n",
       "split2_train_score                                           0.984203   \n",
       "split3_train_score                                           0.982915   \n",
       "split4_train_score                                           0.983021   \n",
       "mean_train_score                                             0.982879   \n",
       "std_train_score                                            0.00147357   \n",
       "\n",
       "                                                                 21  \\\n",
       "mean_fit_time                                               4.50238   \n",
       "std_fit_time                                              0.0873588   \n",
       "mean_score_time                                            0.314742   \n",
       "std_score_time                                            0.0122553   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.3   \n",
       "param_max_depth                                                   3   \n",
       "params              {'booster': 'dart', 'eta': 0.3, 'max_depth': 3}   \n",
       "split0_test_score                                          0.740014   \n",
       "split1_test_score                                          0.657364   \n",
       "split2_test_score                                          0.706554   \n",
       "split3_test_score                                          0.639807   \n",
       "split4_test_score                                          0.557565   \n",
       "mean_test_score                                            0.660261   \n",
       "std_test_score                                            0.0624127   \n",
       "rank_test_score                                                   5   \n",
       "split0_train_score                                         0.980134   \n",
       "split1_train_score                                         0.984123   \n",
       "split2_train_score                                         0.984203   \n",
       "split3_train_score                                         0.982915   \n",
       "split4_train_score                                         0.983021   \n",
       "mean_train_score                                           0.982879   \n",
       "std_train_score                                          0.00147357   \n",
       "\n",
       "                                                                   12  \\\n",
       "mean_fit_time                                                0.485712   \n",
       "std_fit_time                                                0.0156071   \n",
       "mean_score_time                                             0.0363506   \n",
       "std_score_time                                             0.00562707   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.3   \n",
       "param_max_depth                                                     3   \n",
       "params              {'booster': 'gblinear', 'eta': 0.3, 'max_depth...   \n",
       "split0_test_score                                             0.66273   \n",
       "split1_test_score                                             0.61936   \n",
       "split2_test_score                                            0.528968   \n",
       "split3_test_score                                            0.782239   \n",
       "split4_test_score                                            0.577878   \n",
       "mean_test_score                                              0.634235   \n",
       "std_test_score                                              0.0862504   \n",
       "rank_test_score                                                     7   \n",
       "split0_train_score                                           0.815568   \n",
       "split1_train_score                                           0.816169   \n",
       "split2_train_score                                           0.826252   \n",
       "split3_train_score                                           0.797037   \n",
       "split4_train_score                                           0.755005   \n",
       "mean_train_score                                             0.802006   \n",
       "std_train_score                                             0.0253225   \n",
       "\n",
       "                                                                   14  \\\n",
       "mean_fit_time                                                0.493814   \n",
       "std_fit_time                                               0.00759194   \n",
       "mean_score_time                                             0.0396918   \n",
       "std_score_time                                             0.00688606   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.3   \n",
       "param_max_depth                                                     9   \n",
       "params              {'booster': 'gblinear', 'eta': 0.3, 'max_depth...   \n",
       "split0_test_score                                            0.662761   \n",
       "split1_test_score                                            0.619295   \n",
       "split2_test_score                                            0.529087   \n",
       "split3_test_score                                            0.782202   \n",
       "split4_test_score                                            0.577721   \n",
       "mean_test_score                                              0.634213   \n",
       "std_test_score                                              0.0862336   \n",
       "rank_test_score                                                     8   \n",
       "split0_train_score                                           0.815569   \n",
       "split1_train_score                                           0.816169   \n",
       "split2_train_score                                           0.826251   \n",
       "split3_train_score                                           0.797038   \n",
       "split4_train_score                                           0.755004   \n",
       "mean_train_score                                             0.802006   \n",
       "std_train_score                                             0.0253228   \n",
       "\n",
       "                                                                   13  \\\n",
       "mean_fit_time                                                0.495523   \n",
       "std_fit_time                                               0.00834877   \n",
       "mean_score_time                                             0.0374213   \n",
       "std_score_time                                             0.00548295   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.3   \n",
       "param_max_depth                                                     6   \n",
       "params              {'booster': 'gblinear', 'eta': 0.3, 'max_depth...   \n",
       "split0_test_score                                            0.662762   \n",
       "split1_test_score                                            0.619351   \n",
       "split2_test_score                                            0.528998   \n",
       "split3_test_score                                            0.782214   \n",
       "split4_test_score                                            0.577688   \n",
       "mean_test_score                                              0.634203   \n",
       "std_test_score                                              0.0862615   \n",
       "rank_test_score                                                     9   \n",
       "split0_train_score                                           0.815569   \n",
       "split1_train_score                                            0.81617   \n",
       "split2_train_score                                           0.826252   \n",
       "split3_train_score                                           0.797037   \n",
       "split4_train_score                                           0.755007   \n",
       "mean_train_score                                             0.802007   \n",
       "std_train_score                                              0.025322   \n",
       "\n",
       "                                                                   16  \\\n",
       "mean_fit_time                                                0.500992   \n",
       "std_fit_time                                               0.00539736   \n",
       "mean_score_time                                             0.0422152   \n",
       "std_score_time                                             0.00589135   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.5   \n",
       "param_max_depth                                                     6   \n",
       "params              {'booster': 'gblinear', 'eta': 0.5, 'max_depth...   \n",
       "split0_test_score                                            0.668291   \n",
       "split1_test_score                                            0.619093   \n",
       "split2_test_score                                              0.5199   \n",
       "split3_test_score                                             0.78397   \n",
       "split4_test_score                                            0.578492   \n",
       "mean_test_score                                              0.633949   \n",
       "std_test_score                                              0.0894306   \n",
       "rank_test_score                                                    10   \n",
       "split0_train_score                                           0.815907   \n",
       "split1_train_score                                           0.816559   \n",
       "split2_train_score                                           0.826896   \n",
       "split3_train_score                                           0.797456   \n",
       "split4_train_score                                           0.756212   \n",
       "mean_train_score                                             0.802606   \n",
       "std_train_score                                             0.0250619   \n",
       "\n",
       "                                                                   15  \\\n",
       "mean_fit_time                                                0.500983   \n",
       "std_fit_time                                               0.00678189   \n",
       "mean_score_time                                             0.0403194   \n",
       "std_score_time                                             0.00477391   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.5   \n",
       "param_max_depth                                                     3   \n",
       "params              {'booster': 'gblinear', 'eta': 0.5, 'max_depth...   \n",
       "split0_test_score                                            0.668293   \n",
       "split1_test_score                                            0.619081   \n",
       "split2_test_score                                            0.519849   \n",
       "split3_test_score                                            0.783975   \n",
       "split4_test_score                                            0.578297   \n",
       "mean_test_score                                              0.633899   \n",
       "std_test_score                                                0.08947   \n",
       "rank_test_score                                                    11   \n",
       "split0_train_score                                           0.815907   \n",
       "split1_train_score                                           0.816559   \n",
       "split2_train_score                                           0.826895   \n",
       "split3_train_score                                           0.797456   \n",
       "split4_train_score                                           0.756212   \n",
       "mean_train_score                                             0.802606   \n",
       "std_train_score                                             0.0250618   \n",
       "\n",
       "                                                                   17  \\\n",
       "mean_fit_time                                                0.528969   \n",
       "std_fit_time                                                0.0516172   \n",
       "mean_score_time                                             0.0389235   \n",
       "std_score_time                                              0.0060827   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.5   \n",
       "param_max_depth                                                     9   \n",
       "params              {'booster': 'gblinear', 'eta': 0.5, 'max_depth...   \n",
       "split0_test_score                                            0.668298   \n",
       "split1_test_score                                            0.619061   \n",
       "split2_test_score                                            0.519854   \n",
       "split3_test_score                                            0.784057   \n",
       "split4_test_score                                            0.574929   \n",
       "mean_test_score                                               0.63324   \n",
       "std_test_score                                              0.0899248   \n",
       "rank_test_score                                                    12   \n",
       "split0_train_score                                           0.815907   \n",
       "split1_train_score                                            0.81656   \n",
       "split2_train_score                                           0.826895   \n",
       "split3_train_score                                           0.797455   \n",
       "split4_train_score                                           0.756214   \n",
       "mean_train_score                                             0.802606   \n",
       "std_train_score                                             0.0250611   \n",
       "\n",
       "                                                                   9   \\\n",
       "mean_fit_time                                                 0.43019   \n",
       "std_fit_time                                                0.0407312   \n",
       "mean_score_time                                               0.03729   \n",
       "std_score_time                                             0.00309194   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.1   \n",
       "param_max_depth                                                     3   \n",
       "params              {'booster': 'gblinear', 'eta': 0.1, 'max_depth...   \n",
       "split0_test_score                                            0.627935   \n",
       "split1_test_score                                            0.619431   \n",
       "split2_test_score                                            0.543039   \n",
       "split3_test_score                                            0.760324   \n",
       "split4_test_score                                            0.575208   \n",
       "mean_test_score                                              0.625188   \n",
       "std_test_score                                              0.0742278   \n",
       "rank_test_score                                                    13   \n",
       "split0_train_score                                           0.810629   \n",
       "split1_train_score                                           0.809011   \n",
       "split2_train_score                                           0.817447   \n",
       "split3_train_score                                            0.79096   \n",
       "split4_train_score                                           0.743839   \n",
       "mean_train_score                                             0.794377   \n",
       "std_train_score                                             0.0267441   \n",
       "\n",
       "                                                                   11  \\\n",
       "mean_fit_time                                                0.502575   \n",
       "std_fit_time                                               0.00773961   \n",
       "mean_score_time                                             0.0414134   \n",
       "std_score_time                                             0.00247902   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.1   \n",
       "param_max_depth                                                     9   \n",
       "params              {'booster': 'gblinear', 'eta': 0.1, 'max_depth...   \n",
       "split0_test_score                                             0.62805   \n",
       "split1_test_score                                            0.619366   \n",
       "split2_test_score                                            0.542963   \n",
       "split3_test_score                                            0.760304   \n",
       "split4_test_score                                              0.5752   \n",
       "mean_test_score                                              0.625177   \n",
       "std_test_score                                              0.0742402   \n",
       "rank_test_score                                                    14   \n",
       "split0_train_score                                           0.810629   \n",
       "split1_train_score                                           0.809019   \n",
       "split2_train_score                                           0.817446   \n",
       "split3_train_score                                           0.790962   \n",
       "split4_train_score                                           0.743832   \n",
       "mean_train_score                                             0.794378   \n",
       "std_train_score                                             0.0267476   \n",
       "\n",
       "                                                                   10  \\\n",
       "mean_fit_time                                                0.482357   \n",
       "std_fit_time                                                0.0198613   \n",
       "mean_score_time                                             0.0407675   \n",
       "std_score_time                                              0.0058371   \n",
       "param_booster                                                gblinear   \n",
       "param_eta                                                         0.1   \n",
       "param_max_depth                                                     6   \n",
       "params              {'booster': 'gblinear', 'eta': 0.1, 'max_depth...   \n",
       "split0_test_score                                             0.62805   \n",
       "split1_test_score                                            0.619364   \n",
       "split2_test_score                                            0.542953   \n",
       "split3_test_score                                            0.760291   \n",
       "split4_test_score                                              0.5752   \n",
       "mean_test_score                                              0.625171   \n",
       "std_test_score                                              0.0742377   \n",
       "rank_test_score                                                    15   \n",
       "split0_train_score                                           0.810627   \n",
       "split1_train_score                                           0.809024   \n",
       "split2_train_score                                           0.817443   \n",
       "split3_train_score                                           0.790962   \n",
       "split4_train_score                                           0.743836   \n",
       "mean_train_score                                             0.794379   \n",
       "std_train_score                                              0.026746   \n",
       "\n",
       "                                                                   4   \\\n",
       "mean_fit_time                                                 2.20184   \n",
       "std_fit_time                                                 0.627093   \n",
       "mean_score_time                                             0.0317651   \n",
       "std_score_time                                              0.0054471   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.3   \n",
       "param_max_depth                                                     6   \n",
       "params              {'booster': 'gbtree', 'eta': 0.3, 'max_depth': 6}   \n",
       "split0_test_score                                            0.636736   \n",
       "split1_test_score                                            0.584836   \n",
       "split2_test_score                                            0.606486   \n",
       "split3_test_score                                            0.574779   \n",
       "split4_test_score                                            0.420316   \n",
       "mean_test_score                                              0.564631   \n",
       "std_test_score                                              0.0752182   \n",
       "rank_test_score                                                    16   \n",
       "split0_train_score                                           0.999756   \n",
       "split1_train_score                                           0.999918   \n",
       "split2_train_score                                           0.999934   \n",
       "split3_train_score                                           0.999846   \n",
       "split4_train_score                                             0.9999   \n",
       "mean_train_score                                             0.999871   \n",
       "std_train_score                                           6.44945e-05   \n",
       "\n",
       "                                                                 22  \\\n",
       "mean_fit_time                                               6.76226   \n",
       "std_fit_time                                                1.47166   \n",
       "mean_score_time                                            0.468136   \n",
       "std_score_time                                              0.12183   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.3   \n",
       "param_max_depth                                                   6   \n",
       "params              {'booster': 'dart', 'eta': 0.3, 'max_depth': 6}   \n",
       "split0_test_score                                          0.636736   \n",
       "split1_test_score                                          0.584836   \n",
       "split2_test_score                                          0.606486   \n",
       "split3_test_score                                          0.574779   \n",
       "split4_test_score                                          0.420316   \n",
       "mean_test_score                                            0.564631   \n",
       "std_test_score                                            0.0752182   \n",
       "rank_test_score                                                  16   \n",
       "split0_train_score                                         0.999756   \n",
       "split1_train_score                                         0.999918   \n",
       "split2_train_score                                         0.999934   \n",
       "split3_train_score                                         0.999846   \n",
       "split4_train_score                                           0.9999   \n",
       "mean_train_score                                           0.999871   \n",
       "std_train_score                                         6.44945e-05   \n",
       "\n",
       "                                                                 25  \\\n",
       "mean_fit_time                                                4.9323   \n",
       "std_fit_time                                              0.0794922   \n",
       "mean_score_time                                            0.285482   \n",
       "std_score_time                                              0.01865   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.5   \n",
       "param_max_depth                                                   6   \n",
       "params              {'booster': 'dart', 'eta': 0.5, 'max_depth': 6}   \n",
       "split0_test_score                                           0.60935   \n",
       "split1_test_score                                          0.508442   \n",
       "split2_test_score                                          0.558562   \n",
       "split3_test_score                                          0.595516   \n",
       "split4_test_score                                           0.48028   \n",
       "mean_test_score                                             0.55043   \n",
       "std_test_score                                            0.0495062   \n",
       "rank_test_score                                                  18   \n",
       "split0_train_score                                         0.999999   \n",
       "split1_train_score                                                1   \n",
       "split2_train_score                                         0.999999   \n",
       "split3_train_score                                         0.999999   \n",
       "split4_train_score                                         0.999999   \n",
       "mean_train_score                                           0.999999   \n",
       "std_train_score                                         2.86814e-07   \n",
       "\n",
       "                                                                   7   \\\n",
       "mean_fit_time                                                 1.69361   \n",
       "std_fit_time                                                0.0934324   \n",
       "mean_score_time                                             0.0308116   \n",
       "std_score_time                                             0.00431828   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.5   \n",
       "param_max_depth                                                     6   \n",
       "params              {'booster': 'gbtree', 'eta': 0.5, 'max_depth': 6}   \n",
       "split0_test_score                                             0.60935   \n",
       "split1_test_score                                            0.508442   \n",
       "split2_test_score                                            0.558562   \n",
       "split3_test_score                                            0.595516   \n",
       "split4_test_score                                             0.48028   \n",
       "mean_test_score                                               0.55043   \n",
       "std_test_score                                              0.0495062   \n",
       "rank_test_score                                                    18   \n",
       "split0_train_score                                           0.999999   \n",
       "split1_train_score                                                  1   \n",
       "split2_train_score                                           0.999999   \n",
       "split3_train_score                                           0.999999   \n",
       "split4_train_score                                           0.999999   \n",
       "mean_train_score                                             0.999999   \n",
       "std_train_score                                           2.86814e-07   \n",
       "\n",
       "                                                                   1   \\\n",
       "mean_fit_time                                                 3.05424   \n",
       "std_fit_time                                                 0.523732   \n",
       "mean_score_time                                             0.0409647   \n",
       "std_score_time                                             0.00668034   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.1   \n",
       "param_max_depth                                                     6   \n",
       "params              {'booster': 'gbtree', 'eta': 0.1, 'max_depth': 6}   \n",
       "split0_test_score                                            0.595881   \n",
       "split1_test_score                                            0.625855   \n",
       "split2_test_score                                            0.584308   \n",
       "split3_test_score                                            0.592489   \n",
       "split4_test_score                                            0.332725   \n",
       "mean_test_score                                              0.546252   \n",
       "std_test_score                                               0.107684   \n",
       "rank_test_score                                                    20   \n",
       "split0_train_score                                           0.985765   \n",
       "split1_train_score                                           0.992659   \n",
       "split2_train_score                                           0.992957   \n",
       "split3_train_score                                           0.991668   \n",
       "split4_train_score                                           0.992404   \n",
       "mean_train_score                                             0.991091   \n",
       "std_train_score                                            0.00269676   \n",
       "\n",
       "                                                                 19  \\\n",
       "mean_fit_time                                               6.01168   \n",
       "std_fit_time                                                1.53461   \n",
       "mean_score_time                                            0.332416   \n",
       "std_score_time                                            0.0129538   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.1   \n",
       "param_max_depth                                                   6   \n",
       "params              {'booster': 'dart', 'eta': 0.1, 'max_depth': 6}   \n",
       "split0_test_score                                          0.595881   \n",
       "split1_test_score                                          0.625855   \n",
       "split2_test_score                                          0.584308   \n",
       "split3_test_score                                          0.592489   \n",
       "split4_test_score                                          0.332725   \n",
       "mean_test_score                                            0.546252   \n",
       "std_test_score                                             0.107684   \n",
       "rank_test_score                                                  20   \n",
       "split0_train_score                                         0.985765   \n",
       "split1_train_score                                         0.992659   \n",
       "split2_train_score                                         0.992957   \n",
       "split3_train_score                                         0.991668   \n",
       "split4_train_score                                         0.992404   \n",
       "mean_train_score                                           0.991091   \n",
       "std_train_score                                          0.00269676   \n",
       "\n",
       "                                                                 26  \\\n",
       "mean_fit_time                                               5.71629   \n",
       "std_fit_time                                               0.148101   \n",
       "mean_score_time                                            0.284697   \n",
       "std_score_time                                            0.0181049   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.5   \n",
       "param_max_depth                                                   9   \n",
       "params              {'booster': 'dart', 'eta': 0.5, 'max_depth': 9}   \n",
       "split0_test_score                                          0.573475   \n",
       "split1_test_score                                          0.560553   \n",
       "split2_test_score                                          0.482772   \n",
       "split3_test_score                                           0.56311   \n",
       "split4_test_score                                          0.487185   \n",
       "mean_test_score                                            0.533419   \n",
       "std_test_score                                            0.0398119   \n",
       "rank_test_score                                                  22   \n",
       "split0_train_score                                                1   \n",
       "split1_train_score                                                1   \n",
       "split2_train_score                                                1   \n",
       "split3_train_score                                                1   \n",
       "split4_train_score                                                1   \n",
       "mean_train_score                                                  1   \n",
       "std_train_score                                          3.4486e-13   \n",
       "\n",
       "                                                                   8   \\\n",
       "mean_fit_time                                                 3.42853   \n",
       "std_fit_time                                                  1.38746   \n",
       "mean_score_time                                             0.0327769   \n",
       "std_score_time                                             0.00720121   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.5   \n",
       "param_max_depth                                                     9   \n",
       "params              {'booster': 'gbtree', 'eta': 0.5, 'max_depth': 9}   \n",
       "split0_test_score                                            0.573475   \n",
       "split1_test_score                                            0.560553   \n",
       "split2_test_score                                            0.482772   \n",
       "split3_test_score                                             0.56311   \n",
       "split4_test_score                                            0.487185   \n",
       "mean_test_score                                              0.533419   \n",
       "std_test_score                                              0.0398119   \n",
       "rank_test_score                                                    22   \n",
       "split0_train_score                                                  1   \n",
       "split1_train_score                                                  1   \n",
       "split2_train_score                                                  1   \n",
       "split3_train_score                                                  1   \n",
       "split4_train_score                                                  1   \n",
       "mean_train_score                                                    1   \n",
       "std_train_score                                            3.4486e-13   \n",
       "\n",
       "                                                                   5   \\\n",
       "mean_fit_time                                                  2.3374   \n",
       "std_fit_time                                                 0.175279   \n",
       "mean_score_time                                             0.0325922   \n",
       "std_score_time                                             0.00522442   \n",
       "param_booster                                                  gbtree   \n",
       "param_eta                                                         0.3   \n",
       "param_max_depth                                                     9   \n",
       "params              {'booster': 'gbtree', 'eta': 0.3, 'max_depth': 9}   \n",
       "split0_test_score                                            0.529732   \n",
       "split1_test_score                                            0.587855   \n",
       "split2_test_score                                            0.556143   \n",
       "split3_test_score                                            0.544178   \n",
       "split4_test_score                                            0.438116   \n",
       "mean_test_score                                              0.531205   \n",
       "std_test_score                                              0.0503332   \n",
       "rank_test_score                                                    24   \n",
       "split0_train_score                                                  1   \n",
       "split1_train_score                                                  1   \n",
       "split2_train_score                                                  1   \n",
       "split3_train_score                                                  1   \n",
       "split4_train_score                                                  1   \n",
       "mean_train_score                                                    1   \n",
       "std_train_score                                           7.76822e-09   \n",
       "\n",
       "                                                                 23  \\\n",
       "mean_fit_time                                               5.75355   \n",
       "std_fit_time                                                0.19214   \n",
       "mean_score_time                                            0.288041   \n",
       "std_score_time                                             0.010243   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.3   \n",
       "param_max_depth                                                   9   \n",
       "params              {'booster': 'dart', 'eta': 0.3, 'max_depth': 9}   \n",
       "split0_test_score                                          0.529732   \n",
       "split1_test_score                                          0.587855   \n",
       "split2_test_score                                          0.556143   \n",
       "split3_test_score                                          0.544178   \n",
       "split4_test_score                                          0.438116   \n",
       "mean_test_score                                            0.531205   \n",
       "std_test_score                                            0.0503332   \n",
       "rank_test_score                                                  25   \n",
       "split0_train_score                                                1   \n",
       "split1_train_score                                                1   \n",
       "split2_train_score                                                1   \n",
       "split3_train_score                                                1   \n",
       "split4_train_score                                                1   \n",
       "mean_train_score                                                  1   \n",
       "std_train_score                                          7.7683e-09   \n",
       "\n",
       "                                                                 20  \\\n",
       "mean_fit_time                                               6.12182   \n",
       "std_fit_time                                               0.452777   \n",
       "mean_score_time                                            0.326501   \n",
       "std_score_time                                           0.00232989   \n",
       "param_booster                                                  dart   \n",
       "param_eta                                                       0.1   \n",
       "param_max_depth                                                   9   \n",
       "params              {'booster': 'dart', 'eta': 0.1, 'max_depth': 9}   \n",
       "split0_test_score                                          0.584384   \n",
       "split1_test_score                                          0.601401   \n",
       "split2_test_score                                          0.520857   \n",
       "split3_test_score                                          0.520999   \n",
       "split4_test_score                                          0.373867   \n",
       "mean_test_score                                            0.520302   \n",
       "std_test_score                                            0.0801593   \n",
       "rank_test_score                                                  26   \n",
       "split0_train_score                                         0.998261   \n",
       "split1_train_score                                         0.998882   \n",
       "split2_train_score                                         0.999412   \n",
       "split3_train_score                                         0.999376   \n",
       "split4_train_score                                         0.999202   \n",
       "mean_train_score                                           0.999026   \n",
       "std_train_score                                         0.000426065   \n",
       "\n",
       "                                                                   2   \n",
       "mean_fit_time                                                 5.21527  \n",
       "std_fit_time                                                  1.30253  \n",
       "mean_score_time                                             0.0446184  \n",
       "std_score_time                                              0.0073479  \n",
       "param_booster                                                  gbtree  \n",
       "param_eta                                                         0.1  \n",
       "param_max_depth                                                     9  \n",
       "params              {'booster': 'gbtree', 'eta': 0.1, 'max_depth': 9}  \n",
       "split0_test_score                                            0.584384  \n",
       "split1_test_score                                            0.601401  \n",
       "split2_test_score                                            0.520857  \n",
       "split3_test_score                                            0.520999  \n",
       "split4_test_score                                            0.373867  \n",
       "mean_test_score                                              0.520302  \n",
       "std_test_score                                              0.0801593  \n",
       "rank_test_score                                                    26  \n",
       "split0_train_score                                           0.998261  \n",
       "split1_train_score                                           0.998882  \n",
       "split2_train_score                                           0.999412  \n",
       "split3_train_score                                           0.999376  \n",
       "split4_train_score                                           0.999202  \n",
       "mean_train_score                                             0.999026  \n",
       "std_train_score                                           0.000426065  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search_cv.cv_results_ ).sort_values('mean_test_score', ascending = False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualPrices = y\n",
    "actualPrices2 = np.reshape(actualPrices, (610,))\n",
    "print(actualPrices2.shape)\n",
    "predictedPrices = np.reshape(model.predict(X), (610,))\n",
    "print(predictedPrices.shape)\n",
    "#print(actualPrices2)\n",
    "print(predictedPrices)\n",
    "residuals = []\n",
    "for item1, item2 in zip(actualPrices2.tolist(), predictedPrices.tolist()):\n",
    "    residuals.append(item1 - item2)\n",
    "#rediduals = actualPrices2 - predictedPrices\n",
    "#print(residuals.shape)\n",
    "print(len(residuals))\n",
    "#print(residuals)\n",
    "plt.scatter(predictedPrices, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "#plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "#plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('residuals')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
